---
title: "Robust, value-based trial design when nuisance parameters are unknown"
author: "D. T. Wilson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(numDeriv)
require(ggplot2)
require(shiny)
require(pso)
require(viridis)
require(gganimate)
require(transformr)
require(magick)
require(patchwork)
require(statmod)
require(reshape2)
require(mgcv)
require(RColorBrewer)
cols <- brewer.pal(8, "Dark2") 
```


## Non-lienar power value

Assuming a linear single attribute n, and assuming that the rate of substitution depends on $\beta$ but not on $n$ (so, the amount of $n$ we will pay to improve $\beta$ does not depend on $n$), a single attribute value function for $\beta$ will complete our specification.

What can we say, in general, about $v(\beta)$? We can surely say that it plateaus as $\beta$ tends to 0, since by that point decreasing $\beta$ will only give us improvements in the power to detect an effect very close to the null. 

We could argue value will actually start to decrease, if we are to strictly follow the test results, since as $\beta$ tends to 0 we will progress even for effects very close to the null. Counter argument is that if we got a very precise estimate of a small effect we would not progress; but that implies a different test procedure and so different error rates.

At the other end, starting with $\beta = 1- \alpha$, do we expect value to be increasing at a decreasing rate? One potential metric which might be helpful is the integral of the power function beyond the null and up to some maximum effect - equivalently, the average power over a uniform distribution of treatment effects.

```{r}
pow <- function(mu, sd)
{
  power.t.test(n = 200, delta = mu, sd = sd)$pow
}

z <- integrate(pow, lower=0, upper = 5,
          sd = 1)

df <- data.frame(sd = seq(0.1, 5, 0.01))
df$p <- sapply(df$sd, function(x) pow(0.3, x))
df$v <- sapply(df$sd, function(x) integrate(pow, lower=0, upper = 5, sd = x)$value)

plot(df$p, df$v)

df$r <- 1
df$r[1:490] <- df$v[2:491] - df$v[1:490]
```
So we can say that value of power is monotonic increasing, and that its derivative is monotonic decreasing at least up to very large powers (say, 0.95).

To get the value function, we need to elicit the rate at which we would `pay' for more power given we already have $1 - \beta$, for different values of $1 - \beta$. One way to do that is to vary hypothetical power functions (by altering sd) and find the function where the sample size with $1 - \beta$ is optimal. The sample size which achieves this is irrelevant - the same rate of substitution will apply for all $n$.

```{r}
v <- function(x, lambda, delta)
{
  log(x[,2]*delta + 1)/log(delta+1) - lambda*x[,1]
}

l <- 0.0009; d <- 20
l <- 0.0023; d <- 0.001
l <- 0.0013; d <- 0.001

df <- expand.grid(n = 2:400,
                  p = seq(0,1,0.01))
df$v <- v(df, lambda = l, delta = d)

opt <- NULL
for(sd in seq(0.2,5,0.1)){
  z <- cbind(2:400, power.t.test(n = 2:400, delta = 0.3, sd = sd)$pow)
  opt <- rbind(opt, z[which.max(v(z, l, d)),])
}

plot(seq(0.2,5,0.1), opt[,1])

ggplot(df, aes(n, p)) + geom_contour(aes(z = v)) +
  geom_point(data = data.frame(n = opt[,1], p = opt[,2]))
```
i) Value

We can characterise trial design decisions as balancing the cost of sampling against the benefits of precise estimates or, more typically, power to detect an MCID. We generally solve this bi-objective problem by imposing a power constraint.

When faced with a decision like this (under certainty), one approach is to define a function $V$ which articulates our preferences over the attributes $n$ and $\beta$. Solving a specific problem would then calculate $v$ for all the possible pairs and choose that with the largest $v$. The existence of $v$ follows from an assumption of a weak ordering on the attribute space.

One reason we might not want to do this is that it solves a bigger problem than the one we are interested in. If we elicit $v$ then we can solve any SSD problem in this setting (i.e. same clinical area and MCID). In practice, we only need to look at one specific power curve (conditioning on $\sigma$). So why bother?

The issue is that $\sigma$ is not known, but just an estimate. To deal with uncertainty in $\sigma$, we need $v$ to make sure our decision making is coherent / consistent.

Proposal of $v$ structure and therefore elicitation - linear in n; marginal trade-offs depend on $\beta$ but not $n$; can elicit by asking for some hypothetical optimal designs. 

ii) SSD

Suppose we have an estimate and uncertainty interval for $\sigma$ which has come from a previous study. We know what the locally optimal design is for any true $\sigma$. A (frequentist) decision-theoretic approach to dealing with the uncertainty is to choose the $n$ with the best worst-case outcome over the interval - minimax. We define qulaity here not by value itself, but by regret - the difference in value between the proposed desgin and the locally optimal design.

iii) SSR

Regret is clearly minimised when the estimate is true, and the uncertainty interval tends to 0. So, we might want to consider producing a new and more precise estimate via a pilot to help minimise the worst case regret of the optimal n. This pilot could be external or internal. We then need to decide how large the pilot should be, given its costs (which are sampling in external pilots, and alpha spending [and costs of interim stops] in internal pilots).

Given the algorithm for choosing $n$ based on the pilot estimate, we now have a distribution on $n$ and therefore $v$ for any pilot samples size. To choose between these distributions we can look at $E[v]$, which effectively assumes we are risk-neutral. 

To do this, we will need to augment $v$. For an external pilot, we will need to include cost of pilot sampling (which might be different to main sampling), and pilot set-up costs.

Now have, for every pilot n, a range of expected values over true $\sigma$. Similar to SSD, we then get the locally optimal pilots and then choose pilot n based on minimax regret of expected value over the interval.

iv) Illustration and evaluation

Focus on the simple two-sample t-test here.

For SSD, we can compare against Julious2006 which was shown in Shi2020 to be a good (best?) method for the case of a continuous endpoint. Might also want to use the adjusted CI approach of Browne. And compare against using just the point estimate.

For SSR, compare with Whitehead2015 who extended the NCT approach to SSR. Their paper is not very clear. They suggest choosing the pilot n to minimise the total n, but base this on a fixed main n following from the initial estimate of $\sigma$. That is, they don't account for the random main n. This may or may not correspond to the expected n. They do go on to look at the distribution of main power, which seems to back up their approach as the expected power is nominal. So they basically use just the initial point estimate to design the pilot.

Want to do all this for different value parametrisations. four scenarios crossing the two params, so we have different trade offs and also linear / non-linear in power. Have all agree on the same locally opt design conditional on point estimate.

May want to look at different effect sizes - report only if qualitative conclusions change.

v) Extensions

TO show how the general framework can easily apply to other problems, look at e.g. a cRCT with random cluster sizes. For some example initial estimates and uncertainties, find the optimal pilot, and for a simulated pilot data example show how that determines optimal main n. Outline any difficulties, e.g. computational.

vi) Discussion

How does $v$ based approach compare against standard SSD? Latter only makes sense if there is a threshold of power where quality jumps up, but no reason to think this holds. In practice, the standard approach is not actually used. Instead, a value based approach is done implicitly by adjusting the MCID to get a "feasible" sample size. So this proposal is not that radical, and by doing things explicitly we should be able to do better.

Elicitation in practice might be tricky. In particular, who's values are we eliciting? The team writing the proposal? The funder who makes the decision? Or even the patient who might be enrolled? How to reconcile different stakeholders having different values?

We focus on estimating $\sigma$. This is consistent with arguments against using a pilot estimate of the effect as a basis for future sample size calcs. One way to extend would be to add other points where power is calculated to $v$. In the limit, we would have an integral of a function which weights by value. These weights will come from considerations of both the impact and likelihood of a significant result. We might arrive at it by splitting these into defining a prior and a utility function, and then find ourselves in a fully Bayesian decision-theoretic framework. This would also let us deal with $\sigma$ in a probabilistic sense, and make decisions based on expected utility over all uncertainties - parameter values and pilot sampling.

There are lots of reasons to run a pilot, beyond estimating $\sigma$. We could extend the method by including some which directly relate to power - see WP1.1. This extra value could also be taken into account in terms of the cost of the pilot - e.g. saying that the first 30 patients are free, since we will be using that for other feasibility objectives.

To do -
Two sample t test problem. 
Implement methods for choosing n based on estimate and interval - ours, NCT, upper CI.
Implement methods for choosing pilot n based on initial estimate and interval.
Methods to be flexible in terms of intervals, treatment effect, and two value parameters.
Special case on known $\sigma$
Sim study comparing methods for SSD - for different points and interval widths, what is the optimal n? Are the methods meaningfully different? How does the relationship change with effect size? Are some value functions closer to standard methods than others?
(Expect differences not to be massive here, since value params chosen to agree with standard methods).
Sim study comparing methods for SSR. As above, but now what is the optimal n_p? And what are the resulting optimal distributions on power and total sample size / total cost?
(Expect bigger differences here)

Note on NCT method -
This gives a decision rule such that the overall power of a pilot-then-main trial process is nominal. When limiting ourselves to SSD, this is not a relevant comparator, but it will be for SSR. In practice, it leads to a small increase the the usual constrained design, so just compare against that instead. Note that by implicitly arguing an average power is of the same value as a conditional power, the implied utility (and therefore value) function on power is linear. Note that this is then incompatible with the threshold myth - if the myth holds, we would much prefer a sure thing of 0.85 to a 50/50 gamble between 0.75 and 0.95. In comparison, the upper CI approach can be understood as minimax regret when the value function is a threshold.

Note - if we want to include a stepped wedge trial as an example or extension, a recent paper Korevaar2021 has compiled a database of correlation estimates and has a Shiny app for obtaining plausible ranges of these.

Note - if looking at cluster trials, Copas2021 have just published a paper on optimal design when cluster size can be varied which includes a section on parameter uncertainty where they find the design that gives power over a specified range.

## Introduction

In this document we will develop applications of a value-based approach to sample size determination (SSD) when nuisance parameters are unknown. The scope is limited to SSD - we will not consider sample size re-estimation here.

## t-test running example

Consider a two-arm trial with a normal endpoint which will compare the means in each group via a t-test. Denote the variance of the endpoint (common across arms) by $\sigma^2$, and the target difference under the alternative hypothesis (which the trial will be powered to detect) by $\delta_a$. Let the sample size in each arm of the trial be denoted by $n$.

We propose two value functions:
$$
\begin{align}
v_1(n, \sigma; \lambda_1, c_1) & = \underbrace{1 - \Phi\left(z_{1 - \alpha} - \frac{\delta_a}{\sqrt{2\sigma^2/n}}\right)}_\textrm{Power} - \lambda_1 n - c_1 \\
v_2(n, \sigma; \lambda_2, c_2) & = \underbrace{\sqrt{\frac{n}{2\sigma^2}}}_\textrm{Precision} - \lambda_2 n - c_2.
\end{align}
$$

Each value function has a benefit component (the power of the trial and the sampling precision of the estimated mean difference, respectively) offset by a cost component (a multiple of the sample size and a fixed setup cost in each case). These value functions imply a constant trade-off between the sample size of the trial and its power/precision, with the rate given by the parameters $\lambda_1$ and $\lambda_2$. They also include fixed set-up costs, $c_1$ and $c_2$.

To illustrate these value functions, we plot contours and overlay these with specific power and precision curves. Note that we have fixed $\delta_a = 0.3$ and $\alpha = 0.025$.

```{r}
v1 <- function(n, sig, lambda1, c1) {
  delta_a <- 0.3
  1-pt(qt(0.975, 2*(n-1)), 2*(n-1), 0.3/sqrt(2*sig^2/n)) - lambda1*n - c1
  
  # May be useful to put value into units of sample size
  #(1-pt(qt(0.975, 2*(n-1)), 2*(n-1), 0.3/sqrt(2*sig^2/n)))/lambda1 - n - c1/lambda1
}

v2 <- function(n, sig, lambda2, c2) {
  sqrt(n/(2*sig^2)) - lambda2*n - c2
  
  # May be useful to put value into units of sample size
  #sqrt(n/(2*sig^2))/lambda2 - n - c2/lambda2
}

# Define some example trade-offs
lambda1 <- c(0.00224, 0.00392); lambda2 <- c(0.02668, 0.0337)
# These have been chosen to give optimal sample sizes 
# of 176 and 110, corresponding to power of 60 and 80% to 
# detect an effect of 0.3 when sigma = 1
# ns <- 2:500; ns[which.max(sapply(ns, v1, sig = 1, c1 = 0, lambda1 = 0.01))]

# Take a common fixed cost in units of sample size
n_cost <- 15
c1 <- lambda1*n_cost; c2 <- lambda2*n_cost

plots <- vector("list", 4)
for(i in 1:4){
  lambda <- c(lambda1, lambda2)[i]
  C <- c(c1, c2)[i]
  
  df <- data.frame(n=seq(4,500,1))
  df <- rbind(df, df)
  df$sig <- c(rep(1, nrow(df)/2), rep(1.3*1, nrow(df)/2))
  if(i < 3){
    df$v <- v1(n = df$n, sig = df$sig, lambda1 = lambda, c1 = C)
  } else {
    df$v <- v2(n = df$n, sig = df$sig, lambda2 = lambda, c2 = C)
  }
  df$p <- df$v + lambda*df$n + C
  
  opt1 <- df[which.max(df$v*(df$sig == 1)),]
  opt2 <- df[which.max(df$v*(df$sig == 1.3)),]
  
  gr <- expand.grid(n = seq(4, 500, l = 10),
                    p = seq(min(df$p), max(df$p), l = 10))
  gr$v <- gr$p - lambda*gr$n - C
  
  plots[[i]] <- ggplot(df, aes(n, p)) + geom_line(aes(colour=as.factor(sig))) +
    #geom_hline(yintercept = 0.8, linetype=2) +
    geom_contour(data = gr, aes(z = v), alpha = 0.5, colour = cols[3]) +
    geom_contour(data = gr, aes(z = v), breaks = c(C), colour = cols[4]) +
    geom_point(data = df[as.numeric(row.names(rbind(opt1, opt2))), ]) +
    xlab("Sample size") +
    scale_colour_manual(values=cols[1:2], labels=round(c(1, 1.3), 2)) +
    labs(colour = "SD") +
    theme_minimal()
  
  if(i < 3){
    plots[[i]] <- plots[[i]] + ylab("Power")
  } else {
    plots[[i]] <- plots[[i]] + ylab("Precision")
  }
}

p1 <- (plots[[1]] + plots[[3]]) /(plots[[2]] + plots[[4]]) + plot_layout(guides = "collect") & theme(legend.position = 'bottom')
p1
ggsave("./paper/figures/p1.pdf", p1, width = 18, height = 12, units="cm")
```

This figure shows the indifference contours in each case. Power (left column) and precision (right column) as a function of $n$ is shown for two possible values of the nuisance parameter $\sigma$ (1 and 1.3) and an MCID of 0.3, and the optimal (value-maximising) sample size is highlighted for each of these specific cases. The pink line denotes the contour below which it becomes optimal to not run the trial at all due to the setup costs.

The parameters $\lambda_1$ and $\lambda_2$ have been chosen so that the optimal design when $\sigma = 1$ is either $n = 176$ (top row) or $n = 110$ (bottom row). This corresponds to a power of 0.8  or 0.6, respectively.

Now consider how value varies with the true parameter $\sigma$. We are interested in three values here: i) value of the optimal (value maximising) $n$; ii) value of the minimax regret $n$; and iii) value of the s$n$ chosen to maximise value at a specific point estimate of $\sigma$. This needs to be done in the context of a specific point estimate and interval for $\sigma$.


```{r}
# First, choose a point estimate and construct a 95% CI around it
# corresponding to a certain pilot sample size
sig <- 1; k <- 2*30 - 2
lo <- sqrt(k*sig^2/qchisq(0.975, k)) 
up <- sqrt(k*sig^2/qchisq(0.025, k)) 

plots2 <- vector("list", 4)
for(i in 1:4){
  lambda <- c(lambda1, lambda2)[i]
  C <- c(c1, c2)[i]
  
  df <- expand.grid(sig = seq(0.6, 1.5, 0.1),
                    n = 2:500)
  if(i < 3){
    df$v <- v1(n = df$n, sig = df$sig, lambda1 = lambda, c1 = C)
  } else {
    df$v <- v2(n = df$n, sig = df$sig, lambda2 = lambda, c2 = C)
  }
  
  df_wide <- reshape(df, idvar = "sig", direction = "wide", timevar = "n", v.names = "v")
  
  # Get maximum value at each sigma over set of ns
  df2 <- df_wide[,1, drop = FALSE]
  v_opt <- apply(df_wide[,2:ncol(df_wide)], 1, max)
  
  # Transform value into regret
  df_wide_reg <- df_wide
  df_wide_reg[, 2:ncol(df_wide_reg)] <- v_opt -  df_wide_reg[, 2:ncol(df_wide_reg)]
  
  # Get minimax regret n index
  n_mm_ind <- which.min(apply(df_wide_reg[df_wide_reg$sig > lo & df_wide_reg$sig < up, 2:ncol(df_wide_reg)], 2, max))
  
  # Get minimax regret n  value at each sigma
  v_mm <- df_wide[, 1 + n_mm_ind]
  
  # Get n using point estimate sigma = 1
  n_s_ind <- which.max(df_wide[df_wide$sig == 1.0, 2:ncol(df_wide)])
  
  # Get simple n value at each sigma
  v_s <- df_wide[, 1 + n_s_ind]
  
  df3 <- data.frame(sig = rep(df_wide$sig, 3),
                    v = c(v_opt, v_mm, v_s),
                    t = c(rep(c("Optimal", "Minimax", "Simple"), each = length(df_wide$sig))))
  

  
  plots2[[i]] <- ggplot(df3, aes(sig, v, colour = t)) + geom_line() +
    scale_colour_manual(name = "", values = cols) +
    geom_vline(xintercept = c(lo, up), linetype = 2) +
    ylab("Value") + xlab("Standard deviation") +
    theme_minimal()
}
  
  

p2 <- (plots2[[1]] + plots2[[3]]) /(plots2[[2]] + plots2[[4]]) + plot_layout(guides = "collect") & theme(legend.position = 'bottom')
p2
ggsave("./paper/figures/p2.pdf", p2, width = 18, height = 12, units="cm")
```
For all cases, we find that the value of the optimal, simple, and minimax regret ns are pretty similiar over the interval estimate of $\sigma$. This suggests that a fixed design can be very robust to misspecification of $\sigma$ (since moving to the optimal n would not improve value by much), and that the simple fixed design is almost as good a choice as the minimax regret design.

This was for a specific choice of point and interval estimate of $\sigma$. We would like to see what happens when these vary - in particular, does the minimax regret n vary with them? We do this by making a grid of point estimates and degrees of freedom, and plotting the resulting minimax regret n.

```{r}
df <- expand.grid(sig = seq(0.6, 1.5, 0.02), # length = 46
                  k = seq(10, 100, 2))       # length = 46
df$lo <- sqrt(df$k*df$sig^2/qchisq(0.975, df$k)) 
df$up <- sqrt(df$k*df$sig^2/qchisq(0.025, df$k)) 
df$n <- df$n_ci <- df$n_nct <- NA

plots3 <- vector("list", 4)
plots3a <- vector("list", 4)
plots3b <- vector("list", 4)
plots4 <- vector("list", 4)
for(i in 1:4){
  lambda <- c(lambda1, lambda2)[i]
  cost <- c(c1, c2)[i]
  
  # Compute an n x sig matrix of values
  sigs <- seq(min(df$lo), max(df$up), l = 200)
  ns <- c(2:500) # TO ADD - start with 0 for not progressing
  
  vs <- matrix(rep(NA, length(sigs)*length(ns)), ncol = length(sigs))
  for(j in 1:nrow(vs)){
    for(k in 1:ncol(vs)){
      if(i < 3) {
        vs[j, k] <- v1(ns[j], sigs[k], lambda, cost)
      } else {
        vs[j, k] <- v2(ns[j], sigs[k], lambda, cost)
      }
    }
  }
  
  # Find index of scenario with point estimate 1 and degrees of freedom of 30:
  scn_ind <- which(df$sig == 1 & df$k == 58)
    
  
  # For each scenario (point and interval estimate), find the minimax 
  # regret n along with simple fixed and upper CI ns
  for(j in 1:nrow(df)){
    # Restrict value matrix to the interval
    vs_sub <- vs[, sigs >= df[j, "lo"] & sigs <= df[j, "up"]]
    # Get vector of optimal values
    opt_v <- apply(vs_sub, 2, max)
    # Get matrix of regrets (+ve is worse)
    regs <- -t(t(vs_sub) - opt_v)
    # Get vector of max regrets
    max_regs <- apply(regs, 1, max)
    # Find minimax regret n and the n which achieves it
    df$n[j] <- ns[which.min(max_regs)]
    df$r[j] <- min(max_regs)
    
    # Use the point estimate only
    sig <- df$sig[j]
    # Most relevant set of values
    vals <- vs[,which(sigs > sig)[1]]
    df$n_fix[j] <- ns[which.max(vals)]
    df$r_fix[j] <- max(regs[which(ns == df$n_fix[j]),])
  
    # Use the upper CI point, as in Browne
    sig <- df$up[j]
    # Get power of all ns
    pows <- v1(ns, sig, lambda, cost) + lambda*ns + cost
    # Choose lowest n with 80% power at upper ci, or maximum n if none 
    # satisfy that constraint
    if(sum(pows > 0.8) > 0){
      df$n_ci[j] <- ns[which(pows > 0.8)[1]]
      df$r_ci[j] <- max(regs[which(ns == df$n_ci[j]),])
    } else {
      df$n_ci[j] <- max(ns)
    }
    df$r_ci[j] <- max(regs[which(ns == df$n_ci[j]),])

    # Find the difference in maximum regret of the two methods, and translate
    # this into units of sample size.
    df$r_dif <- (df$r_ci - df$r)/lambda
        
    # Use the non-central t method of Julious
    #ns2 <- 3:1000
    #if(i == 1 | i == 2){
    #  z <- (2*qt(0.8, df$k[j], qt(0.975, 2*ns2 - 2))^2)*(df$sig[j]^2)/(0.3^2)
    #} else {
    #  z <- (2*qt(0.8, df$k[j], qt(0.975, 2*ns2 - 2))^2)*(df$sig[j]^2)/(0.3794427^2)
    #}
    #df$n_nct[j] <- ns2[ns2 >= z][1]
    

    if(j == scn_ind) {
      df2 <- data.frame(n = ns)
      df2$r <- max_regs
    }
  }
  print(range(df$n))
  print(range(df$n_ci - df$n))
  print(range(df$r_dif))
  
  # Minimax regret sample size
  plots3[[i]] <- ggplot(df, aes(k)) + geom_tile(aes(y = sig, fill = n)) +
    scale_fill_viridis(discrete=FALSE, limits = c(0,500)) + 
      #geom_line(data = df[df$sig == 1,], aes(y = up)) +
      #geom_line(data = df[df$sig == 1,], aes(y = lo)) +
      ylim(c(0.6, 1.5)) +
    ylab("Point estimate SD") + xlab("Degrees of freedom") +
    theme_minimal()
  
  # Difference in minimax and constrained n
  plots3a[[i]] <- ggplot(df, aes(k)) + geom_tile(aes(y = sig, fill = n_ci - n)) +
    scale_fill_viridis(discrete=FALSE, limits = c(-400,500)) + 
      #geom_line(data = df[df$sig == 1,], aes(y = up)) +
      #geom_line(data = df[df$sig == 1,], aes(y = lo)) +
      ylim(c(0.6, 1.5)) +
    ylab("Point estimate SD") + xlab("Degrees of freedom") +
    theme_minimal()
    
  # Difference in minimax and constrained regret, in units of sample size
  plots3b[[i]] <- ggplot(df, aes(k)) + geom_tile(aes(y = sig, fill = r_dif)) +
    scale_fill_viridis(discrete=FALSE, limits = c(0,400)) + 
      #geom_line(data = df[df$sig == 1,], aes(y = up)) +
      #geom_line(data = df[df$sig == 1,], aes(y = lo)) +
      ylim(c(0.6, 1.5)) +
    ylab("Point estimate SD") + xlab("Degrees of freedom") +
    theme_minimal()
  
  df3 <- data.frame(n = c(df$n[scn_ind], df$n_fix[scn_ind], df$n_ci[scn_ind]),
                    r = c(df$r[scn_ind], df$r_fix[scn_ind], df$r_ci[scn_ind]),
                    t = c("Minimax", "Simple", "Upper CI"))
  
  plots4[[i]] <- ggplot(df2, aes(n, r)) + geom_line() +
      geom_point(data = df3, aes(colour = t)) +
    scale_colour_manual(name = "", values = cols[c(1,3,4)]) +
    ylab("Maximum regret") + xlab("Sample size") +
    theme_minimal()
}

# Minimax regret sample size
p3 <- (plots3[[1]] + plots3[[3]]) /(plots3[[2]] + plots3[[4]]) + plot_layout(guides = "collect") & theme(legend.position = 'bottom')
p3
ggsave("./paper/figures/p3.pdf", p3, width = 18, height = 12, units="cm")
```

These plots show that if we use power in our value function, there is not a lot of variation in the minimax regret n over possible point and interval estimates. Most of the variation comes from the point estimate, suggesting that a more precise pilot estimate *at the same point* will not change our choice of $n$ considerably. This is shown in starker terms when using precision, where the minimax regret n now depends sharply on the point estimate (increasing as the point estimate decreases) but again is not sensitive to the interval around it.

The final thing to examine is how the maximum regret varies with n for a specific point and interval estimate, so we can show how the minimax regret n compares against the simple fixed n and alternatives, particularly the n chosen using the upper end of the interval. Code is embedded in the above to avoid replication.

```{r}
p4 <- (plots4[[1]] + plots4[[3]]) /(plots4[[2]] + plots4[[4]]) + plot_layout(guides = "collect") & theme(legend.position = 'bottom')
p4
ggsave("./paper/figures/p4.pdf", p4, width = 18, height = 12, units="cm")
```

We find again that, for this specific point and interval example (where $\hat{\sigma} = 1$ and $k = 58$), the simple fixed n (where we maximise value based on the point estimate) is very close or identical to the minimax regret n and thus has a similar or identical maximum regret. This holds in all scenarios. In comparison, the upper CI n (where we choose the smallest n giving 80% power using the upper 95\% CI as the estimate) is always much higher and has a much larger maximum regret.

Also embedded in the above code are plots comparing the minimax approach and the upper CI approach. First we plot the difference in the recommended sample sizes, and then look at the difference in maximum regret attained at these sample sizes (in units of sample size).

```{r}
# Difference in minimax and constrained n
p3a <- (plots3a[[1]] + plots3a[[3]]) /(plots3a[[2]] + plots3a[[4]]) + plot_layout(guides = "collect") & theme(legend.position = 'bottom')
p3a

# Difference in minimax and constrained regret, in units of sample size
p3b <- (plots3b[[1]] + plots3b[[3]]) /(plots3b[[2]] + plots3b[[4]]) + plot_layout(guides = "collect") & theme(legend.position = 'bottom')
p3b
```

These plots show firstly that the upper CI method always gives a higher n than the power-value approach, but can give a lower n than the precision-value approach. The differences can be very large, over the scenarios we have considered. Obviously the value-based methods always have lower maximum regret, but there are regions of the scenario space where the difference is small (particularly when the point estimate is small, meaning that the upper CI does not overinflate the sample size by too much).

[It might be useful to give a table of (minimax regret) optimal sample sizes for a range of scenarios. One approach is to determine value function parameters such that the (value) optimal sample size is typical in some way. For example, we could find a value function corresponding to choosing 80% power to detect a standardised effect size of 0.3. But this is tricky now since we've added a setup cost, so no unique solution. Better to just provide the software to find the n for any given scenario.]

## Uncertain effect size

Our approach gives us a principled way to design a trial when there is uncertainty in the nuisance parameter. Can we extend these ideas to deal with uncertainty in the effect? 

One approach is to change our value function to one focussed on expected effectiveness, rather than on power (or precision). That is, we assume a linear trade-off between sample size and power $\times$ effect. We then proceed as before, but now we have a 2D parameter space to do our minimax regret operations over. We may find that the choice of n is not as robust to movement in this space.

If we think about effect and power as being two attributes, we are assuming a very specific value structure between these bu including only their interaction. This implies that effect **alone** is worthless - it must be enabled by at least some power. Similarly, power is useless unless we have a positive effect to discover. 

```{r}
# Value function based on expected effectiveness
v3 <- function(n, sig, delta, lambda3, c3){
  pow <- 1-pt(qt(0.975, 2*(n-1)), 2*(n-1), delta/sqrt(2*sig^2/n))
  prob_use <- 1 # exp(10*(delta - 0.21))/(1 + exp(10*(delta - 0.21)))
  delta*pow*prob_use - lambda3*n - c3
}

# Define space to get value over - corresponds to a specific interval estimates
sig <- 1; delta <- 0.3;  k <- 2*30 - 2; n_p <- 30
alpha <- 0.975
lo <- sqrt(k*sig^2/qchisq(alpha, k)) 
up <- sqrt(k*sig^2/qchisq(1-alpha, k)) 

deltas <- seq(delta - qt(alpha, k)*sig/sqrt(n_p/2), delta + qt(alpha, k)*sig/sqrt(n_p/2), length.out = 10^2)
sigs <- seq(lo, up, length.out = 10^2)
ns <- 2:300

df <- expand.grid(d = deltas,
                  s = sigs,
                  n = ns)

# Calculate value at each point for one example choice of lambda,
# chosen so that the optimal design at the point estimate sigma = 1
# and mcid delta = 0.3 has 80% power.
#lambda3 <- 0.00067; c3 <- lambda3*n_cost
lambda3 <- 0.000365; c3 <- lambda3*n_cost
#which.max(v3(ns, 1, 0.3, lambda3, c3)) + 1
df$v  <- v3(df$n, df$s, df$d, lambda3, c3)

# Reshape into params x n format
df2 <- reshape(df, direction = "wide", idvar = c("d", "s"), timevar = "n", v.names = "v")

# Optimal n at each parameter point [analogous to second plots in 
# running example]
df3 <- df2[,1:2]
df3$v <- apply(df2[,3:ncol(df2)], 1, max)

ggplot(df3, aes(d, s)) + geom_tile(aes(fill = v)) +
    scale_fill_viridis(discrete=FALSE) +
    ylab("Standard deviation") + xlab("Effect size") +
    theme_minimal()
```

We see here that value is overwhelmingly determined by the true effect size, and not by the true standard deviation. This is not surprising since the former comes in directly and through power in the values function.

Now, find the simple design which takes the point estimates and chooses the n which maximises value at that point.


```{r}
# Get a vector of values over ns
vs <- delta*(1-pt(qt(0.975, 2*(ns-1)), 2*(ns-1), delta/sqrt(2*sig^2/ns))) - lambda3*ns - c3
fix_n <- ns[which.max(vs)]
df4 <- df2[,c(1,2, 1 + which.max(vs))]
# Print simple n and its power at point estimates
fix_n
1-pt(qt(0.975, 2*(fix_n-1)), 2*(fix_n-1), 0.3/sqrt(2*sig^2/fix_n))
```

Compare this against the n chosen to minimise the maximum regret over the given interval estimates.

```{r}
df5 <- df2 # Contains values for each point point in parameter space and each n
# For all ns, subtract value from that of best choice to give regrets
df5[,3:ncol(df5)] <- df3$v - df5[,3:ncol(df5)]
# For all ns, find maximum regret over all points. This returns a vector where
# ech entry corresponds to an n and gives max regret.
max_reg <- apply(df5[,3:ncol(df5)], 2, max)
# Find n which minimaxes regret
mm_reg_n <- ns[which.min(max_reg)]
# Print minimax regret n and its power at the point estimate
mm_reg_n
1-pt(qt(0.975, 2*(mm_reg_n-1)), 2*(mm_reg_n-1), 0.3/sqrt(2*sig^2/mm_reg_n))

# Plot regret of minimax regret n at each parameter point
df5$v <- df2[,1 + which.min(max_reg)]

# Plot regret for this n which minimises max regret
ggplot(df5, aes(d)) + geom_tile(aes(y = s, fill = df3$v - v)) +
    scale_fill_viridis(discrete=FALSE, name = "Regret") +
    xlab("Effect size") + ylab("Standard deviation") +
    theme_minimal()
```
We see that for this minimax regret n, regret is maximised at about 0.05 (in units of expected effectiveness) and this generally happens when we have a low effect size (when n will be too large), but also for a medium effect and a high sd (when n will be too small).

Now we can plot the maximum regret as a function of n, and point out where the alternative points (the simple choice) can be highlighted. Note that we don't use the upper CI approach here - would need to think about how this would actually be used in practice but since we generally know that powering off an estimated effect size is a bad idea, might be a straw man.

```{r}
df6 <- data.frame(n = ns,
                  r = max_reg)

df7 <- data.frame(n = c(mm_reg_n, fix_n),
                    r = c(max_reg[mm_reg_n - 1], max_reg[fix_n - 1]),
                    t = c("Minimax", "Simple"))

ggplot(df6, aes(n, r)) + geom_line() +
  geom_point(data = df7, aes(colour = t)) +
  scale_colour_manual(name = "", values = cols[c(1,3)]) +
  ylab("Maxmimum regret") +
  theme_minimal()

df7
```

In this case, the simple value-maximising choice gives us a maximum regret almost twice the size of the optimal n.

Big difference here is that the simple fixed n (the one maximising value at the point estimates) is now quite larger than the minimax regret n. Since the value function parameters were chosen to give the former a power of 0.9, this means we have a minimax regret n which gives a power at that point of only 0.68. So, we choose to have worse local value at the point estimate, to get improved worst-case performance over the interval estimate. If it seems undesirable, we could change our interval to make it smaller.

Returning to the basic premise, we have set up a value function which is linear the the true treatment effect if power is constant. The potential problem is that this ignores the importance aspect of the effect to patients / clinicians - the rationale behind using an MCID as a point which would actually change practice. We have explored this by including in our value function a probability of uptake, which we have taken as a known function of the true effect. We have chosen to centre this at $\delta = 0.21$, so that there is a 0.5 chance of uptake at this value. This corresponds to the point at which the optimal design at the point estimate of $\sigma = 1$ (n = 176) has 50% power. This introduces a new problem - now we have some unknown adoption function which we need to hypothesise, so to follow through with the general motivation we should think about a space of these and do maximin regret, since it is an unknown and not a value judgement. So perhaps not a helpful extension? Or work through the details of parametrising this uptake function and maximin regret over that too - but probably too much for one manuscript?


# Superceded

## Binary cRCT


Rutterford2015

To illustrate the general approach being applied to a different and more complex problem, consider a cRCT with a binary outcome. We will use the power value function and consider a specific scenario characterised by point and interval estimates for the two nuisance parameters and specific value function parameters. Then go straight to calculating the max regret for a range of n, and highlight the simple and the upper CI choices here too. Do the rest, but keep for the SM?


```{r}
pow <- function(m, n, var_b, p_0)
{
  # Large sample power
  p_1 <- p_0 - 0.2
  1 - pnorm(qnorm(0.975) - 0.2/sqrt((2*var_b + p_0*(1 - p_0)/n + p_1*(1 - p_1)/n)/m))
}

v <- function(m, n, p, lambda, gamma)
{
  # Value function
  -(log(p*gamma + 1)/log(gamma + 1) - lambda[1]*m - lambda[2]*m*n)# - lambda[1]*4*(m > 0))
}

pow(m = 21, n = 7, var_b = 0.021, p_0 = 0.7)
lambda <- grad(function(z) pow(z[1], z[2]/z[1], var_b = 0.021, p_0 = 0.7), c(21,7))
lambda <- 0.63*c(0.013839575, 0.0008678032)
gamma <- 2
  
df <- expand.grid(m = 2:50, n = c(7))
df$p_0 <- 0.7
df <- rbind(df, df)
  
df$var_b <- c(rep(0.021, nrow(df)/2), rep(0.042, nrow(df)/2))
df$p <- pow(df$m, df$n, df$var_b, df$p_0)

df$v <- v(df$m, df$n, df$p, lambda, gamma)
  
opt1 <- df[which.min(df$v - 10000*(df$n == 7 & df$var_b == 0.021)),]
opt2 <- df[which.min(df$v - 10000*(df$n == 7 & df$var_b == 0.042)),]
opt1
  
gr <- expand.grid(m = seq(2, 50, l = 10),
                  p = seq(0, 1, l = 10))
gr$v <- v(gr$m, n = 7, gr$p, lambda, gamma)
  
ggplot(df, aes(m, p)) + 
  geom_line(aes(colour=as.factor(var_b))) +
    geom_hline(yintercept = 0.8, linetype=2) +
    geom_contour(data = gr, aes(z = v), alpha = 0.5, colour = cols[3]) +
    geom_contour(data = gr, aes(z = v), breaks = c(0), colour = cols[4]) +
    geom_point(data = df[as.numeric(row.names(rbind(opt1, opt2))), ]) +
    ylab("Power") + xlab("Sample size") +
    scale_colour_manual(values=cols[1:2]) +
    labs(colour = expression(sigma[b]^2)) +
    theme_minimal()
```

```{r}
df <- expand.grid(p_0 = seq(0.2001, 1, l=100),
                  var_b = seq(0.0001, 0.5, l = 100))

sols <- expand.grid(m = 2:40,
                    n = 2:70)

opt_sols_ids <- apply(df, 1, function(z) which.min(v(sols$m, sols$n, pow(sols$m, sols$n, as.numeric(z[2]), as.numeric(z[1])), lambda, gamma)))

df$m <- sols[opt_sols_ids, "m"]
df$n <- sols[opt_sols_ids, "n"]
df$v <- v(df$m, df$n, pow(df$m, df$n, df$var_b, df$p_0), lambda, gamma)

# Get fixed design for later, using point estimates in the middle of the intervals
fixed <- df[df$p_0 > 0.7 & df$var_b > 0.025,][1, 3:4]


plot1 <- ggplot(df, aes(p_0, var_b, fill = m)) + geom_tile() +
  theme_minimal() +
  scale_fill_gradient(low="white", high=cols[4]) +
  xlab("Control probability") + ylab("Between cluster variance") +
  labs(fill = "m")

plot2 <- ggplot(df, aes(p_0, var_b, fill = n*m)) + geom_tile() +
  theme_minimal() +
  scale_fill_gradient(low="white", high=cols[5]) +
  xlab("Control probability") + ylab("Between cluster variance") +
  labs(fill = "m x n")

plot1 + plot2
```
Find the minimax regret design, plot maximum regret.

```{r}
p_0_int <- c(0.6, 0.8)
var_b_int <- c(0.01, 0.04)

get_reg <- function(x, m, n)
{
  var_b <- x[1]; p_0 <- x[2]
  # Get locally optimal design
  sols <- expand.grid(m = 2:50,
                    n = 2:70)

  l_opt_id <- which.min(v(sols$m, sols$n, pow(sols$m, sols$n, var_b, p_0), lambda, gamma))
  l_m <- sols[l_opt_id, "m"]; l_n <- sols[l_opt_id, "n"]
  l_opt_v <- v(l_m, l_n, pow(l_m, l_n, var_b, p_0), lambda, gamma)
  
  - v(m, n, pow(m, n, var_b, p_0), lambda, gamma) + v(l_m, l_n, pow(l_m, l_n, var_b, p_0), lambda, gamma)
}

get_max_reg <- function(x)
{
  m <- x[1]; n <- x[2]
  optim(c(0.02, 0.7), get_reg,
        lower = c(var_b_int[1], p_0_int[1]), upper = c(var_b_int[2], p_0_int[2]),
        method = "L-BFGS-B",
        m = m, n = n)$value
}

sols$max_r <- apply(sols, 1, get_max_reg)
sols_points <- sols[which.max(sols$max_r),]
sols_points <- rbind(sols_points, sols[sols$m == fixed$m & sols$n == fixed$n,])
sols_points$t <- c("Minimax", "Fixed")

ggplot(sols, aes(m, n)) + 
  geom_tile(aes(fill = max_r)) + 
  geom_contour(aes(z = max_r)) +
  geom_point(data = sols_points, aes(shape = t)) +
  scale_fill_gradient(low="white", high=cols[5]) +
  theme_minimal() +
  labs(fill = "Max regret",
       shape = " ")
```

Plot regret for the optimal design.

```{r}
p_0_int <- c(0.6, 0.8)
var_b_int <- c(0.01, 0.04)

mm_opt <- sols_points[1,1:2]

df <- expand.grid(var_b = seq(0.0001, 0.05, l=100),
                  p_0 = seq(0.5, 0.9, l=100))
df$reg <- apply(df, 1, function(z) get_reg(z[1:2], m = mm_opt$m, n = mm_opt$n))

ggplot(df, aes(p_0, var_b, fill = reg)) + geom_tile() + 
#ggplot(df, aes(p_0, var_b, z = reg)) + geom_contour() + 
  geom_rect(xmin = 0.6, xmax = 0.8, ymin  = 0.01, ymax = 0.04, fill = NA, colour = "black", linetype = 2) +
  xlab("Control probability") + ylab("Between cluster variance") +
  labs(fill = "Regret") +
  theme_minimal()
```



## Running example

Value function is
$$
v(n, \beta) = \beta + \lambda n.
$$
```{r}
value <- function(x, lambda)
{
  n <- x[1]; pow <- x[2]
  pow - lambda*n
}
```


Plotting value for a two-sample t-test over a range of $n$:
```{r, echo=F}
sliderInput("lambda", "Choose lambda", 0, 0.1, 0.025, step=0.01, animate = T)
sliderInput("sd", "Choose sd", 0.5, 2, 1, step=0.1)

renderPlot({

  df <- expand.grid(n=2:50, pow=1)
  df$pow <- sapply(df$n, function(n) power.t.test(n=n, delta=1, sd=input$sd)$power)
  df$v <- apply(df, 1, value, lambda = input$lambda)

  opt <- df[which.max(df$v),]
  
  const <- data.frame(n=ceiling(power.t.test(delta=1, sd=input$sd, power=0.8)$n),
                      pow=0.8)

  # value contours
  df2 <- expand.grid(n=2:50, pow=seq(0,1,0.1))
  df2$v <- apply(df2, 1, value, lambda=input$lambda)

  ggplot(df2, aes(n, pow)) + geom_contour(aes(z=v, colour=..level..)) +
    geom_point(data=df) + geom_point(data=opt, colour=cols[1]) +
    #geom_abline(slope=input$lambda, intercept = opt$p - input$lambda*opt$n, colour="red") +
    geom_segment(aes(x=opt$n, y=opt$pow, xend=opt$n, yend=0), colour=cols[1], linetype=2) +
    geom_point(data=const, colour=cols[2]) +
    geom_segment(aes(x=const$n, y=const$pow, xend=const$n, yend=0), colour=cols[2], linetype=2) +
    ylab("Power") + theme_minimal()
  
})
```

```{r, echo=F}
lambda <- 0.025
df <- expand.grid(n=2:50, pow=1)
df$pow <- sapply(df$n, function(n) power.t.test(n=n, delta=1, )$power)
df$v <- apply(df, 1, value, lambda = lambda)

opt <- df[which.max(df$v),]

# value contours
df2 <- expand.grid(n=2:50, pow=seq(0,1,0.05))
df2$v <- apply(df2, 1, value, lambda=lambda)

ggplot(df2, aes(n, pow)) + geom_contour(aes(z=v, colour=..level..)) +
  geom_point(data=df) + geom_point(data=opt, colour="red") +
  geom_abline(slope=lambda, intercept = opt$p - lambda*opt$n, colour="red") +
  ylab("Power") + theme_minimal()

#ggsave("./paper/figures/ex1_tangent.pdf", height=9, width=14, units="cm")
```

As the figure suggests, if our assumption about the form of the value function holds then we can determine the value of $\lambda$ by simply plotting the power curve and choosing $n$; $\lambda$ is then the gradiant of the tangent of the power curve at that point. Is this an accurate representation of our preferences? Do we aggree with its implications? Note that, for example, $\lambda = 0.025$ implies that $(n=0, \beta=0) \sim (n=40, \beta=1)$, and so 40 is the maximum sample size we would ever consider. When it comes to determining $\lambda$, we could use any hypothetical "power" curve and if our assumptions hold we should always choose the sample size that gives the same tangent gradient. For example, consider two other power curves obtained by changing the standard deviation from 1 to 0.7 and 1.3:

```{r}

df3 <- expand.grid(n=2:50, pow=1)
df3$pow <- sapply(df3$n, function(n) power.t.test(n=n, delta=1, sd=0.7)$power)
df3$v <- apply(df3, 1, value, lambda = lambda)
opt3 <- df3[which.max(df3$v),]

df4 <- expand.grid(n=2:50, pow=1)
df4$pow <- sapply(df4$n, function(n) power.t.test(n=n, delta=1, sd=1.3)$power)
df4$v <- apply(df4, 1, value, lambda = lambda)
opt4 <- df4[which.max(df4$v),]

ggplot(df2, aes(n, pow)) + geom_contour(aes(z=v, colour=..level..)) +
  geom_point(data=df) + geom_point(data=opt, colour="red") +
  geom_abline(slope=lambda, intercept = opt$p - lambda*opt$n, colour="red") +
  
  geom_point(data=df3, shape=15) + geom_point(data=opt3, colour="red") +
  geom_abline(slope=lambda, intercept = opt3$p - lambda*opt3$n, colour="red") +
  
  geom_point(data=df4, shape=17) + geom_point(data=opt4, colour="red") +
  geom_abline(slope=lambda, intercept = opt4$p - lambda*opt4$n, colour="red") +
  
  geom_hline(yintercept = 0.8, linetype=2) +
  
  ylab("Power") + theme_minimal()

#ggsave("./paper/figures/ex1_3tangents.pdf", height=9, width=14, units="cm")

cbind(rbind(opt3, opt, opt4), sd=c(0.7, 1, 1.3))
```

In the above we have proposed a _normative_ model for choosing the sample size of a trial, and argured that it may also be a _descriptive_ model of what happens in practice, in which case there would be no benefit of using it as it will lead to the same decisions. However, the potential implications of our model are hinted at in the above example - when the true power function is not known, as will be the case whenever we are uncertain about a nuisance parameter value, using a value function to make decisions provides a unified approach which will ensure consistency. 

Contrast with a common method for dealing with nuisance parameter uncertainty, sample size re-estimation (SSR). Following on from the example above, suppose we guess that $\sigma = 1$ and then choose $n = 17$ to give 80\% power. Under the SSR approach, if we learn from an interim analysis that $\sigma = 1.3$ then we should inflate our sample size to $n = 27$ to maintain the same power. Under our value model, this behaviour is not internally coherent. We are now saying that an increase in power of `r power.t.test(n=27, delta=1, sd=1.3)$power - power.t.test(n=26, delta=1, sd=1.3)$power` justifies an increase in sample size from 26 to 27; but when we did our original calculations, we felt that the increase of `r power.t.test(n=18, delta=1, sd=1)$power - power.t.test(n=17, delta=1, sd=1)$power` obtained by moving from 17 to 18 was _not_ justified.

Another way to see the flaw in SSR is to consider an extreme scenrio, e.g. discovering that $\sigma = 10$. Following SSR we should then increase the sample size to `r power.t.test(delta=1, sd=10, power=0.8)$n`!! Of course in practice such a discover would mean the trial is terminated - but the SSR methods available do not provide any guidance on exactly when we should decide the inflation is simply too much. Only by explicitly incoprorating cost into the method can we avoid such problems. 

As suggested by the example above, an interesting property of our method when applied to a problem with normally distributed outcomes is that as $\sigma$ increases, the required sample size does not necessarily increase (in contrast with the SSR apporach). We can illustrate by extending the previous plot, showing the power curve for a larger range of $\sigma$ along with their optimal sample sizes:

```{r}
df <- expand.grid(n=2:70, sig=seq(0.3,2,0.05))
df$pow <- apply(df, 1, function(x) power.t.test(n=x[1], delta=1, sd=x[2])$power)
df$v <- df$pow - lambda*df$n

f <- function(n, sig)
{
  pow <- power.t.test(n=n, delta=1, sd=sig)$power
  -(pow - lambda*n)
}

opt <- data.frame(sig=seq(0.3,2,0.05), n=1)
opt$n <- sapply(opt$sig, function(x) optim(10, f, method="Brent", lower=2, upper=50, sig=x)$par)
opt$pow <- apply(opt, 1, function(x) power.t.test(n=x[2], delta=1, sd=x[1])$power)
opt$inter <- opt$pow - lambda*opt$n

tangent <- df[,1:2]
tangent$pow <- apply(tangent, 1, function(x, opt) opt[opt$sig == x[2],]$inter + lambda*x[1], opt=opt)

const <- data.frame(sig=seq(0.3,2,0.05), n=1)
const$n <- sapply(const$sig, function(s) power.t.test(delta=1, sd=s, power=0.8)$n)
const$pow <- 0.8

#ggplot(df, aes(n, pow, colour=sig, group=as.factor(sig))) + geom_line() +
#  geom_point(data=opt, colour="red") +
#  geom_vline(xintercept = max(opt$n), linetype=2, colour="red")

p <- ggplot(df, aes(n, pow)) + geom_line() +
  geom_point(data=opt, colour=cols[1]) + geom_line(data=tangent, colour=cols[1], linetype=2) +
  geom_point(data=const, colour=cols[2]) + geom_hline(yintercept = 0.8, colour=cols[2], linetype=2) +
  ylab("Power") + scale_y_continuous(breaks=seq(0,1,0.2), limits=c(0,1)) + theme_minimal() +
  transition_time(time=sig)
 
a_gif <- animate(p, rewind=T)

p2 <- ggplot(opt, aes(y=n)) + 
  geom_point(aes(x=sig), colour=cols[1], size=2) + 
  geom_point(data=const, aes(x=sig), colour=cols[2], size=2) +
  geom_line(data=data.frame(sig2=opt$sig, n=opt$n), aes(x=sig2), colour=cols[1]) +
  geom_line(data=data.frame(sig2=const$sig, n=const$n), aes(x=sig2), colour=cols[2]) +
  theme_minimal() +
  transition_time(time=sig)  

b_gif <- animate(p2, rewind=T)

a_mgif <- image_read(a_gif)
b_mgif <- image_read(b_gif)

new_gif <- image_append(c(a_mgif[1], b_mgif[1]))
for(i in 2:100){
  combined <- image_append(c(a_mgif[i], b_mgif[i]))
  new_gif <- c(new_gif, combined)
}

new_gif

#opt[which.max(opt$n),]
```

Clearly, the optimal sample size under our formulation is much less sensitive to the nuisance parameter. Can we choose a fixed sample size design, and thus avoid the logistic and practical difficulties associated with interim analyses? To start, we can look at the original design choice based on the best estimate of the nuisance parameter. We can calculate the value of that design accross a range of parameter values, and contrast with the value of the optimal design:

```{r}
lambda <- grad(function(n) power.t.test(n=n, delta=0.3, sd=1)$power, 235)

get_value <- function(n, sig, lambda)
{
  pow <- power.t.test(n=n, delta=0.3, sd=sig)$power
  -(pow - lambda*n)
}

opt <- data.frame(n= 22, sig=seq(0.3,3,0.05))

opt$n <- sapply(opt$sig, function(x) optim(10, get_value, method="Brent", lower=2, upper=500, sig=x, lambda=lambda)$par)
opt$v <- apply(opt, 1, function(x) get_value(x[1],x[2], lambda))

rob <- data.frame(n= 235, sig=seq(0.3,3,0.05))
rob$v <- apply(rob, 1, function(x) get_value(x[1], x[2], lambda))

ggplot(opt, aes(sig, -v)) + geom_line() +
  geom_line(data=rob, colour="red")
```

We find that more generally any reasonable choice of n will in general not be far from the optimal n in terms of our value function. For example, if our original design was for n=22 to give 90% power at sd=1, keeping that n means that for sd in the range 0.75 to 2.3 (approx) the difference is within 0.05, which recall is in units of power. So we can avoid SSR and be pretty confident that even if the sd is significantly underestimated, we are not losing much. Changing the scale, asking for 90 power initially for an effect of 0.3 rather than 1, leads to the same differences in values for optimal and fixed designs.

Plotting the differences in the optimal and fixed values for a range of sds, we see that one rationale for the choice of fixed design is to restrict attention to a specific range of sds and find that which minimises the maximal difference. This is similar to the approach in [@Breukelen2015], although there the design problem is to choose number of clusrees and cluster size given a fixed overall budget; and the value measure they use is just the variance of the estimate. SO our approach is quite different, since we provide a way to decide how much overall budget should be used, and as shown below the use of precision as a value measure is not helpful since it does not plateu. Note that work is extended in [@Breukelen2018] to allow for heterogeneity in the costs and variances in each arm. The minimax approach is, however, going to be very sensitive to the "plausable" range of sds. It might be easier to simply plot the value curves for a range of ns and qualitatively judge them.

An alternative optimality criterion for a fixed design is to define a tollerable deviation from the optimal design's value, and to choose the fixed design which lies within this margin for the largest region of the parameter space. Apply this to the above example:

```{r, eval=F}
coverage <- function(fixed, opt, diff, lambda)
{
  df <- data.frame(sig = opt$sig)
  df$n <- fixed
  df$v <- sapply(df$sig, function(x) get_value(fixed, x, lambda))
  df$d <- df$v - opt$v
  -sum(df$d <= diff)
}

opt <- data.frame(sig=seq(0.3,2,0.01))
opt$n <- sapply(opt$sig, function(x) optim(10, get_value, method="Brent", lower=2, upper=500, sig=x, lambda=lambda)$par)
opt$v <- apply(opt, 1, function(x) get_value(x[2],x[1], lambda))

diff <- 0.02
o <- psoptim(200, coverage, opt=opt, diff=diff, lambda=lambda,
              lower = 2, upper = 500)
```

A key question now is - how sensitive is this optimal fixed design to the choice of the value parameter lambda? Given that there may be some variability in its choice, both within and between decision makers, robustness in this regard would make the methodology quite attractive.


Note that for the same lambda, the optimal designs and their values will be constant. As we change the fixed sample size, we change the value of sd for which the fixed design is also the optimal. We see that as this sd value reduces, the fixed value curve becomes more discrepant. In the above example, if we thought an sd of 0.7 was most likely but used the same lambda, we would get a trial of 95% power and n=145. But when we compare a fixed design of n=145 against the optimal the largest difference is around 0.127 - much larger than the 0.05 before. So our approach looks like it won't work well if the plausable sd values correspond with a very high power. In conrast, if lambda is the same but we think the sd is higher at 1.5, the optimal design is n=345 (giving power 75%) and as a fixed design this is very robust around sd=1.5. So broadly speaking, the lambda gives the optimal designs and the maximum sd for which a trial can be considered, and a fixed design approach works better the closer our plausable sd rage is to that maximum (or equivalently, the lower the power of the locally optimal design at the best guess). 

## Example - t-test


```{r}
regret <- function(sig, n, lambda, gamma)
{
  # Get the optimal design for that sig
   opt_n <- which.min(v(3:500, pow(3:500, sig), lambda, gamma)) + 2
   opt_pow <- pow(opt_n, sig)
   opt_n2 <- opt_n*(v(opt_n, opt_pow, lambda, gamma) < 0)
   opt_pow <- opt_pow*(v(opt_n, opt_pow, lambda, gamma) < 0)
   v(opt_n2, opt_pow, lambda, gamma) - v(n, pow(n, sig), lambda, gamma)
}

minimax <- function(n, lim)
{
  -optim(1, regret, lower = lim[1], upper = lim[2], method = "Brent",
        n = n, lambda = lambda, gamma = gamma)$value
}

res <- NULL
for(sig in 0.3/c(0.3, 0.5)){
  # For sample sd estimates corresponding to standardised effect sizes
  # of 0.3, 0.5, 0.8
  for(j in 1:5){
    # degrees of freedom from which sample sd = 1 was obtained
    k <- c(10000, 100, 50, 30, 12)[j]
    # upper CI
    up <- sqrt(k*sig^2/qchisq(0.025, k)) 
    # lower
    lo <- sqrt(k*sig^2/qchisq(0.975, k)) 
      
    for(i in 1:nrow(params)){
      lambda <- params[i,1]; gamma <- params[i, 2]
      
      # minimax regret n
      n <- optim(100, minimax, lower = 3, upper = 500, method = "Brent",
            lim = c(lo, up))$par
      
      res <- rbind(res, c(n, up-lo, sig, i))
    }
      
      # Use the upper CI point, as in Browne
      n_ci <- which(pow(3:500, up) >= 0.8)[1] + 2
      
      res <- rbind(res, c(n_ci, up - lo, sig, 5))
      
      # Use the non-central t method of Julious
      ns <- 3:500
      z <- (2*qt(0.8, k, qt(0.975, 2*ns - 2))^2)*(sig^2)/(0.3^2)
      n_nct <- 2*ns[ns >= z][1]
      
      res <- rbind(res, c(n_nct, up-lo, sig, 6))
  }
}

df <- as.data.frame(res)
names(df) <- c("n", "w", "sig", "m")

ggplot(df, aes(w, n, linetype = as.factor(sig), colour = as.factor(m))) + geom_line() +
  scale_colour_manual(name = "Method", values = cols,
                      labels = c("Value (i)",
                                 "Value (ii)",
                                 "Value (iii)",
                                 "Value (iv)",
                                 "Upper CI",
                                 "NCT")) +
  xlab("Confidence interval width") +
  theme_minimal()
```

So, for the four value functions we've looked at, the precision of the SD estimate makes a negligible difference to the minimaxed sample size. For both cases (an sd estimate of 1 and 0.6), less precision leads to a slight reduction in the sample size for all four example value functions. This contrasts with the upper CI and NCT methods, which both increase n as precision worsens.

This suggests that there is little to be gained from following the outlined procedure - eliciting the value function and minimaxing the regret over an interval - compared to simply using the point estimate and maximising value locally. The final sample size will not be much different.

Plot the value functions of the locally optimal designs and that of the optimal design for the point estimate, when the point estimate is 1.
```{r}
sig <- 1

comp_vals <- function(sig, n, lambda, gamma)
{
  # Get the optimal design for that sig
   opt_n <- which.min(v(3:500, pow(3:500, sig), lambda, gamma)) + 2
   opt_pow <-  pow(opt_n, sig)
   opt_n2 <- opt_n*( v(opt_n, opt_pow, lambda, gamma) < 0)
   opt_pow <- opt_pow*( v(opt_n, opt_pow, lambda, gamma) < 0)
   c(v(opt_n2, opt_pow, lambda, gamma), v(n, pow(n, sig), lambda, gamma))
}

plots3 <- vector("list", 4)
for(i in 1:nrow(params)){
  lambda <- params[i,1]; gamma <- params[i, 2]
  
  fix_n <- which.min(v(3:500, pow(3:500, sig), lambda, gamma)) + 2
  
  df <- data.frame(sig=seq(0.1,3,0.1))
  df <- cbind(df, t(sapply(df$sig, comp_vals, n = fix_n, lambda = lambda, gamma = gamma)))
  df2 <- data.frame(sig = df$sig,
                    value = df[,2] - df[,3])
  df <- melt(df, id.vars = "sig")
  
  plots3[[i]] <- ggplot(df, aes(sig, -value)) + geom_line(aes(colour = variable)) +
    geom_line(data = df2, linetype = 2) +
    scale_colour_manual(name = "", 
                        values = cols,
                        labels = c("Local", "Fixed")) +
    xlab("Standard deviation") + ylab("Value") +
    ylim(c(-0.5, 0.75)) +
    theme_minimal()
}

(plots3[[1]] + plots3[[2]]) /(plots3[[3]] + plots3[[4]])
```

In some cases, we see that there is very little difference in value between the fixed and the locally optimal design over a good range of SDs.

Now, consider the SSR problem. We have some estimate and interval of $\sigma$, and we are considering running a pilot trial to get a better one. This new estimate will thn be used to determine the main trial sample size, using the approach above. Now, the value of this whole process is uncertain since the pilot outcome is uncertain. We then need to move from value (deterministic) to utility (stochastic). We will assume we are risk-neitral, so these are the same thing and we can focus on expected value. To calculate this, we need to integrate over the pilot sampling distribution. We do this by first fidning the interval over which the optimal main sample size is not 0, and integrating over that. This makes things numerically stable, as it avoids discontinuities. We can speed things up by finding the optimal n for sigs and then model with a gam, so we can pull these optimal ns out repeatedly over the simulations in a vectorised way.

```{r}
exp_val_def<- function(n_p, lambda, gamma, w, sig, lim, g, fit)
{
  # Expected value of program with pilot sample size n_p, value params
  # lambda and gamma, and relative sampling cost w (low double means cheap pilot)
  
  # Shape and scale params of the sample variance dist
  k <- (n_p-1)/2; theta <- 2*sig^2/(n_p-1)
  
  # Transform quad nodes
  to_eval <- 0.5*(lim - 0.1^2)*g$nodes + 0.5*(lim + 0.1^2)
  
  # Find optimal ns for those variances
  ns <- predict(fit, newdata = data.frame(sig = sqrt(to_eval)))
  
  # Get overall value at each point, weighted by the sampling distribution
  vs <- v(ns, pow(ns, sig), lambda, gamma)*dgamma(to_eval, shape = k, scale = theta)
  
  # Calculate the integral as the weighted sum of these values
  # and add the penalty for the pilot sample size
  sum(g$weights*vs*0.5*(lim - 0.1^2)) + lambda*w*n_p #+ 0.3365181*w
}

n_p <- 15; w <- 1; sig <- 1.2

lambda <- params[i, 1]; gamma <- params[i, 2]

# Get the internal definite integral
ptm <- proc.time()
g <- gauss.quad(1000,"legendre")
exp_val_def(n_p, lambda, gamma, w, sig, lims[i], g, fits[[i]])
proc.time() - ptm

# Check using MC - simulate from sampling dist and take mean value
ptm <- proc.time()
k <- (n_p-1)/2; theta <- 2*sig^2/(n_p-1)
to_eval <- rgamma(10^5, shape = k, scale = theta)
to_eval <- to_eval[to_eval <= lims[i] & to_eval >= 0.1^2]
ns <- predict(fits[[i]], newdata = data.frame(sig = sqrt(to_eval)))
vs <- v(ns, pow(ns, sig), lambda, gamma) + lambda*w*n_p
mean(vs)
proc.time() - ptm
```


For each of the four value functions, we can plot the expected value for a range of $n_p$ and a number of weights, conditional on some true value of $\sigma$:

```{r}
sig <- 0.8
g <- gauss.quad(200,"legendre")
plots4 <- vector("list", 4)
for(i in 1:nrow(params)){
  print(i)
  lambda <- params[i, 1]; gamma <- params[i, 2]
  
  df <- expand.grid(n_p = 3:50,
                   w = c(0.2, 0.5, 0.8, 1))

  df$v <- apply(df, 1, function(x) exp_val_def(x[1], lambda, gamma, x[2], sig,
                                               lims[i], g, fits[[i]]))
  
  df2 <- data.frame(n_p = c(5, 15, 30, 50))
  df2$lo <- sqrt(df2$n_p*1^2/qchisq(0.975, df2$n_p))
  df2$up <- sqrt(df2$n_p*1^2/qchisq(0.025, df2$n_p))
  
  fix_ns <- apply(df2, 1, function(x) optim(100, minimax, lower = 3, upper = 500, method = "Brent", lim = c(x[2], x[3]))$par)
  df2$v <- v(fix_ns, pow(fix_ns, sig), lambda, gamma)

  plots4[[i]] <- ggplot(df, aes(n_p, -v)) + geom_line(aes(linetype = as.factor(w))) +
    geom_point(data = df2, colour = cols[1]) +
    theme_minimal() +
    xlab("Pilot sample size") + ylab("Expected value") +
    labs(linetype = "w")
}

(plots4[[1]] + plots4[[2]]) /(plots4[[3]] + plots4[[4]])
```

We can now define expected regret as the expected value of the pilot-main trial program, minus the value of the locally optimal main trial. This provides a basis for choosing $n_p$ - we want to minimise the maximum (worst-case) regret over our initial interval for $\sigma$.

```{r}
g <- gauss.quad(200,"legendre")
i <- 3
lambda <- params[i, 1]; gamma <- params[i, 2]
  
sigs <- seq(0.1, 3, 0.01)
n_ps <- c(3:30)
res <- matrix(rep(NA, length(sigs)*length(n_ps)), nrow = length(sigs))
for(j in 1:length(n_ps)){
  for(k in 1:length(sigs)){
    if(sigs[k] < sqrt(lims[i])){
      opt_n <- predict(fits[[i]], newdata = data.frame(sig = sigs[k]))
      opt_v <- v(opt_n, pow(opt_n, sigs[k]), lambda, gamma)
    } else {
      opt_n <- 0
      opt_v <- 0
    }
    res[k, j] <- exp_val_def(n_ps[j], lambda, gamma, 1, sigs[k], lims[i], g, fits[[i]]) - opt_v
  }
}
# Locally optimal n_p
plot(max.col(-res))

sigs <- seq(0.1, 3, 0.01)
ns <- 3:400
res2 <- matrix(rep(NA, length(sigs)*length(ns)), nrow = length(sigs))
for(j in 1:length(ns)){
  for(k in 1:length(sigs)){
    if(sigs[k] < sqrt(lims[i])){
      opt_n <- predict(fits[[i]], newdata = data.frame(sig = sigs[k]))
      opt_v <- v(opt_n, pow(opt_n, sigs[k]), lambda, gamma)
    } else {
      opt_n <- 0
      opt_v <- 0
    }
    res2[k, j] <- v(ns[j], pow(ns[j], sigs[k]), lambda, gamma) - opt_v
  }
}
# Locally optimal n
plot(max.col(-res2))

minmax_np <- function(est, k, sigs, res, res2)
{
  # upper CI
  up <- sqrt(k*est^2/qchisq(0.025, k))
  up <- head(which(sigs > up), 1)
  # lower
  lo <- sqrt(k*est^2/qchisq(0.975, k))
  lo <- tail(which(sigs < lo), 1)
  
  max_reg <- rep(NA, length(n_ps))
  for(j in 1:length(n_ps)){
    max_reg[j] <- max(res[lo:up,j])
  }
    
  max_reg2 <- rep(NA, length(ns))
  for(j in 1:length(ns)){
    max_reg2[j] <- max(res2[lo:up,j])
  }
  
  plot(res2[,which.min(max_reg2)])
  points(res[,which.min(max_reg)], col="red")
  
  if(min(max_reg) < min(max_reg2)){
    n_ps[which.min(max_reg)]
  } else {
    0
  }
}

minmax_np(1, 100, sigs, res, res2)
```



We can compare against Whitehead et al, who suggest optimal sample sizes for different estimated sds (or equivalently, standardised effect sizes). 




```{r}
MC_vs <- function(n_p)
{
  k <- (n_p-1)/2; theta <- 2*sig^2/(n_p-1)
  to_eval <- rgamma(50, shape = k, scale = theta)
  ns <- sapply(to_eval, function(x) which.min(v(3:1000, pow(3:1000, sqrt(x)), lambda, gamma)) + 2)
  v(ns + 1*n_p, pow(ns, sig), lambda, gamma)
}

n_ps <- NULL; vs <- NULL
for(j in 3:50){
  n_ps <- c(n_ps, j)
  vs <- c(vs, mean(MC_vs(j)))
}

plot(n_ps, vs)
```

We see that the optimal pilot size tends to 0, so the benefits are not justified by the cost of extra sampling. Tbis makes sense based on what we saw earlier - the width of the CI over which we do minimax regret doesnt have much effect on the chosen n; so reducing the width via a larger pilot will make no difference to the n we end up with. But, a variable pilot will gives us chaotic point estimates, and so more variation in the chosen n...



```{r}
# Re-estimation - for a true sd and a pilot sample of k = 6, get distribution of final sample size
v <- (c(0.7, 1, 1.3)*sig)^2
df <- data.frame(vs = rep(v, 10^3))
df$v_est <- df$vs*rchisq(10^3, 11)/11
df$n <- sapply(df$v_est, function(x) optim(10, fn=f, lower=2, upper=1000, sig=sqrt(x), lambda=lambda, method="Brent")$par)
df$p <- apply(df, 1, function(x) pow(x[3], sqrt(x[1])))
df$t <- "v"

df2 <- df[,1:2]
df2$n <- sapply(df2$v_est, function(x) power.t.test(delta=0.3, sd=sqrt(x), power=0.8)$n*2)
df2$p <- apply(df2, 1, function(x) pow(x[3], sqrt(x[1])))
df2$t <- "c"

df2 <- df[,1:2]
df2$n <- sapply(df2$v_est, function(x) power.t.test(delta=0.3, sd=sqrt(x), power=0.8)$n*2)
df2$p <- apply(df2, 1, function(x) pow(x[3], sqrt(x[1])))
df2$t <- "c"

df <- rbind(df, df2)

p1 <- ggplot(df, aes(as.factor(vs), n, fill=t)) + geom_boxplot() +
  theme_minimal() + ylab("Sample size") +
  scale_fill_manual(name="Method", values=cols[1:2], labels=c("Constrained", "Value-based")) +
  scale_x_discrete(name="Standard deviation", 
                   labels = c(expression(0.7*sigma), 
                              expression(sigma),
                              expression(1.3*sigma))) +
  theme(panel.grid.major.x = element_blank())

#ggsave("./paper/figures/n_dist_t_test.pdf", width = 15, height = 10, units="cm")
#ggsave("./paper/figures/n_dist_t_test.eps", width = 15, height = 10, units="cm", device = cairo_ps())
  
p2 <- ggplot(df, aes(as.factor(vs), p, fill=t)) + geom_boxplot() +
  theme_minimal() + ylab("Power") + 
  scale_y_continuous(breaks=seq(0,1,0.2)) +
  scale_fill_manual(name="Method", values=cols[1:2], labels=c("Constrained", "Value-based")) +
  scale_x_discrete(name="Standard deviation", 
                   labels = c(expression(0.7*sigma), 
                              expression(sigma),
                              expression(1.3*sigma))) +
  theme(panel.grid.major.x = element_blank())

p1 + p2

#ggsave("./paper/figures/p_dist_t_test.pdf", width = 15, height = 10, units="cm")
#ggsave("./paper/figures/p_dist_t_test.eps", width = 15, height = 10, units="cm", device = cairo_ps())

#ggpubr::ggarrange(p1, p2, ncol=2, nrow=1, common.legend = TRUE, legend="bottom")

#ggsave("./paper/figures/dists_t_test.pdf", width = 15, height = 10, units="cm")
#ggsave("./paper/figures/dists_t_test.eps", width = 15, height = 10, units="cm", device = cairo_ps())
```

For a number of pilot sample size choices, simulate pilot estimates, use these to get main n, compare value against fixed design based on an initial guess.

```{r}
g <- function(x)
{
  v <- (x[1]*sig)^2
  
  # Get fixed design based on sig
  n_fix <- optim(10, fn=f, lower=2, upper=1000, sig=sig, lambda=lambda, method="Brent")$par
  
  # Sample pilot estimates
  n_p <- x[2]
  v_est <- v*rchisq(10^3, (n_p - 1))/(n_p - 1)
  
  # Get optimal designs based on pilot estimates
  n <- sapply(v_est, function(x) optim(10, fn=f, lower=2, upper=1000, sig=sqrt(x), lambda=lambda, method="Brent")$par)
  
  # Calculate actual power and value
  a <- 1
  p <- pow(n, sqrt(v))
  val <- (p - lambda*(n + n_p))
  #val <- (1 - exp(-a*val))/a
  
  fix_val <- pow(n_fix, sqrt(v)) - lambda*n_fix
  #fix_val <- (1 - exp(-a*fix_val))/a
  
  c(#mean(val > fix_val), 
    mean(val), 
    fix_val)
}

df <- expand.grid(sig_prop = seq(0.5, 2, 0.05),
                  n_p = c(3, 6, 12, 30, 60))
df <- cbind(df, t(apply(df, 1, g)))
names(df)[3:4] <- c("value", "fixed")

ggplot(df, aes(sig_prop, colour=as.factor(n_p))) + geom_line(aes(y = value)) +
  geom_line(aes(y = fixed), linetype = 2)
```

```{r}
g <- function(n, lambda, delta, samp)
{
  # Given a sample of points in the nuisance parameter space and their optimal values,
  # evaluate a fixed design by seeing at how many of these points the fixed value
  # is within a delta tolerance of the optimal value
  sum( (samp[,2] - f(n, sig=samp[,1], lambda)) > -delta )
}

opt$v <- f(opt$n, opt$sig, lambda)

# Get samples, their optimal designs, and values
M <- 2000
samp <- data.frame(sig=runif(M, min(opt[,1]), max(opt[,1])))
samp$v <- apply(samp, 1, function(x) optim(10, fn=f, lower=2, upper=1000, sig=x, lambda=lambda, method="Brent")$value)

# Find the m which best evaluates according to g
delta <- 0.03
rob <- (4:100)[which.max(sapply(4:100, g, lambda=lambda, delta=delta, samp=samp))]
rob

# Get value of a fixed design for comparison
opt3 <- opt[,1:3]; opt3$n <- rob; opt3$t <- "f"
opt3$v<- f(n=rob, sig=opt3[,1], lambda)

d <- -opt$v + opt3$v

ggplot(rbind(opt, opt3), aes(sig)) + geom_line(aes(y=-v, colour=t)) +
  geom_line(data=opt3, aes(y=d), colour="black", linetype=2) +
  theme_minimal() +
  geom_hline(yintercept = delta, linetype=3) +
  scale_color_manual(name="Design", values=cols[3:4], labels=c("Fixed", "Optimal")) +
  ylab("Value") + xlab("Standard deviation")

#ggsave("./paper/figures/fixed_t_test.pdf", width = 15, height = 10, units="cm")
#ggsave("./paper/figures/fixed_t_test.eps", width = 15, height = 10, units="cm", device = cairo_ps())
```

```{r}
# Given a range of nuisnace parameter values, find the design with the minimax discrepancy over that range
h <- function(sig, n, lambda)
{
  v <- -optim(10, fn=f, lower=2, upper=1000, sig=sig, lambda=lambda, method="Brent")$value
  -(v + f(n, sig, lambda))
}

h2 <- function(n, lambda, sig_range, samp)
{
  sub <- samp[samp[,1] > sig_range[1] & samp[,1] < sig_range[2], ]
  vs <- f(n, sub$sig, lambda)
  optim(sub[which.max(-sub$v + vs), 1], fn=h, lower=sig_range[1], upper=sig_range[2], n=n, lambda=lambda, method="L-BFGS-B")$value
}

sig_range <- c(0.75*sig, 1.25*sig)
rob2 <- (4:50)[which.max(sapply(4:50, h2, lambda=lambda, sig_range=sig_range, samp=samp))]
rob2

h2(rob2, lambda=lambda, sig_range=sig_range, samp=samp)

# Get value of a fixed design for comparison
opt4 <- opt[,1:3]; opt4$n <- rob2; opt4$t <- "f"
opt4$v<- f(n=rob2, sig=opt4[,1], lambda)

d <- -opt$v + opt4$v

ggplot(rbind(opt, opt4), aes(sig)) + geom_line(aes(y=-v, colour=t)) +
  geom_line(data=opt4, aes(y=d), colour="black", linetype=2) +
  theme_minimal() +
  scale_color_manual(name="Design", values=cols[3:4], labels=c("Fixed", "Optimal")) +
  ylab("Value") + xlab("Standard deviation") +
  geom_vline(xintercept = sig_range, linetype=3)

#ggsave("./paper/figures/minimax_t_test.pdf", width = 15, height = 10, units="cm")
#ggsave("./paper/figures/minimax_t_test.eps", width = 15, height = 10, units="cm", device = cairo_ps())
```


For a given choice of sample size, there is a trade-off parameter corresponding to every possible MCID. We are willing to pay a higher rate for a larger MCID. For example, if we determine value based on 80\% power to detect MCID of 0.3 we get a robust design of 29, but if instead it is 96\% power to detect 0.4 our robust design is 70. Does this make sense? So, two people have the same optimal design conditional on a np estimate; but very different robust designs. That is, one is more sensitive to changes in the np than the other. 

First, note that this is not specific to our approach. under the constrained approach, the optimal design becomes more sensitive to the np as the threshold is increased. 

A more general framework would be that we want to simultaneously maximise power at all effect sizes greater than the true MCID where we want 50\%. We need to combine this continum of objectives into a single measure, e.g. through a weighted sum. In the continuous case, this is integrating with respect to some weighting function, where the weighting function integrates to 1, i.e. is a probability distribution. So, this is equivalent to maximising expected power with respect to a prior on the effect size. Since trunctaed at the true MCID, this could be interpreted as prior prob of trial success, given there is a true worthwhile effect.

```{r}
pow2 <- function(n, sig, mu)
{
  n <- n/2
  #power.t.test(n = n, delta = 0.3, sd = sig)$power
  1-pt(qt(0.975, 2*(n-1)), 2*(n-1), mu/sqrt(2*sig^2/n))
}

n <- 2*19; sig <- 0.3205898; mu <- 0.4
pow2(n, sig, mu=mu)

lambda <- grad(pow2, n, sig=sig, mu=mu)
```

## Example - cluster RCT, m and k

```{r}
clus_pow <- function(x, var_t, rho)
{
  k <- x[1]; n <- x[2]; m <- n/k
  clus_var <- var_t*rho + (var_t - var_t*rho)/m
  power.t.test(n=k, delta=0.3, sd=sqrt(clus_var))$power
}

# plot power function to find value params
df <- expand.grid(k=seq(10, 40, 5), n=seq(2,700,20))
df$pow <- apply(df, 1, clus_pow, var_t=1, rho=0.05)

ggplot(df, aes(n, pow, group=k, colour=k)) + geom_line() +
  geom_point(data=data.frame(k=15, n=470, pow=clus_pow(c(15, 470), 1, 0.05)), colour="red") +
  theme_minimal()

#ggsave("./paper/figures/cluster_pow.pdf", height=9, width=14, units="cm")

#des <- c(25, 450)
#des <- c(15, 470)
des <- c(25, 270)
des <- c(19, 19*18)
clus_pow(des, 1, 0.05)
lambda <- grad(clus_pow, des, var_t=1, rho=0.05)

f <- function(x, var_t, rho, lambda)
{
  pow <- clus_pow(x, var_t, rho)
  -(pow - sum(lambda*x))
}

opt <- expand.grid(var_t=seq(0.5, 1.5, 0.05), rho=seq(0.025, 0.1, 0.005))
opt <- cbind(opt, t(apply(opt, 1, function(x) optim(c(10,10), fn=f, lower=c(2,2), upper=c(50,1000), var_t=x[1], rho=x[2], lambda=lambda, method="L-BFGS-B")$par)))
names(opt)[3:4] <- c("k", "n")
opt$v <- apply(opt, 1, function(x) f(x[3:4],x[1],x[2], lambda))

ggplot(opt, aes(k,n, colour=rho, size=var_t)) + geom_point()
```

```{r}
g <- function(x, lambda, delta, samp)
{
  # Given a sample of points in the nuisance parameter space and their optimal values,
  # evaluate a fixed design by seeing at how many of these points the fixed value
  # is within a delta tolerance of the optimal value
  sum( (samp[,3] - f(x, var_t=samp[,1], rho=samp[,2], lambda)) > -delta )
}

# Get samples, their optimal designs, and values
M <- 5000
samp <- data.frame(var_t=runif(M, min(opt[,1]), max(opt[,1])), rho=runif(M, min(opt[,2]), max(opt[,2])))
samp$v <- apply(samp, 1, function(x) optim(c(10,10), fn=f, lower=c(2,2), upper=c(50,1000), var_t=x[1], rho=x[2], lambda=lambda, method="L-BFGS-B")$value)

# Find the m which best evaluates according to g
delta <- 0.03
eval <- data.frame(k=rep(4:50, each=29)); eval$n <- 2:30*eval$k
eval <- eval[eval$n < 600,]
rob <- eval[which.max(apply(eval, 1, g, lambda=lambda, delta=delta, samp=samp)),]
rob

g(as.numeric(rob), lambda, delta, samp)

# Get value of a fixed design for comparison
opt3 <- opt[,1:2]; opt3$k <- rob[[1]]; opt3$n <- rob[[2]]; opt3$t <- "f"
opt3$v<- f(x=as.numeric(rob), var_t = opt3[,1], rho = opt3[,2], lambda)

opt$d <- -opt$v + opt3$v

ggplot(opt, aes(var_t, rho, z=d, colour=as.factor(..level..))) + geom_contour(breaks=seq(0.01, 0.05, 0.01)) + #geom_contour(breaks=seq(0.01, 0.05, 0.005)) +
  theme_minimal() +
  ylab("Intracluster correlation") + xlab("Total variance") +
  scale_colour_discrete(name = "Difference in value")

#ggsave("./paper/figures/mult_design.pdf", width = 15, height = 10, units="cm")
#ggsave("./paper/figures/mult_design.eps", width = 15, height = 10, units="cm", device = cairo_ps())
```

## Example - survival

```{r}
# see chapter 9, D.Collet Modelling survival data in medical research
s_bar <- function(x, med1, med2)
{
  # Assuming exponential models with medians med1 and med2,
  # so S(x) = exp(-ln(2)x/med)
  (exp( -(log(2)/med1)*x ) + exp( -(log(2)/med2)*x ))/2
}
  
get_pow <- function(x, med1)
{
  # med1 - median survival control
  # x - (accrual time period, total trial time)
  # a - accrual time period
  # f - follow-up time period
  a <- x[[1]]; f <- max(x[[2]] - x[[1]], 0)
  dif <- 2
  med2 <- med1 + dif
  m <- 10.3 # monthly accrual rate
  theta <- log(med1/med2) # log harzard ratio
  
  # Expected sample size
  n <- m*a
  # Probability of an event
  prob_d <- 1 - (s_bar(f, med1, med2) + 4*s_bar(0.5*a + f, med1, med2) + s_bar(a + f, med1, med2))/6
  # Expected number of events
  d <- n*prob_d
  # Power
  pnorm( -theta*sqrt(d)/2 - qnorm(0.975) )
}

get_pow(c(23,48), 4.5)
lambda <- grad(get_pow, c(23,48), med1=4.5)
```

Finding the optimal design (i.e. that which maximises the value function) for any given value of the nuisance parameter:

```{r}
get_value <- function(x, med1, lambda)
{
  #if(x[1] < 1 | x[1] > 100 | x[2] < 1 | x[2] > 100) return(100000)
  pow <- get_pow(x, med1)
  
  -(pow - sum(lambda*x))
}

get_design <- function(med1)
{
  o <- optim(c(25,50), get_value, med1=med1, lambda=lambda,
              lower = c(0,0), upper = c(100,100), method="L-BFGS-B")
  return(c(o$par, -o$value))
} 

df <- data.frame(med1=seq(2, 8, 0.05))
df <- cbind(df, t(sapply(df$med1, get_design)))
names(df)[2:4] <- c("a", "t", "v")

df2 <- data.frame(med1=df$med1)
fixed <- c(21, 47)
df2$a <- fixed[1]; df2$t <- fixed[2]
df2$v <- -sapply(df2$med1, function(x) get_value(fixed, x, lambda))
df2$d <- df2$v - df$v

# Optimal designs
ggplot(df, aes(a, t-a, colour=med1)) + geom_point() +
  xlab("Accrual") + ylab("Follow-up") +
  theme_minimal()

# Optimal and fixed design values
ggplot(df2, aes(med1)) + geom_line(aes(y=v)) + geom_line(aes(y=-d), colour="blue") +
  geom_line(data=df, aes(y=v), colour="red") + 
  xlab("Median survival time") + ylab("Value") +
  theme_minimal()
```

The initial choice of design appears to generally be quite robust, and a visual inspection of the above plot might be enough to confirm its choice. But more generally, what metrics could we use to find the best fixed design? One obvious solution is to use a distribution on the nuisance parameter to give an expected value and maximise that, but we want to avoid any half Bayesian methods and stick with the frequentist paradigm here. Two possible approaches:

- Define a range of interest on the nuisance parameter and then choose the minimax design, i.e. that with the smallest maximum difference in value between fixed and optimal designs;
- Define a maximum tollerated difference in value between fixed and optimal designs, and choose that which gives the largest area of nuisance parameter space within this margin.

The first will be highly sensitive to the choice of range which we want to avoid, so let's examine the second approach.

```{r, eval=F}
coverage <- function(fixed, df, diff, lambda)
{
  df2 <- data.frame(med1=df$med1)
  df2$a <- fixed[1]; df2$t <- fixed[2]
  df2$v <- -sapply(df2$med1, function(x) get_value(fixed, x, lambda))
  df2$d <- df2$v - df$v
  -sum(-df2$d <= diff)
}

df <- data.frame(med1=seq(2, 8, 0.01))
df <- cbind(df, t(sapply(df$med1, get_design)))
names(df)[2:4] <- c("a", "t", "v")

diff <- 0.01
opt <- psoptim(c(35,50), coverage, df=df, diff=diff, lambda=lambda,
              lower = c(0,0), upper = c(100,100))#, method="L-BFGS-B")
```

## Stepped wedge

From Hooper et al. (2016).

```{r}
def_c <- function(m, rho)
{
  # Design effect due to clustering
  1 + (m - 1)*rho
}

def_r <- function(r, l, t, b, c, d)
{
  # Design effect due to repeated assessment 
  l^2 * (1 - r)*(1 + t*r)/(4*(l*b - d + (b^2 + l*t*b - t*d - l*c)*r))
}

m <- 36 # cluster size
rho <- 0.33; pi <- 0.9; sig <- 5; tau <- 0.7 # nuisance parameter estimates
l <- 3; t <- 3; k <- 4 # l=t= number of arms (ie rows) in stepped wedge design; k = number of clusters per arm
a <- matrix(c(0,0,0, 1,0,0, 1,1,0, 1,1,1), nrow = 3) # stepped wedge matrix
b <- sum(a); c <- sum(colSums(a)^2); d <- sum(rowSums(a)^2)
```

```{r}
# Repeated cross section
r_cc_rcs <- function(m, rho, pi)
{
  # Correlation in a repeated cross-section cluster design
  (m*rho*pi)/(1 + (m - 1)*rho)
}

pow_rcs <- function(mm, k, rho, pi, l, t, b, c, d, sig)
{
  # Effective individually randomised sample size
  eff_n <- l*k*mm/(def_r(r_cc_rcs(mm, rho, pi), l, t, b, c, d)*def_c(mm, rho))
  power.t.test(n = eff_n/2, delta = 2, sd = sig)$power
}

r_cc_rcs(m, rho, pi)

def_r(r_cc_rcs(m, rho, pi), l, t, b, c, d)

pow_rcs(m, k, rho, pi, l, t, b, c, d, sig)

lambda <- grad(pow_rcs, m, method="Richardson", k=k, rho=rho, pi=pi, l=l, t=t, b=b, c=c, d=d, sig=sig)

ms <- 2:45; ps <- pow_rcs(ms, k, rho, pi, l, t, b, c, d, sig)

f <- function(m, k, rho, pi, l, t, b, c, d, sig, lambda)
{
  # Value function
  p <- pow_rcs(m, k, rho, pi, l, t, b, c, d, sig)
  -(p - lambda*m)
}

# Find optimal m for various nuisance parameter values
opt <- expand.grid(rho=seq(0,0.7,0.01), pi=seq(0.7, 1, 0.01), sig=c(4,5,6,7))
opt <- cbind(opt, apply(opt, 1, function(x) optim(10, fn=f, lower=2, upper=1000, 
                                                    k=k, rho=x[1], pi=x[2], l=l, t=t, b=b, c=c, d=d, sig=x[3], lambda=lambda,
                                                    method="L-BFGS-B")$par))
names(opt)[4] <- c("m")
opt$v <- f(opt[,4], k, opt[,1], opt[,2], l, t, b, c, d, opt[,3], lambda)

ggplot(opt, aes(rho, pi, z=m, colour=..level..)) + geom_contour() +
  facet_wrap(.~sig)

g <- function(m, k, l, t, b, c, d, lambda, vs, delta, samp)
{
  # Given a sample of points in the nuisance parameter space and their optimal values,
  # evaluate a fixed design by seeing at how many of these points the fixed value
  # is within a delta tolerance of the optimal value
  sum( (vs - f(m, k, samp[,1], samp[,2], l, t, b, c, d, samp[,3], lambda)) > -delta )
}

# Get samples, their optimal designs, and values
M <- 2000
samp <- data.frame(rho=runif(M, min(opt[,1]), max(opt[,1])), 
                   pi=runif(M, min(opt[,2]), max(opt[,2])),
                   sig=runif(M, min(opt[,3]), max(opt[,3])))
samp$v <- apply(samp, 1, function(x) optim(10, fn=f, lower=2, upper=1000, 
                                                    k=k, rho=x[1], pi=x[2], l=l, t=t, b=b, c=c, d=d, sig=x[3], lambda=lambda,
                                                    method="L-BFGS-B")$value)

# Find the m which best evaluates according to g
rob <- (2:50)[which.max(sapply(2:50, g, k=k, l=l, t=t, b=b, c=c, d=d, lambda=lambda, vs=samp[,4], 
                        delta=0.05, samp=samp))]

# Get value of a fixed design for comparison
opt$v_fix <- f(m=rob, k, opt[,1], opt[,2], l, t, b, c, d, opt[,3], lambda)
ggplot(opt, aes(rho, pi, z=v - v_fix, colour=..level..)) + geom_contour(breaks=-c(0.01, 0.025, 0.05, 0.1)) +
  facet_wrap(.~sig)
```



```{r}
# Closed cohort
r_cc_clch <- function(m, rho, pi, tau)
{
  # Correlation in a closed cohort
  (m*rho*pi + (1 - rho)*tau)/(1 + (m - 1)*rho)
}

pow_clch <- function(mm, k, rho, pi, tau, l, t, b, c, d, sig)
{
  # Effective individually randomised sample size
  eff_n <- l*k*mm/(def_r(r_cc_clch(mm, rho, pi, tau), l, t, b, c, d)*def_c(mm, rho))
  power.t.test(n = eff_n/2, delta = 2, sd = sig)$power
}

m <- 10

r_cc_clch(m, rho, pi, tau)

def_r(r_cc_clch(m, rho, pi, tau), l, t, b, c, d)

pow_clch(m, k, rho, pi, tau, l, t, b, c, d, sig)

lambda <- grad(pow_clch, m, method="Richardson", k=k, rho=rho, pi=pi, tau=tau, l=l, t=t, b=b, c=c, d=d, sig=sig)

ms <- 2:45; ps <- pow_clch(ms, k, rho, pi, tau, l, t, b, c, d, sig)

f2 <- function(m, k, rho, pi, tau, l, t, b, c, d, sig, lambda)
{
  # Value function
  p <- pow_clch(m, k, rho, pi, tau, l, t, b, c, d, sig)
  -(p - lambda*m)
}

# Find optimal m for various nuisance parameter values
opt <- expand.grid(rho=seq(0,0.7,0.01), pi=seq(0.7, 1, 0.01), sig=c(4,5,6), tau=c(0.5, 0.7, 0.9))
opt <- cbind(opt, apply(opt, 1, function(x) optim(10, fn=f2, lower=2, upper=1000, 
                                                    k=k, rho=x[1], pi=x[2], tau=x[4], l=l, t=t, b=b, c=c, d=d, sig=x[3], lambda=lambda,
                                                    method="L-BFGS-B")$par))
names(opt)[5] <- c("m")
opt$v <- f2(opt[,5], k, opt[,1], opt[,2], opt[,4], l, t, b, c, d, opt[,3], lambda)

# Plot optimal m for different parameter values
ggplot(opt, aes(rho, pi, z=m, colour=..level..)) + geom_contour() +
  facet_wrap(sig ~ tau) +
  theme_minimal()
```

```{r}
g2 <- function(m, k, l, t, b, c, d, lambda, vs, delta, samp)
{
  # Given a sample of points in the nuisance parameter space and their optimal values,
  # evaluate a fixed design by seeing at how many of these points the fixed value
  # is within a delta tolerance of the optimal value
  sum( (vs - f2(m, k, samp[,1], samp[,2], samp[,4], l, t, b, c, d, samp[,3], lambda)) > -delta )
}

# Get samples, their optimal designs, and values
M <- 5000
samp <- data.frame(rho=runif(M, min(opt[,1]), max(opt[,1])), 
                   pi=runif(M, min(opt[,2]), max(opt[,2])),
                   sig=runif(M, min(opt[,3]), max(opt[,3])),
                   tau=runif(M, min(opt[,4]), max(opt[,4])))
samp$v <- apply(samp, 1, function(x) optim(10, fn=f2, lower=2, upper=1000, 
                                                    k=k, rho=x[1], pi=x[2], tau=x[4], l=l, t=t, b=b, c=c, d=d, sig=x[3], lambda=lambda,
                                                    method="L-BFGS-B")$value)

# Find the m which best evaluates according to g
rob2 <- (2:50)[which.max(sapply(2:50, g2, k=k, l=l, t=t, b=b, c=c, d=d, lambda=lambda, vs=samp[,5], 
                        delta=0.03, samp=samp))]
rob2

# Get value of a fixed design for comparison
opt$v_fix <- f2(m=rob2, k, opt[,1], opt[,2], opt[,4], l, t, b, c, d, opt[,3], lambda)

ggplot(opt, aes(rho, pi, z=-(v - v_fix), colour=as.factor(..level..))) + geom_contour(breaks= seq(0.01, 0.05, 0.01)) +
  facet_wrap(sig ~ tau) + 
  theme_minimal() +
  theme(legend.position="bottom") +
  scale_colour_discrete(name = "Difference in value") +
  xlab("Intracluster correlation") + ylab("Cluster autocorrelation")
  
#ggsave("./paper/figures/fixed_sw.pdf", width = 15, height = 15, units="cm")
#ggsave("./paper/figures/fixed_sw.eps", width = 15, height = 15, units="cm", device = cairo_ps())
```

```{r}
# Closed cohort - fixed m
r_cc_clch <- function(m, rho, pi, tau)
{
  # Correlation in a closed cohort
  (m*rho*pi + (1 - rho)*tau)/(1 + (m - 1)*rho)
}

pow_clch <- function(k, mm, rho, pi, tau, l, t, b, c, d, sig)
{
  # Effective individually randomised sample size
  eff_n <- l*k*mm/(def_r(r_cc_clch(mm, rho, pi, tau), l, t, b, c, d)*def_c(mm, rho))
  power.t.test(n = eff_n/2, delta = 2, sd = sig)$power
}

m <- 10
k <- 4

r_cc_clch(m, rho, pi, tau)

def_r(r_cc_clch(m, rho, pi, tau), l, t, b, c, d)

pow_clch(k, m, rho, pi, tau, l, t, b, c, d, sig)

lambda <- grad(pow_clch, k, method="Richardson", mm=m, rho=rho, pi=pi, tau=tau, l=l, t=t, b=b, c=c, d=d, sig=sig)

ms <- 2:45; ps <- pow_clch(ms, k, rho, pi, tau, l, t, b, c, d, sig)
ks <- 2:6; ps <- pow_clch(ks, m, rho, pi, tau, l, t, b, c, d, sig)

f2 <- function(k, m, rho, pi, tau, l, t, b, c, d, sig, lambda)
{
  # Value function
  p <- pow_clch(k, m, rho, pi, tau, l, t, b, c, d, sig)
  -(p - lambda*k)
}

# Find optimal m for various nuisance parameter values
opt <- expand.grid(rho=seq(0,0.7,0.01), pi=seq(0.7, 1, 0.01), sig=c(4,5,6), tau=c(0.5, 0.7, 0.9))
opt <- cbind(opt, apply(opt, 1, function(x) optim(5, fn=f2, lower=2, upper=1000, 
                                                    m=m, rho=x[1], pi=x[2], tau=x[4], l=l, t=t, b=b, c=c, d=d, sig=x[3], lambda=lambda,
                                                    method="L-BFGS-B")$par))
names(opt)[5] <- c("k")
opt$v <- f2(opt[,5], m, opt[,1], opt[,2], opt[,4], l, t, b, c, d, opt[,3], lambda)

# Plot optimal m for different parameter values
ggplot(opt, aes(rho, pi, z=k, colour=as.factor(..level..))) + geom_contour(breaks = 2:6) +
  facet_wrap(sig ~ tau) +
  theme_minimal() +
  theme(legend.position="bottom") +
  scale_colour_discrete(name = "Number of clusters") +
  xlab("Intracluster correlation") + ylab("Cluster autocorrelation")

#ggsave("./paper/figures/opt_sw_k.pdf", width = 15, height = 15, units="cm")
#ggsave("./paper/figures/opt_sw_k.eps", width = 15, height = 15, units="cm", device = cairo_ps())
```

```{r}
g2 <- function(k, m, l, t, b, c, d, lambda, vs, delta, samp)
{
  # Given a sample of points in the nuisance parameter space and their optimal values,
  # evaluate a fixed design by seeing at how many of these points the fixed value
  # is within a delta tolerance of the optimal value
  sum( (vs - f2(k, m, samp[,1], samp[,2], samp[,4], l, t, b, c, d, samp[,3], lambda)) > -delta )
}

# Get samples, their optimal designs, and values
M <- 5000
samp <- data.frame(rho=runif(M, min(opt[,1]), max(opt[,1])), 
                   pi=runif(M, min(opt[,2]), max(opt[,2])),
                   sig=runif(M, min(opt[,3]), max(opt[,3])),
                   tau=runif(M, min(opt[,4]), max(opt[,4])))
samp$v <- apply(samp, 1, function(x) optim(5, fn=f2, lower=2, upper=1000, 
                                                    m=m, rho=x[1], pi=x[2], tau=x[4], l=l, t=t, b=b, c=c, d=d, sig=x[3], lambda=lambda,
                                                    method="L-BFGS-B")$value)

# Find the m which best evaluates according to g
rob2 <- (2:10)[which.max(sapply(2:10, g2, m=m, l=l, t=t, b=b, c=c, d=d, lambda=lambda, vs=samp[,5], 
                        delta=0.03, samp=samp))]
rob2

# Get value of a fixed design for comparison
opt$v_fix <- f2(k=rob2, m, opt[,1], opt[,2], opt[,4], l, t, b, c, d, opt[,3], lambda)

ggplot(opt, aes(rho, pi, z=-(v - v_fix), colour=as.factor(..level..))) + geom_contour(breaks= seq(0.01, 0.05, 0.01)) +
  facet_wrap(sig ~ tau) + 
  theme_minimal() +
  theme(legend.position="bottom") +
  scale_colour_discrete(name = "Difference in value") +
  xlab("Intracluster correlation") + ylab("Cluster autocorrelation")
  
#ggsave("./paper/figures/fixed_sw_k.pdf", width = 15, height = 15, units="cm")
#ggsave("./paper/figures/fixed_sw_k.eps", width = 15, height = 15, units="cm", device = cairo_ps())
```

## Discussion

Is power the right metric for value? Could consider precision, e.g. expected width of a confidence interval, instead.  

```{r}
ns <- 2:80
pow <- sapply(ns, function(x) power.t.test(n=x, delta=1)$power)
se <- sqrt(2*1/ns)

plot(ns, se)
plot(se, pow)
```

Looking at precision (in this case the standard error of a mean difference) doesn't work, because precision will keep increasing with n - there isn't a natural plataue. As a result, minimising a wieghted sum of n and of se will lead to steady increases in sample size as we increase the variance. 

So really, power can be thought of as a value function on precision/information, as it encapsulates the fact that after a point, extra precision is not useful for us because we will already have enough to reliably make our decisions. It gives us a better idea of how much precision we need for the question at hand.

Does this hold up? Increasing precision can let us decrease the portion of parameter space wehere decisions will be random. 

```{r}
n <- 100; d <- 0.3; sig <- 1
power.t.test(n=n, delta=d, sd=sig)$power

pnorm(d/sqrt(2*sig/n) - qnorm(0.975))

lambda <- 0.0025

f <- function(n, lambda)
{
  -(dnorm(d*sqrt(n)/sqrt(2*sig^2) - qnorm(0.975))*d/sqrt(8*n*sig^2) - lambda)
}

g <- function(n, lambda)
{
  -(pnorm(d*sqrt(n)/sqrt(2*sig^2) - qnorm(0.975)) - lambda*n)
}

f <- function(x)
{
  v <- max(pnorm(d*sqrt(1:5000)/sqrt(2*x[1]^2) - qnorm(0.975)) - x[2]*(1:5000))
  
  cn <- ceiling((qnorm(0.8) + qnorm(0.975))^2*x[1]^2*2/d^2)
  c <- max(pnorm(d*sqrt(cn)/sqrt(2*x[1]^2) - qnorm(0.975)) - x[2]*(cn))
  
  v - c
}

df <- expand.grid(sig = seq(0.01, 1, l=100),
                  lambda = seq(0.0001, 0.1, l=100))
df$o <- apply(df, 1, f)

ggplot(df, aes(sig, lambda, z=o)) + stat_contour()
```

## Application - cluster randomised trials

Consider the problem of choosing the sample size of a cluater randomised trial comparing two groups on a continuous endpoint. The size of each cluster will vary and is not under the control of the experiment, but we know the expected cluster size. We expect outcomes within the same cluster to be correlated. Within and between cluster variances are expeceted to be different in each arms, and there is considerable uncertainty about their values. For the purposes of power calculations, we assume that the trial will be analysed by comparing the mean cluster outcomes between the two arms. We restrict attention to the case of equal numbers of clusters in each arm, noting that our uncertainty in the variance components means any unequal randomisation would not be justified (at least not on the grounds of statistical efficiency).

# Comparisons

The alternative approach to dealing with uncertain nuisance parameters within a frequentist framework is SSR, so let's do a comparison of the two approaches. We will use the same example in a paper on SSR for cluster trials, where uncertainty is in the variance compoentns and in the variability of cluster size as in our application above.

```{r, eval=F}
library(numDeriv)

get_power <- function(k, sd)
{
  if(k <= 1){
    return(0)
  } else {
    d <- 1
    # Use the large-sample normal approximation
    p <- 1-pnorm(qnorm(0.975)*sqrt((2*sd^2)/k), d, sqrt((2*sd^2)/k))
    return(p)
  }
}

# Guess of the sd of cluster means is ~ 1.9
get_power(57, 1.9)
# Infer the value function parameter based on our original choice
lambda <- grad(get_power, 57, sd=1.9)

get_value <- function(k, sd)
{
  pow <- get_power(k, sd)
  return(-(pow - lambda*k))
}

get_k <- function(sd)
{
  # Find the k such that value is maximised
  sd <- sd[[1]]
  o <- optim(1, get_value, lower=c(0), upper=c(100), sd=sd, method="Brent")
  return(c(o$par, o$value))
}

get_k_SSR <- function(sd)
{
  # For comparison, show the k needed to get 80% power
  k <- 2*(qnorm(1-0.025) + qnorm(0.8))^2 * (sd^2/1)
  return(c(k, get_value(k, sd)))
}


df <- data.frame(sd=seq(0.1,4,0.05))
df <- cbind(df, t(sapply(df$sd, get_k)))
names(df)[2:3] <- c("k", "v")
df <- cbind(df, t(sapply(df$sd, get_k_SSR)))
names(df)[4:5] <- c("k_SSR", "v_SSR")
df$pow <- sapply(1:nrow(df), function(i, df) get_power(df[i,2], df[i,1]), df=df)

ggplot(df, aes(sd, k)) + geom_line() +
  geom_vline(xintercept = 1.9, linetype=2) +
  geom_line(data=df, aes(sd,k_SSR), colour="darkred")
```

We see that the usual SS method agrees with our method when the initial guess of the nuisance parameter is correct. Now, take the largest sample size required by our method as our robust choice, and examine the 

```{r, eval=F}
# Robust sample size
k_rob <- max(df$k)
df_rob <- data.frame(sd = df$sd)
df_rob$pow <- sapply(df_rob, get_power, k=k_rob)


# Simulate SSR
sim_SSR <- function(sd)
{
  k <- 29; m <- 4
  c_means <- rnorm(2*k, 0, sd)
  sd_est <- sd(c_means)
  new_k <- max(57, get_k_SSR(sd_est)[1])
  return(new_k)
}

batch_SSR <- function(sd)
{
  ks <- replicate(1000, sim_SSR(sd))
  pows <- sapply(ks, get_power, sd=sd)
  return(c(as.numeric(quantile(ks, c(0.01,0.1,0.5,0.9,0.99))), as.numeric(quantile(pows, c(0.01,0.1,0.5,0.9,0.99)))))
}

df_SSR <- data.frame(sd = df$sd)
df_SSR <- cbind(df_SSR, t(sapply(df_SSR$sd, batch_SSR)))
names(df_SSR)[2:6] <- c("kq01", "kq10", "kq50", "kq90", "kq99")
names(df_SSR)[7:11] <- c("pq01", "pq10", "pq50", "pq90", "pq99")

# Plot sample sizes
ggplot(df_SSR, aes(sd, kq50)) + geom_line(colour="darkred") +
  geom_ribbon(aes(ymin = kq10, ymax = kq90), alpha=0.2, fill="darkred") + 
  geom_ribbon(aes(ymin = kq01, ymax = kq99), alpha=0.2, fill="darkred") +
  geom_hline(yintercept = k_rob, colour="darkgreen") +
  ylim(c(0,400))

# Plot powers
ggplot(df_SSR, aes(sd, pq50)) + geom_line(colour="darkred") +
  geom_ribbon(aes(ymin = pq10, ymax = pq90), alpha=0.2, fill="darkred") + 
  geom_ribbon(aes(ymin = pq01, ymax = pq99), alpha=0.2, fill="darkred") +
  geom_line(data=df_rob, aes(sd, pow), colour="darkgreen") +
  ylim(c(0,1))
```

What is the problem with SSR? The key point is that it entails a strict following of the constrained approach to trial design, in contrast to the initial design where flexibility is implictly allowed, generally done, but always hidden. A better SSR apprach would be to re-estimate the parameter and then do the SS calculation as we do in practice, including allowing us to change the effect size we want to detect to avoid admitting we have low power. 

# Controlling type I error

In the bove we have fixed $\alpha$ at some standard choice. Consider varying it as a design parameter.

```{r, eval=F}
get_power <- function(x, sd)
{
  k <- x[1]; alpha <- x[2]; d <- 0.3
  if(k <= 1){
    return(0)
  } else {
    # Use the large-sample normal approximation
    p <- 1-pnorm(qnorm(1-alpha)*sqrt((2*sd^2)/k), d, sqrt((2*sd^2)/k))
    return(p)
  }
}

get_power(c(235, 0.025), 1)
lambda <- grad(get_power, c(235, 0.025), sd=1)

get_value <- function(x, sd)
{
  k <- x[1]; alpha <- x[2]; d <- 1
  if(k < 2 | k > 800 | alpha <=0 | alpha >=1) return(10000)
  pow <- get_power(c(k, alpha), sd=sd)
  return(-(pow - lambda[1]*k - lambda[2]*alpha))
}

get_design <- function(sd, starting=c(95.5,0.025))
{
  o <- optim(starting, get_value, sd=sd)
  return(c(o$par, o$value))
}


sds <- seq(0.1,3,0.02)
df <- NULL
df <- rbind(df, c(sds[1], get_design(sds[1])))
for(i in 2:length(sds)){
  df <- rbind(df, c(sds[i], get_design(sds[i], starting = df[i-1,2:3])))
}
df <- as.data.frame(df)
names(df) <- c("sd", "k", "a", "v")
df$p <- apply(df[,1:3], 1, function(x) get_power(x[2:3], sd=x[1]))
df$c <- apply(df, 1, function(x) sqrt(2*x[1]^2/x[2])*qnorm(1-x[3]))

ggplot(df, aes(k, a, colour=sd)) + geom_point() 

# Get value of fixed design
rob <- data.frame(sd=sds, k=235, a=0.025)
rob$v <- apply(rob, 1, function(x) get_value(x[2:3], x[1]))

df2 <- rbind(rob, df[,c("sd", "k", "a", "v")])

ggplot(df2, aes(sd, v, colour=k)) + geom_point()

# Get vakue of optimal design with constrained alpha

get_value2 <- function(x, sd)
{
  k <- x[1]; d <- 1; alpha <- 0.025
  pow <- get_power(c(k, alpha), sd=sd)
  return(-(pow - lambda[1]*k - lambda[2]*alpha))
}

get_design2 <- function(sd, starting=c(95.5))
{
  o <- optim(starting, get_value2, sd=sd, method="Brent", lower = 2, upper = 800)
  return(c(o$par, o$value))
}

df3 <- data.frame(sd=df$sd)
df3 <- cbind(df3, t(sapply(df3$sd, get_design2)))
names(df3)[2:3] <- c("k", "v")
df3$a <- 0.025

df2 <- rbind(df2, df3[,c(1,2,4,3)])
df2$t <- c(rep("rob", 146), rep("opt", 146), rep("opt2", 146))

ggplot(df2, aes(sd, v, colour=k, shape=t)) + geom_point()
```

An increase in $\sigma$ leads to a higher optimal type I error rate, accopanied by a lower inflation of the sample size compared to the previous case where $\alpha$ was fixed. If we want to ptotect the design against only a $\sigma$ larger than our best guess, we could argue that the conservative approach is to keep $\alpha$ at the low initial value and then conditional on this, choose the maximum optimal sample size (as we did previously). 

Key point is that for a larger $\sigma$ the optimal design will increase the type I error, but we can instead choose to keep it fixed and increase the sample size instead. Bear in mind that the addtive value function assumption and the method of eliciting the parameters may not be applicable for type I error rates - wehereas we can argue that we get round the power "constraiint" by adjusting the MCID, it appears that the 0.025 one sided $\alpha$ is usued pretty much all the time.

## Application - survival 

For our cluster randomised trial we had a single design variable, the sample size. Here we consider a problem with two design variables, the sample size and the follow-up time for a trial comparing a time-to-event outcome. We start as before, with a power function and a value function whose parameters are deterimined from the choice of local design. 

```{r, eval=F}
# see chapter 9, D.Collet Modelling survival data in medical research
s_bar <- function(x, med1, med2)
{
  # Assuming exponential models with medians med1 and med2,
  # so S(x) = exp(-ln(2)x/med)
  (exp( -(log(2)/med1)*x ) + exp( -(log(2)/med2)*x ))/2
}
  
get_beta <- function(x, med1)
{
  # med1 - median survival control
  # x - (accrual time period, total trial time)
  # a - accrual time period
  # f - follow-up time period
  a <- x[[1]]; f <- x[[2]] - x[[1]]
  dif <- 2
  med2 <- med1 + dif
  m <- 10.3 # monthly accrual rate
  theta <- log(med1/med2) # log harzard ratio
  
  # Expected sample size
  n <- m*a
  # Probability of an event
  prob_d <- 1 - (s_bar(f, med1, med2) + 4*s_bar(0.5*a + f, med1, med2) + s_bar(a + f, med1, med2))/6
  # Expected number of events
  d <- n*prob_d
  # Power
  pow <- pnorm( -theta*sqrt(d)/2 - qnorm(0.975) )
  return(1-pow)
}

get_beta(c(23,48), 4.5)
w <- grad(get_beta, c(23,48), med1=4.5)
```

Finding the optimal design (i.e. that which maximises the value function) for any given value of the nuisance parameter:

```{r, eval=F}

get_value <- function(x, med1)
{
  if(x[1] < 1 | x[1] > 100 | x[2] < 1 | x[2] > 100) return(100000)
  beta <- get_beta(x, med1)
  return(beta - sum(w*x))
}

get_design <- function(med1)
{
  o <- optim(c(5,5), get_value, med1=med1)
  return(c(o$par, o$value))
}

df <- data.frame(med1=seq(2, 10, 0.05))
df <- cbind(df, t(sapply(df$med1, get_design)))
names(df)[2:4] <- c("a", "t", "v")

ggplot(df, aes(a, t, colour=med1)) + geom_point() 
```


Now, take the total study time $t$ to be fixed so we are down to a single design variable, the accrual time $a$. Taking the same approach as above, what value of $a$ is robust to uncertainty in the nuisance parameter (the median time in the control group)?

```{r, eval=F}
# Fixed t, total time for study (i.e. accrual time + follow-up time)
obj_func_fixed_t <- function(a, t, med1)
{
  x <- c(a, t)
  beta <- get_beta(x, med1=med1)
  return(beta - sum(w*x))
}

eval_p_fixed_t <- function(med1, t)
{
  # Find accrual time a that maximises value for known med1
  opt <- optim(par=3, fn=obj_func_fixed_t, t=t, med1=med1,
               lower = 2, upper = t, method="Brent")
  return(-opt$par)
}

opt_fixed_t <- function(t)
{
  # Search over med1 to find the largest a possibly required
  opt <- optim(par=5, fn=eval_p_fixed_t, t=t, lower = 1, upper = 100, method="L")
  return(-opt$value)
}

# For example, 
opt_fixed_t(36)
```

Similarly, we can do the same but taking the accrual time as fixed and looking for the maximum total study time we would want if the nuisance parameter were known.

```{r, eval=F}
# Fixed a, accrual time
obj_func_fixed_a <- function(t, a, med1)
{
  x <- c(a, t)
  beta <- get_beta(x, med1=med1)
  return(beta - sum(w*x))
}

eval_p_fixed_a <- function(med1, a)
{
  opt <- optim(par=10, fn=obj_func_fixed_a, a=a, med1=med1,
               lower = 3, upper = 100, method="Brent")
  return(-opt$par)
}

opt_fixed_a <- function(a)
{
  opt <- optim(par=5, fn=eval_p_fixed_a, a=a, lower = 1, upper = 100, method="L")
  return(-opt$value)
}

# For example,
opt_fixed_a(24)
```


So, for any value of $a$ we can find the value of $t$ that is robust to the nuisance parameter, and vice versa:

```{r, eval=F}
ts <- 3:75
t_o <- sapply(ts, opt_fixed_t)

as <- 3:75
a_o <- sapply(as, opt_fixed_a)

df <- data.frame(t=ts, t_o=t_o, a=as, a_o=a_o)

ggplot() + geom_point(data=df, aes(t, t_o, colour="t_fix")) + geom_point(data=df, aes(a_o, a, colour="a_fix"))
```

We see that the functions cross at an equilibrium point $(a^*, t^*)$, which can be found using the following algorithm:

```{r, eval=F}
t1 <- runif(1,3,100); a1 <- runif(1,3,100)
converged <- FALSE
iter <- 1
print(c(a1,t1))
while(!converged & iter < 100){
  t2 <- opt_fixed_a(a1)
  a2 <- opt_fixed_t(t2)
  print(c(a2, t2))
  if(dist(rbind(c(t1,a1),c(t2,a2))) < 0.00001) converged <- TRUE
  t1 <- t2; a1 <- a2
}
```

For a given design, is there a value of $med1$ where another design, not smaller in either aspect, gives a better value?
```{r, eval=F}

f <- function(y, med1, x)
{
  if(sum(y < x) > 0 | y[1] >= y[2]) return(10000)
  get_value(y, med1)
}

g <- function(med1, x)
{
  if(med1 < 1 | med1 > 20) return(10000)
  opt <- optim(par=x, fn = f, x=x, med1=med1, lower=x, method="L-BFGS-B")
  return(-sum(opt$par - x))
}

h <- function(x)
{
  meds <- runif(100, 1, 20)
  start <- meds[which.min(sapply(meds, g, x=x))]
  #opt <- optim(par=start, fn=g, x=x, lower=1, upper=1+2*start, method="Brent")
  opt <- nlm(g, start, x=x)
  opt$minimum
}

df <- expand.grid(a=seq(25,35,1), t=seq(55, 85, 1))
df <- df[df$t > df$a,]
df$v <- apply(df, 1, h)

ggplot(df, aes(a,t,colour=v<0)) + geom_point()
```

## Pilot trials

Connecting with our work on testing feasibility in pilots, where we assume a fixed main trial sample siZe and define null and alternative hypothees in terms of the power that will be obtained. Will be essentially the same when we are focussing on nuisance parameters like a common variance rather than feasibility parameters. The method as given above gives us a fixed main trial sample size, which will be conservative - it will never be optimal to increase it, no matter what the true value of the parameter is. But is some cases the optimal sample size will be zero, so we would have run a futile trial. Can we use the testing idea to reduce the chance of this happeneing? How will it compare with the alternative SSR approach?

Copied from above:
```{r, eval=F}
lambda <- 0.005
df <- expand.grid(n=2:500, sig=seq(1,5,0.1))
df$pow <- apply(df, 1, function(x) power.t.test(n=x[1], delta=1, sd=x[2])$power)
df$v <- df$pow - lambda*df$n

f <- function(n, sig)
{
  pow <- power.t.test(n=n, delta=1, sd=sig)$power
  -(pow - lambda*(n + 32))
}

opt <- data.frame(sig=seq(1,5,0.1), n=1)
opt$n <- sapply(opt$sig, function(x) optim(10, f, method="Brent", lower=2, upper=500, sig=x)$par)
opt$pow <- apply(opt, 1, function(x) power.t.test(n=x[2], delta=1, sd=x[1])$power)
opt$value <- opt$pow - 0.005*(opt$n + 32)

ggplot(df, aes(n, pow, colour=sig, group=as.factor(sig))) + geom_line() +
  geom_point(data=opt, colour="red") +
  geom_vline(xintercept = max(opt$n), linetype=2, colour="red")

opt[which.max(opt$n),]
df0 <- opt
```


So we have a fixed main trial sample size of 88. What does power look like for our range of variances?
```{r, eval=F}
df2 <- data.frame(sig=seq(1,4,0.1))
df2$pow <- sapply(df2$sig, function(x) power.t.test(n=88, delta=1, sd=x)$power)
ggplot(df2, aes(sig, pow)) + geom_point()
```
At some point the power obtained won't justify the expense. Say that 60\% is our threshold. If the true sd is around 2.98, we will get 60\% power and will be indifferent about running vs not running the trial. Given the value function, this translates into a set-up cost of 32 patients or 0.005*32 = 0.16 value units (we have included this in the above code). We can see the value function crossing 0 at this point:

```{r, eval=F}
ggplot(df0, aes(sig, n)) + geom_line() + 
  geom_line(data=df0, aes(sig, 100*value), colour="darkgreen") +
  geom_vline(xintercept = 2.98, linetype =2)
```

So, if the true $\sigma$ is > 2.98 our robust trial will actually be worse than no trial at all. If we want to test feasibility in a pilot, we could ask that we have 50\% power for a $\sigma = 2.98$ so that our indfiference at this point is reflected. Then we need to decide how large the pilot should be. We can take the same approach as for the main trial, by extending the value function, finding the optimal pilot sample size for a range of $\sigma$, and then being conservative by choosing the largest of these. To extend the value function we need to now think of the overall power of the two-trial system, and now consider the expected sample size (where the expectation is over the pilot data / test result):

```{r, eval=F}
get_value <- function(n_p, sig)
{
  alpha <- 0.5; n <- 88
  c <- qchisq(alpha, n_p-1)*2.98^2/(n_p-1)
  pilot_pow <- pchisq(c*(n_p-1)/sig^2, n_p-1)
  main_pow <- power.t.test(n=n, delta=1, sd=sig)$power
  # Return both the pilot value, and the value of not doing a pilot at all
  c(-(pilot_pow*main_pow - lambda*(n_p/2+pilot_pow*(n+32))), main_pow - lambda*(n+32))
}

df3 <- data.frame(sig=seq(1,6,0.1))
df3$n <- sapply(df3$sig, function(x) optim(30, function(y, x) get_value(y, x)[1], x=x,
                                           lower = 2, upper = 200, method="Brent")$par)
ggplot(df3, aes(sig, n)) + geom_line()
```
In this example we see that optimal pilot size drops to 0 as we get to the point of equivalence. There are two peaks, one on either side of this point. As we move to the extremes, the optimal size reduces, as we would expect - it becomes easier to make the correct decision with only a few data. The maximum here is $n_p \approx 12$. What value would we obtain if we use that?

```{r, eval=F}
df4 <- expand.grid(sig=seq(1,6,0.1))
df4 <- cbind(df4, t(sapply(df4$sig, function(x, n_p) get_value(n_p, x), n_p = max(df3$n))))
names(df4)[2:3] <- c("v", "v2")

ggplot(df4, aes(sig, -v)) + geom_line() +
  geom_line(data=df4, aes(sig, -(v+v2)), linetype=3) +
  geom_vline(xintercept = 2.98, linetype =2)
```
The differnece in value between our "robust" pilot and not running one at all (dotted line) is slightly negative for $\sigma < 2.98$ and positive thereafter, with the difference becoming very large as $\sigma$ increases past this threshold.

We have in the above fixed the main $n$ and optimised over $n_p$. What happens if we do the reverse?

```{r, eval=F}
get_value2 <- function(n, sig)
{
  alpha <- 0.5; n_p <- 12
  c <- qchisq(alpha, n_p-1)*2.98^2/(n_p-1)
  pilot_pow <- pchisq(c*(n_p-1)/sig^2, n_p-1)
  main_pow <- power.t.test(n=n, delta=1, sd=sig)$power
  # Return both the pilot value, and the value of not doing a pilot at all
  c(-(pilot_pow*main_pow - lambda*(n_p/2+pilot_pow*(n+32))), -(main_pow - lambda*(n+32)))
}

df5 <- data.frame(sig=seq(1,6,0.1))
df5$n <- sapply(df5$sig, function(x) optim(30, function(y, x) get_value2(y, x)[1], x=x,
                                           lower = 2, upper = 200, method="Brent")$par)
ggplot(df5, aes(sig, n)) + geom_line()
```

We see that the optimal $n$ is the same regardless of the pilot sample size, including when it s $n_p = 0$. This makes sense when we see that the overall value function takes the form

$$
\begin{aligned}
v_p(n, n_p) &= x(n_p)f(n) - \lambda \big[y(n_p) + xg(n) \big] \\
&= x(n_p) \big[f(n) - \lambda g(n) \big] - \lambda y(n_p) \\ 
&= x(n_p) v(n) - \lambda y(n_p),
\end{aligned}
$$

so for fixed $n_p$ the extedned value is a linear transformation of the mian trial value.

What does change is the value function. As the pilot size increases the break even point where $v = 0$ reduces - for example when $n_p = 12$ it is at $\sigma \approx 2.81$, not 2.98 as before when $n_p = 0$.

# cRCT + SW

```{r}
clus_pow2 <- function(x, var_w, rho)
{
  n <- x; m <- 18; k <- n/m
  var_b <- rho*var_w/(1-rho)
  clus_var <- var_b + var_w/m
  power.t.test(n=k, delta=0.3, sd=sqrt(clus_var))$power
}

# Plot the motivating example, with two nuisance parameters power curves
df <- data.frame(n=seq(2,700,20))
df$k <- df$n/18
df <- rbind(df, df)
df$rho <- c(rep(0.02, nrow(df)/2), rep(0.07, nrow(df)/2))
df$p <- apply(df, 1, function(x) clus_pow(x[2:1], var_w=1, rho=x[3]))

grad(clus_pow2, 262, var_w=1, rho=0.02)

ggplot(df, aes(n, p, colour=as.factor(rho))) + geom_line() +
  geom_hline(yintercept = 0.8, linetype=2) +
  ylab("Power") + xlab("Sample size") +
  scale_y_continuous(breaks=seq(0,1,0.2)) +
  theme(panel.grid.major = element_line(colour = "grey50")) +
  theme(panel.grid.minor = element_line(colour = "grey80")) +
  scale_colour_manual(values=cols) +
  labs(colour = "ICC") +
  theme_minimal() +
  geom_point(data=data.frame(n=c(262,442), p=c(0.8,0.8), rho=c(0.02, 0.07)), shape=16, size=2) +
  geom_abline(intercept = 0.8 - 0.001592332*262, slope = 0.001592332, linetype=3) + 
  geom_abline(intercept = 0.16, slope = 0.001592332, linetype=3) +
  geom_point(data=data.frame(n=280, p=0.61, rho=0.07), shape=3, size=2, stroke=2)

ggsave("./presentations/cRCT+SW 2019/motivate.png", height=9, width=15, units="cm", bg = "transparent")
```


```{r}
clus_pow <- function(x, var_w, rho)
{
  k <- x[1]; n <- x[2]; m <- n/k
  var_b <- rho*var_w/(1-rho)
  clus_var <- var_b + var_w/m
  power.t.test(n=k, delta=0.3, sd=sqrt(clus_var))$power
}

# plot power function to find value params
df <- expand.grid(k=seq(10, 40, 5), n=seq(2,700,20))
df$pow <- apply(df, 1, clus_pow, var_w=1, rho=0.05)

ggplot(df, aes(n, pow, group=k, colour=as.factor(k))) + geom_line() +
  geom_point(data=data.frame(k=15, n=470, pow=clus_pow(c(15, 470), 1, 0.05)), colour="red") +
  theme_minimal()

#ggsave("./paper/figures/cluster_pow.pdf", height=9, width=14, units="cm")

des <- c(25, 500) # 90
des <- c(20, 360) # 80
clus_pow(des, 1, 0.05)
lambda <- grad(clus_pow, des, var_w=1, rho=0.05)

# Plot the optimal designs as we vary rho

f <- function(x, var_w, rho)
{
  pow <- clus_pow(x, var_w, rho)
  -(pow - sum(lambda*x))
}

opt <- expand.grid(var_t=1, rho=seq(0,0.25,0.001))
opt <- cbind(opt, t(apply(opt, 1, function(x) optim(c(10,10), fn=f, lower=c(2,2), upper=c(70,1000), var_w=x[1], rho=x[2], method="L-BFGS-B")$par)))
names(opt)[3:4] <- c("k", "n")

ggplot(opt, aes(rho)) + geom_line(aes(y=k), colour=cols[3]) + geom_line(aes(y=n/k), colour=cols[4]) +
  theme_minimal() + xlab("Intracluster correlation coefficient (ICC)") + ylab("")

ggsave("./presentations/cRCT+SW 2019/opt_ssr.png", height=9, width=11, units="cm", bg = "transparent")
```

```{r}
opt$v <- apply(opt, 1, function(x) f(x[3:4],x[1],x[2]))

rob <- opt
rob$k <- des[1]
rob$n <- des[2]
opt$v2 <- apply(rob, 1, function(x) f(x[3:4],x[1],x[2]))
opt$d <- opt$v - opt$v2

# get optimal conventional design, assuming m = 20

f2 <- function(x, var_w, rho)
{
  pow <- clus_pow(x, var_w, rho)
  sum(lambda*x) + 10000*(0.8-pow)^2
}

opt <- cbind(opt, t(apply(opt, 1, function(x) optim(c(10,100), fn=f2, lower=c(2,2), upper=c(100,1000), var_w=x[1], rho=x[2], method="L-BFGS-B")$par)))
names(opt)[8:9] <- c("k2", "n2")

ggplot(opt, aes(rho)) + 
  geom_line(aes(y=-v), colour=cols[1]) +
  geom_line(aes(y=-v2), colour=cols[2]) +
  geom_line(aes(y=-d), colour=cols[8], linetype=2) +
  theme_minimal() + xlab("Intracluster correlation coefficient (ICC)") + 
  ylab("Value")

ggsave("./presentations/cRCT+SW 2019/value.png", height=9, width=11, units="cm", bg = "transparent")
```

```{r}
# Get true power of each of these designs when rho = 0.05
opt$p <- apply(opt, 1, function(x) clus_pow(x[3:4], 1, 0.05))
opt$p2 <- apply(opt, 1, function(x) clus_pow(x[8:9], 1, 0.05))

var_c <- 0.1026316; var_b <- var_c - 1/18
vb_est <- var_b*rchisq(10^3, 11)/11 
rho_est <- vb_est/(vb_est + 1) 
rho_ind <- round(rho_est, 3)*1000 
k_est <- opt[rho_ind,"k"]; n_est <- opt[rho_ind,"n"]; m_est <- n_est/k_est 
k2_est <- opt[rho_ind,"k2"]; n2_est <- opt[rho_ind,"n2"]
p_est <- opt[rho_ind,"p"]; p2_est <- opt[rho_ind,"p2"]

hi <- data.frame(rho=rho_est, k=k_est, n=n_est, k2=k2_est, n2=n2_est, p=p_est, p2=p2_est)

ggplot(hi, aes(rho)) + geom_density(fill=cols[1], alpha=0.2) +
  theme_minimal() + xlab("Estimated ICC sampling distribution") +
  theme(panel.background = element_blank()) +
  theme(
    plot.background = element_blank()
   ,panel.grid.major = element_blank()
   ,panel.grid.minor = element_blank()
   ,panel.border = element_blank()
  )

ggsave("./presentations/cRCT+SW 2019/ICC.png", height=5, width=8, units="cm", bg = "transparent")

hi2 <- data.frame(k=c(k_est, k2_est), n=c(n_est, n2_est), p=c(p_est, p2_est), t=c(rep("Value-based", length(vb_est)), rep("Constrained", length(vb_est))))

ggplot(hi2, aes(t, n, fill=t)) + geom_boxplot() +
  scale_x_discrete(breaks = NULL) +
  theme_minimal() + xlab("") + ylab("Sample size") +
  scale_fill_manual(values=cols[5:6], name= "Method") + theme(legend.position="bottom")

ggsave("./presentations/cRCT+SW 2019/SS_dist.png", height=8, width=8, units="cm", bg = "transparent")

ggplot(hi2, aes(t, p, fill=t)) + geom_boxplot() +
  scale_x_discrete(breaks = NULL) +
  theme_minimal() + xlab("") + ylab("Power") +
  scale_fill_manual(values=cols[5:6], name= "Method") + theme(legend.position="bottom")

ggsave("./presentations/cRCT+SW 2019/pow_dist.png", height=8, width=8, units="cm", bg = "transparent")
```

```{r}
s <- 1; n <- 200; mu <- 0.3
c <- qnorm(0.975)*sqrt(2/n)

z <- rnorm(100, mu, sqrt(2/200))

mean(z > c)
mean(z[z > c])

pow <- function(n)
{
  1 - pnorm(qnorm(0.975)*sqrt(2*s/n), 0.3, sqrt(2*s/n))
}

bias <- function(n)
{
  # Bias of the effect estimate when conditioning on a signifcant result (z > c)
  (dnorm(qnorm(0.975) - 0.3/sqrt(2*s/n)) / (1 - pnorm(qnorm(0.975) - 0.3/sqrt(2*s/n))))*sqrt(2*s/n)
}

ns <- 100:300
plot(ns, bias(ns)/mu)
plot(pow(ns), bias(ns))

s <- 1; n <- 200; mu <- 0.3
c <- qnorm(0.975)*sqrt(2/n)

ps1 <- ps2 <- NULL
for(i in 1:10000){
  z <- rnorm(100, mu, sqrt(2/n))
  ps1 <- c(ps1, 1-pnorm(c, mu, sd(z)))
  ps2 <- c(ps2, mean(z > c))
}

```





EVSI

Rather than optimising over splines, suppose we optimise over the individual samples. Doing this directly doesn't work, as we end up with an over-fitted and not continuous function. 