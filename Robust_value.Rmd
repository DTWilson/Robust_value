---
title: "Robust, value-based trial design when nuisance parameters are unknown"
author: "D. T. Wilson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(numDeriv)
require(ggplot2)
require(shiny)
require(pso)
require(viridis)
require(gganimate)
require(transformr)
require(magick)
require(patchwork)
require(statmod)
require(reshape2)
require(mgcv)
require(RColorBrewer)
cols <- brewer.pal(8, "Dark2") 
```


## Non-lienar power value

Assuming a linear single attribute n, and assuming that the rate of substitution depends on $\beta$ but not on $n$ (so, the amount of $n$ we will pay to improve $\beta$ does not depend on $n$), a single attribute value function for $\beta$ will complete our specification.

What can we say, in general, about $v(\beta)$? We can surely say that it plateaus as $\beta$ tends to 0, since by that point decreasing $\beta$ will only give us improvements in the power to detect an effect very close to the null. 

We could argue value will actually start to decrease, if we are to strictly follow the test results, since as $\beta$ tends to 0 we will progress even for effects very close to the null. Counter argument is that if we got a very precise estimate of a small effect we would not progress; but that implies a different test procedure and so different error rates.

At the other end, starting with $\beta = 1- \alpha$, do we expect value to be increasing at a decreasing rate? One potential metric which might be helpful is the integral of the power function beyond the null and up to some maximum effect - equivalently, the average power over a uniform distribution of treatment effects.

```{r}
pow <- function(mu, sd)
{
  power.t.test(n = 200, delta = mu, sd = sd)$pow
}

z <- integrate(pow, lower=0, upper = 5,
          sd = 1)

df <- data.frame(sd = seq(0.1, 5, 0.01))
df$p <- sapply(df$sd, function(x) pow(0.3, x))
df$v <- sapply(df$sd, function(x) integrate(pow, lower=0, upper = 5, sd = x)$value)

plot(df$p, df$v)

df$r <- 1
df$r[1:490] <- df$v[2:491] - df$v[1:490]
```
So we can say that value of power is monotonic increasing, and that its derivative is monotonic decreasing at least up to very large powers (say, 0.95).

To get the value function, we need to elicit the rate at which we would `pay' for more power given we already have $1 - \beta$, for different values of $1 - \beta$. One way to do that is to vary hypothetical power functions (by altering sd) and find the function where the sample size with $1 - \beta$ is optimal. The sample size which achieves this is irrelevant - the same rate of substitution will apply for all $n$.

```{r}
v <- function(x, lambda, delta)
{
  log(x[,2]*delta + 1)/log(delta+1) - lambda*x[,1]
}

l <- 0.0009; d <- 20
l <- 0.0023; d <- 0.001
l <- 0.0013; d <- 0.001

df <- expand.grid(n = 2:400,
                  p = seq(0,1,0.01))
df$v <- v(df, lambda = l, delta = d)

opt <- NULL
for(sd in seq(0.2,5,0.1)){
  z <- cbind(2:400, power.t.test(n = 2:400, delta = 0.3, sd = sd)$pow)
  opt <- rbind(opt, z[which.max(v(z, l, d)),])
}

plot(seq(0.2,5,0.1), opt[,1])

ggplot(df, aes(n, p)) + geom_contour(aes(z = v)) +
  geom_point(data = data.frame(n = opt[,1], p = opt[,2]))
```
i) Value

We can characterise trial design decisions as balancing the cost of sampling against the benefits of precise estimates or, more typically, power to detect an MCID. We generally solve this bi-objective problem by imposing a power constraint.

When faced with a decision like this (under certainty), one approach is to define a function $V$ which articulates our preferences over the attributes $n$ and $\beta$. Solving a specific problem would then calculate $v$ for all the possible pairs and choose that with the largest $v$. The existence of $v$ follows from an assumption of a weak ordering on the attribute space.

One reason we might not want to do this is that it solves a bigger problem than the one we are interested in. If we elicit $v$ then we can solve any SSD problem in this setting (i.e. same clinical area and MCID). In practice, we only need to look at one specific power curve (conditioning on $\sigma$). So why bother?

The issue is that $\sigma$ is not known, but just an estimate. To deal with uncertainty in $\sigma$, we need $v$ to make sure our decision making is coherent / consistent.

Proposal of $v$ structure and therefore elicitation - linear in n; marginal trade-offs depend on $\beta$ but not $n$; can elicit by asking for some hypothetical optimal designs. 

ii) SSD

Suppose we have an estimate and uncertainty interval for $\sigma$ which has come from a previous study. We know what the locally optimal design is for any true $\sigma$. A (frequentist) decision-theoretic approach to dealing with the uncertainty is to choose the $n$ with the best worst-case outcome over the interval - minimax. We define qulaity here not by value itself, but by regret - the difference in value between the proposed desgin and the locally optimal design.

iii) SSR

Regret is clearly minimised when the estimate is true, and the uncertainty interval tends to 0. So, we might want to consider producing a new and more precise estimate via a pilot to help minimise the worst case regret of the optimal n. This pilot could be external or internal. We then need to decide how large the pilot should be, given its costs (which are sampling in external pilots, and alpha spending [and costs of interim stops] in internal pilots).

Given the algorithm for choosing $n$ based on the pilot estimate, we now have a distribution on $n$ and therefore $v$ for any pilot samples size. To choose between these distributions we can look at $E[v]$, which effectively assumes we are risk-neutral. 

To do this, we will need to augment $v$. For an external pilot, we will need to include cost of pilot sampling (which might be different to main sampling), and pilot set-up costs.

Now have, for every pilot n, a range of expected values over true $\sigma$. Similar to SSD, we then get the locally optimal pilots and then choose pilot n based on minimax regret of expected value over the interval.

iv) Illustration and evaluation

Focus on the simple two-sample t-test here.

For SSD, we can compare against Julious2006 which was shown in Shi2020 to be a good (best?) method for the case of a continuous endpoint. Might also want to use the adjusted CI approach of Browne. And compare against using just the point estimate.

For SSR, compare with Whitehead2015 who extended the NCT approach to SSR. Their paper is not very clear. They suggest choosing the pilot n to minimise the total n, but base this on a fixed main n following from the initial estimate of $\sigma$. That is, they don't account for the random main n. This may or may not correspond to the expected n. They do go on to look at the distribution of main power, which seems to back up their approach as the expected power is nominal. So they basically use just the initial point estimate to design the pilot.

Want to do all this for different value parametrisations. four scenarios crossing the two params, so we have different trade offs and also linear / non-linear in power. Have all agree on the same locally opt design conditional on point estimate.

May want to look at different effect sizes - report only if qualitative conclusions change.

v) Extensions

TO show how the general framework can easily apply to other problems, look at e.g. a cRCT with random cluster sizes. For some example initial estimates and uncertainties, find the optimal pilot, and for a simulated pilot data example show how that determines optimal main n. Outline any difficulties, e.g. computational.

vi) Discussion

How does $v$ based approach compare against standard SSD? Latter only makes sense if there is a threshold of power where quality jumps up, but no reason to think this holds. In practice, the standard approach is not actually used. Instead, a value based approach is done implicitly by adjusting the MCID to get a "feasible" sample size. So this proposal is not that radical, and by doing things explicitly we should be able to do better.

Elicitation in practice might be tricky. In particular, who's values are we eliciting? The team writing the proposal? The funder who makes the decision? Or even the patient who might be enrolled? How to reconcile different stakeholders having different values?

We focus on estimating $\sigma$. This is consistent with arguments against using a pilot estimate of the effect as a basis for future sample size calcs. One way to extend would be to add other points where power is calculated to $v$. In the limit, we would have an integral of a function which weights by value. These weights will come from considerations of both the impact and likelihood of a significant result. We might arrive at it by splitting these into defining a prior and a utility function, and then find ourselves in a fully Bayesian decision-theoretic framework. This would also let us deal with $\sigma$ in a probabilistic sense, and make decisions based on expected utility over all uncertainties - parameter values and pilot sampling.

There are lots of reasons to run a pilot, beyond estimating $\sigma$. We could extend the method by including some which directly relate to power - see WP1.1. This extra value could also be taken into account in terms of the cost of the pilot - e.g. saying that the first 30 patients are free, since we will be using that for other feasibility objectives.

To do -
Two sample t test problem. 
Implement methods for choosing n based on estimate and interval - ours, NCT, upper CI.
Implement methods for choosing pilot n based on initial estimate and interval.
Methods to be flexible in terms of intervals, treatment effect, and two value parameters.
Special case on known $\sigma$
Sim study comparing methods for SSD - for different points and interval widths, what is the optimal n? Are the methods meaningfully different? How does the relationship change with effect size? Are some value functions closer to standard methods than others?
(Expect differences not to be massive here, since value params chosen to agree with standard methods).
Sim study comparing methods for SSR. As above, but now what is the optimal n_p? And what are the resulting optimal distributions on power and total sample size / total cost?
(Expect bigger differences here)

Note on NCT method -
This gives a decision rule such that the overall power of a pilot-then-main trial process is nominal. When limiting ourselves to SSD, this is not a relevant comparator, but it will be for SSR. In practice, it leads to a small increase the the usual constrained design, so just compare against that instead. Note that by implicitly arguing an average power is of the same value as a conditional power, the implied utility (and therefore value) function on power is linear. Note that this is then incompatible with the threshold myth - if the myth holds, we would much prefer a sure thing of 0.85 to a 50/50 gamble between 0.75 and 0.95. In comparison, the upper CI approach can be understood as minimax regret when the value function is a threshold.

## Introduction

In this document we will develop applications of a value-based approach to sample size determination (SSD) when nuisance parameters are unknown. The scope is limited to SSD - we will not consider sample size re-estimation here.

## t-test

Note - if we want to include a stepped wedge trial as an example or extension, a recent paper Korevaar2021 has compiled a database of correlation estimates and has a Shiny app for obtaining plausible ranges of these.

Note - if looking at cluster trials, Copas2021 have just published a paper on optimal design when cluster size can be varied which includes a section on parameter uncertainty where they find the design that gives power over a specified range.

Note - we have effectively looked at how the method works when we use power as a metric of study value. Instead of playing around with this (with log transforms), we can instead look at another potential metric - precision. In both cases we argue that these will be value indpenendant of sample size, so a constant rate of substitution which does not depend on either current power/precision or current sample. So still have four scenarios to illustrate - power and precision metrics, each parameterised twice to give certain optimal ns for example sds. When we look at precision we see an even more extreme relationship where the minimax regret n depends almost completely on the point estimate and not on the interval. This scenario is also analytically tractable - we can get closed form locally optimal ns, possibly max regret too.

Note - following on from the above, could we consider an extension to using expected effect as a metric to balance against sample size, where exp effect is the true effect multiplied by the trial power. Again, we'd be assuming that the trade-off is linear. The essential method would be the same, looking for an such that we get minimax regret over the unkown parameter space, but the latter is now 2D with delats and sigma both unknown.

Consider a two-arm trial with a normal endpoint which will compare the means in each group via a t-test. Denote the variance of the endpoint (common across arms) by $\sigma^2$, and the target difference under the alternative hypothesis (which the trial will be powered to detect) by $\delta_a$. Let the sample size in each arm of the trial be denoted by $n$.

We propose two value functions:
$$
\begin{align}
v_1(n; \lambda_1, c_1) & = \overbrace{1 - \Phi\left(z_{1 - \alpha} - \frac{\delta_a}{\sqrt{2\sigma^2/n}}\right)}^\textrm{Power} - \lambda_1 n - c_1 \\
v_2(n; \lambda_2, c_2) & = \underbrace{\sqrt{\frac{n}{2\sigma^2}}}_\textrm{Precision} - \lambda_2 n - c_2.
\end{align}
$$

Each value function has a benefit component (the power of the trial and the sampling precision of the estimated mean difference, respectively) offset by a cost component (a multiple of the sample size in each case). These value functions imply a constant trade-off between the sample size of the trial and its power/precision, with the rate given by the parameters $\lambda_1$ and $\lambda_2$. They also include fixed set-up costs, $c_1$ and $c_2$.

To illustrate these value functions, we plot contours and overlay these with specific power and precision curves. Note that we have fixed $\delta_a = 0.3$ and $\alpha = 0.025$.

```{r}
v1 <- function(n, sig, lambda1, c1) {
  delta_a <- 0.3
  1-pt(qt(0.975, 2*(n-1)), 2*(n-1), 0.3/sqrt(2*sig^2/n)) - lambda1*n - c1
}

v2 <- function(n, sig, lambda2, c2) {
  sqrt(n/(2*sig^2)) - lambda2*n - c2
}

# Define some example trade-offs
lambda1 <- c(0.00224, 0.00392); lambda2 <- c(0.02668, 0.0337)
# These have been chosen to give optimal sample sizes 
# of 176 and 110, corresponding to power of 60 and 80% to 
# detect an effect of 0.3 when sigma = 1
# ns <- 2:500; ns[which.max(sapply(ns, v1, sig = 1, c1 = 0, lambda1 = 0.01))]

# Take a common fixed cost in units of sample size
n_cost <- 15
c1 <- lambda1*n_cost; c2 <- lambda2*n_cost

plots <- vector("list", 4)
for(i in 1:4){
  lambda <- c(lambda1, lambda2)[i]
  C <- c(c1, c2)[i]
  
  df <- data.frame(n=seq(4,500,1))
  df <- rbind(df, df)
  df$sig <- c(rep(1, nrow(df)/2), rep(1.3*1, nrow(df)/2))
  if(i < 3){
    df$v <- v1(n = df$n, sig = df$sig, lambda1 = lambda, c1 = C)
  } else {
    df$v <- v2(n = df$n, sig = df$sig, lambda2 = lambda, c2 = C)
  }
  df$p <- df$v + lambda*df$n + C
  
  opt1 <- df[which.max(df$v*(df$sig == 1)),]
  opt2 <- df[which.max(df$v*(df$sig == 1.3)),]
  
  gr <- expand.grid(n = seq(4, 500, l = 10),
                    p = seq(min(df$p), max(df$p), l = 10))
  gr$v <- gr$p - lambda*gr$n - C
  
  plots[[i]] <- ggplot(df, aes(n, p)) + geom_line(aes(colour=as.factor(sig))) +
    geom_hline(yintercept = 0.8, linetype=2) +
    geom_contour(data = gr, aes(z = v), alpha = 0.5, colour = cols[3]) +
    geom_contour(data = gr, aes(z = v), breaks = c(C), colour = cols[4]) +
    geom_point(data = df[as.numeric(row.names(rbind(opt1, opt2))), ]) +
    xlab("Sample size") +
    scale_colour_manual(values=cols[1:2], labels=round(c(sig, 1.3*sig), 2)) +
    labs(colour = "SD") +
    theme_minimal()
  
  if(i < 3){
    plots[[i]] <- plots[[i]] + ylab("Power")
  } else {
    plots[[i]] <- plots[[i]] + ylab("Precision")
  }
}

(plots[[1]] + plots[[3]]) /(plots[[2]] + plots[[4]]) + plot_layout(guides = "collect") & theme(legend.position = 'bottom')
```

This figure shows the indifference curves in each case. Power as a function of $n$ is shown for two possible values of the nuisance parameter $\sigma$ and an MCID of 0.3, and the optimal (value-maximising) sample size is highlighted for each of these. The pink line denotes the contour below which it becomes optimal to not run the trial at all due to the setup costs.

The scenarios have been chosen so that the optimal design when $\sigma = 1$ corresponds to a power of 0.8 (top row) or 0.6 (bottom row). They also differ in the parameter $\gamma$ used in the power component of the value function,
$$
-\frac{\log([1-\beta]\gamma + 1)}{\log(\gamma + 1)},
$$
where $\gamma = 0.00001$ (left side) or $\gamma = 2$ (right side).

For these four value scenarios, how does the optimal sample size vary with the true value of $\sigma$? How do these compare against the sample sizes obtained by the usual approach of minimising $n$ subject to a power constraint?

```{r}
plots2 <- vector("list", 4)
fits <- vector("list", 4); lims <- NULL
for(i in 1:nrow(params)){
  lambda <- params[i, 1]; gamma <- params[i, 2]
  
  df <- data.frame(sig=seq(0.1,5,0.01))
  df$n <- sapply(df$sig, function(x) which.min(v(3:1000, pow(3:1000, x), lambda, gamma)) + 2)
  df$v <- v(df$n, pow(df$n, df$sig), lambda, gamma)
  df$n <- df$n*(df$v < 0)
  df$t <- "v"
  
  # Storing GAM approximations for later
  fits[[i]] <- gam(n ~ s(sig), data = df[df$n > 0,])
  lim <- df[df$n > 0, "sig"]
  lim <- lim[length(lim)]^2
  lims <- c(lims, lim)

  df2 <- data.frame(sig=seq(0.1,5,0.01))
  df2$n <- sapply(df$sig, function(x) which(pow(3:1000, x) >= (0.8*(i %in% c(1,2)) + 0.6*(i %in% c(3,4))))[1] + 2)
  df2$t <- "c"

plots2[[i]] <- ggplot(rbind(df[,-3], df2), aes(sig, n, colour=t, linetype=t)) + geom_line() +
  theme_minimal() +
  ylab("Sample size") + xlab("Standard deviation") +
  scale_colour_manual(name="Method", values=cols[1:2], labels=c("C", "V")) +
  scale_linetype_manual(name="Method", values=c(1,2), labels=c("C", "V"))
}

(plots2[[1]] + plots2[[2]]) /(plots2[[3]] + plots2[[4]]) + plot_layout(guides = "collect") & theme(legend.position = 'bottom')
```



When given a point and interval estimate of $\sigma$, we can choose $n$ to minimise the maximum regret over that interval. 

Look at a specific point estimate and interval. For each scenario, plot the values of (i) the optimal n, (ii) the minimax n, and (iii) the locally optimal n at the point estimate.

[Add value of upper CI design; place these after the minimax graphs]

```{r}
sig <- 1; k <- 30
lo <- sqrt(k*sig^2/qchisq(0.975, k)) 
up <- sqrt(k*sig^2/qchisq(0.025, k)) 

plots4 <- vector("list", 4)
for(i in 1:nrow(params)){
  # Now compute an n x sig matrix of values
  lambda <- params[i,1]; gamma <- params[i, 2]
  sigs <- seq(0.1, 3, l = 200)
  ns <- c(0, 2:500)
  
  vs <- matrix(rep(NA, length(sigs)*length(ns)), ncol = length(sigs))
  for(j in 1:nrow(vs)){
    for(k in 1:ncol(vs)){
      p <- 0
      if(ns[j] != 0 ) p <- pow(ns[j], sigs[k])
      vs[j, k] <- v(ns[j], p, lambda, gamma)
    }
  }
  
  # Get value of optimal designs
  opt_v_all <- -apply(vs, 2, min)
  
  # Get minimax design
  vs_sub <- vs[, sigs >= lo & sigs <= up]
  opt_v <- -apply(vs_sub, 2, min)
  regs <- t(t(vs_sub) + opt_v)
  n <- ns[which.min(apply(regs, 1, max))]
  minmax_v <- -vs[which.min(apply(regs, 1, max)),]
    
  # Use the point estimate
  vals <- v(ns[-1], pow(ns[-1], df$sig[j]), lambda, gamma)
  vals <- c(v(0, 0, lambda, gamma), vals)
  n_fix <- ns[which.min(vals)]
  fix_v <- -vs[which.min(vals),]
  #fix_v <- -vs[20,]
  
  df <- data.frame(sig = rep(sigs, 3),
                   v = c(opt_v_all, minmax_v, fix_v),
                   t = rep(c("Local", "Minimax", "Fixed"), each = length(sigs)))
  
  plots4[[i]] <- ggplot(df, aes(sig, v, colour = t)) + geom_line() +
    scale_colour_manual(name = "", values = cols) +
    geom_vline(xintercept = c(lo, up), linetype = 2) +
    ylab("Value") + xlab("Standard deviation") +
    theme_minimal()
}

(plots4[[1]] + plots4[[2]]) /(plots4[[3]] + plots4[[4]]) + plot_layout(guides = "collect") & theme(legend.position = 'bottom')
```

Now find the optimal (i.e. minimax regret) design for a range of point estimates and intervals (summarised by their dof).

```{r}
df <- expand.grid(sig = seq(0.3, 2, l = 50),
                  k = seq(10, 100, l = 50))
df$lo <- sqrt(df$k*df$sig^2/qchisq(0.975, df$k)) 
df$up <- sqrt(df$k*df$sig^2/qchisq(0.025, df$k)) 
df$n <- df$n_ci <- df$n_nct <- NA

plots3 <- vector("list", 4)
for(i in 1:nrow(params)){
  # Now compute an n x sig matrix of values
  lambda <- params[i,1]; gamma <- params[i, 2]
  sigs <- seq(min(df$lo), max(df$up), l = 200)
  ns <- c(0, 2:500)
  
  vs <- matrix(rep(NA, length(sigs)*length(ns)), ncol = length(sigs))
  for(j in 1:nrow(vs)){
    for(k in 1:ncol(vs)){
      p <- 0
      if(ns[j] != 0 ) p <- pow(ns[j], sigs[k])
      vs[j, k] <- v(ns[j], p, lambda, gamma)
    }
  }
  
  for(j in 1:nrow(df)){
    vs_sub <- vs[, sigs >= df[j, "lo"] & sigs <= df[j, "up"]]
    opt_v <- -apply(vs_sub, 2, min)
    regs <- t(t(vs_sub) + opt_v)
    df$n[j] <- ns[which.min(apply(regs, 1, max))]
    df$r[j] <- min(apply(regs, 1, max))
    
    # Use the point estimate only
    vals <- v(ns[-1], pow(ns[-1], df$sig[j]), lambda, gamma)
    vals <- c(v(0, 0, lambda, gamma), vals)
    df$n_fix[j] <- ns[which.min(vals)]
    df$r_fix[j] <- max(regs[which(ns == df$n_fix[j]),])
  
    # Use the upper CI point, as in Browne
    #df$n_ci[j] <- which(pow(3:1500, df$up[j]) >= 0.8)[1] + 2
        
    # Use the non-central t method of Julious
    #ns2 <- 3:1000
    #if(i == 1 | i == 2){
    #  z <- (2*qt(0.8, df$k[j], qt(0.975, 2*ns2 - 2))^2)*(df$sig[j]^2)/(0.3^2)
    #} else {
    #  z <- (2*qt(0.8, df$k[j], qt(0.975, 2*ns2 - 2))^2)*(df$sig[j]^2)/(0.3794427^2)
    #}
    #df$n_nct[j] <- ns2[ns2 >= z][1]
  }
  
  plots3[[i]] <- ggplot(df, aes(k)) + geom_tile(aes(y = sig, fill = n)) +
    scale_fill_viridis(discrete=FALSE, limits = c(0,250)) + 
      geom_line(data = df[df$sig == 1,], aes(y = up)) +
      geom_line(data = df[df$sig == 1,], aes(y = lo)) +
      ylim(c(0.3, 2)) +
    ylab("Point estimate SD") + xlab("Degrees of freedom") +
    theme_minimal()
}

(plots3[[1]] + plots3[[2]]) /(plots3[[3]] + plots3[[4]]) + plot_layout(guides = "collect") & theme(legend.position = 'bottom')
```

[To do - repeat above getting the constrained n and / or the upper CI n, and plot the differences, so we can show the practical implications. All might be better in SM / appendix]

```{r}
ggplot(df, aes(k)) + geom_tile(aes(y = sig, fill = r_fix - r)) +
    scale_fill_viridis(discrete=FALSE) + #, limits = c(0,250)) + 
      geom_line(data = df[df$sig == 1,], aes(y = up)) +
      geom_line(data = df[df$sig == 1,], aes(y = lo)) +
      ylim(c(0.3, 2)) +
    ylab("Point estimate SD") + xlab("Degrees of freedom") +
    theme_minimal()
```

For a specific estimate and interval, plot the minimax regret for each scenario for each n and highlight the optimal n and other choices.

```{r}
sig <- 1; k <- 30
lo <- sqrt(k*sig^2/qchisq(0.975, k)) 
up <- sqrt(k*sig^2/qchisq(0.025, k)) 

plots5 <- vector("list", 4)
for(i in 1:nrow(params)){
  # Now compute an n x sig matrix of values
  lambda <- params[i,1]; gamma <- params[i, 2]
  sigs <- seq(0.1, 3, l = 200)
  ns <- c(0, 2:500)
  
  vs <- matrix(rep(NA, length(sigs)*length(ns)), ncol = length(sigs))
  for(j in 1:nrow(vs)){
    for(q in 1:ncol(vs)){
      p <- 0
      if(ns[j] != 0 ) p <- pow(ns[j], sigs[q])
      vs[j, q] <- v(ns[j], p, lambda, gamma)
    }
  }
  
  # Get minimax design
  vs_sub <- vs[, sigs >= lo & sigs <= up]
  opt_v <- -apply(vs_sub, 2, min)
  regs <- t(t(vs_sub) + opt_v)
  n <- ns[which.min(apply(regs, 1, max))]
  
  # Minimax regret for all ns
  minmax_regs <- apply(regs, 1, max)
    
  # Use the point estimate
  vals <- v(ns[-1], pow(ns[-1], sig), lambda, gamma)
  vals <- c(v(0, 0, lambda, gamma), vals)
  n_fix <- ns[which.min(vals)]
  
  # Use the upper CI point, as in Browne
  n_ci <- which(pow(3:1500, up) >= 0.8)[1] + 2
        
  # Use the non-central t method of Julious
  #ns2 <- 3:1000
  #if(i == 1 | i == 2){
  #  z <- (2*qt(0.8, k, qt(0.975, 2*ns2 - 2))^2)*(sig^2)/(0.3^2)
  #} else {
  #  z <- (2*qt(0.8, k, qt(0.975, 2*ns2 - 2))^2)*(sig^2)/(0.3794427^2)
    #z <- (2*qt(0.6, k, qt(0.975, 2*ns2 - 2))^2)*(sig^2)/(0.3^2)
  #}
  #n_nct <- ns2[ns2 >= z][1]
  
  df <- data.frame(n = ns,
                   r = minmax_regs)
  
  df2 <- rbind(df[df$n == n,],
               df[df$n == n_fix,],
               df[df$n == n_ci,])
               #df[df$n == n_nct,])
  
  df2$t <- c("Minimax", "Fixed", "Upper CI")

  plots5[[i]] <- ggplot(df, aes(n, r)) + geom_line() +
      geom_point(data = df2, aes(colour = t)) +
    scale_colour_manual(name = "", values = cols[c(1,3,4,5)]) +
    ylab("Maximum regret") + xlab("Sample size") +
    theme_minimal()
}

(plots5[[1]] + plots5[[2]]) /(plots5[[3]] + plots5[[4]]) + plot_layout(guides = "collect") & theme(legend.position = 'bottom')
```


It might be useful to give a table of (minimax regret) optimal sample sizes for a range of scenarios. One approach is to determine value function parameters such that the (value) optimal sample size is typical in some way. For example, we could find a value function corresponding to choosing 80% power to detect a standardised effect size of 0.3.

Note that standardised effect size is all that matters, i.e. we can fix the standard deviation in our examples.

Need a function to determine $\lambda$, given $\gamma$, $\mu$, $\sigma$ and $\beta$.

```{r}
df <- expand.grid(gamma = c(0.0001, 2, 20),
                  ses = c(0.3, 0.5, 0.7),
                  beta = c(0.1, 0.2))

df$n <- apply(df, 1, function(z) power.t.test(delta = z[2], power = 1 - z[3])$n)

f_lambda <- function(lambda, gamma, ses, beta)
{
  df2 <- data.frame(n=seq(4,5000,1))
  df2$sig <- 1
  df2$p <- 1-pt(qt(0.975, 2*(df2$n-1)), 2*(df2$n-1), ses/sqrt(2/df2$n))
  df2$v <- v(df2$n, df2$p, lambda, gamma)
  
  n_v <- df2[which.min(df2$v), "n"]
  n_v
}

get_lambda <- function(gamma, ses, beta, target)
{
  # Bisection search
  lo <- 0.000001; hi <- 1
  conv <- F
  while(!conv){
    l <- (lo + hi)/2
    n_v <- f_lambda(l, gamma, ses, beta)
    dif <- n_v - target
    if( dif > 0 & dif < 1){
      conv <- T
    } else {
      if(dif < 0){
        hi <- l
      } else {
        lo <- l
      }
    }
  }
  l
}

df$lambda <- apply(df, 1, function(z) get_lambda(z[1], z[2], z[3], z[4]))

df <- cbind(df, k = rep(c(5, 10, 20, 40, 80), each = nrow(df)))

# Now get minimax regret design for each scenario and a range of pilot dof
get_mm_reg <- function(lambda, gamma, k, ses)
{
  lo <- sqrt(k*1^2/qchisq(0.975, k)) 
  up <- sqrt(k*1^2/qchisq(0.025, k)) 
  
  sigs <- seq(lo, up, l = 200)
  ns <- c(0, 2:500)
  
  vs <- matrix(rep(NA, length(sigs)*length(ns)), ncol = length(sigs))
  for(j in 1:nrow(vs)){
    for(i in 1:ncol(vs)){
      p <- 0
      if(ns[j] != 0 ) p <- 1-pt(qt(0.975, 2*(ns[j]-1)), 2*(ns[j]-1), ses/sqrt(2*sigs[i]/ns[j]))
      vs[j, i] <- v(ns[j], p, lambda, gamma)
    }
  }
  
  opt_v <- -apply(vs, 2, min)
  regs <- t(t(vs) + opt_v)
  ns[which.min(apply(regs, 1, max))]
}

df$mm_reg <- apply(df, 1, function(z) get_mm_reg(z[5], z[1], z[6], z[2]))


```


## Uncertain effect size

Can we extend these ideas to deal with uncertainty in the effect? One approach is to change our value function to one focussed on expected effectiveness, rather than on power (or precision). That is, we assume a linear trade-off between sample size and power x effect. We then proceed as before, but now we have a 2D parameter space. We may find that the choice of n is not as robust to movement in this space.

So main methods (power and precision), extension (effectiveness), and illustrative application?

```{r}
# Define s[ace to get value over]
deltas <- seq(-0.25, 0.75, length.out = 10^2)
sigs <- seq(0.5, 1.5, length.out = 10^2)
ns <- 2:50

df <- expand.grid(d = deltas,
                  s = sigs,
                  n = ns)

# Calculate value at each point
lambda <- 0.001
df$v  <- df$d*(1-pt(qt(0.975, 2*(df$n-1)), 2*(df$n-1), df$d/sqrt(2*df$s^2/df$n))) - lambda*df$n

# Reshape into params x n format
df2 <- reshape(df, direction = "wide", idvar = c("d", "s"), timevar = "n", v.names = "v")

# Plot value of optimal n at each parameter point
df3 <- df2[,1:2]
df3$v <- apply(df2[,3:ncol(df2)], 1, max)

ggplot(df3, aes(d)) + geom_tile(aes(y = s, fill = v)) +
    scale_fill_viridis(discrete=FALSE) +
    theme_minimal()

# Plot value of fixed n at each parameter point
ns <- 2:50
vs <- 0.25*(1-pt(qt(0.975, 2*(ns-1)), 2*(ns-1), 0.25/sqrt(2*1^2/ns))) - lambda*ns
fix_n <- ns[which.max(vs)]
df4 <- df3
df4$v <- df2[,2 + which.max(vs)]

ggplot(df4, aes(d)) + geom_tile(aes(y = s, fill = v)) +
    scale_fill_viridis(discrete=FALSE) +
    theme_minimal()

# Plot difference in value
ggplot(df4, aes(d)) + geom_tile(aes(y = s, fill = v - df3$v)) +
    scale_fill_viridis(discrete=FALSE) +
    theme_minimal()

# Find the optimal (minimax regret) n
df5 <- df2
df5[,3:ncol(df5)] <- df3$v - df5[,3:ncol(df5)]
max_reg <- apply(df5[,3:ncol(df5)], 2, max)
opt_n <- ns[which.min(max_reg)]

# Plot value of opt n at each parameter point
df5$v <- df2[,2 + which.min(max_reg)]

ggplot(df5, aes(d)) + geom_tile(aes(y = s, fill = v)) +
    scale_fill_viridis(discrete=FALSE) +
    theme_minimal()

# Plot difference in value
ggplot(df5, aes(d)) + geom_tile(aes(y = s, fill = v - df3$v)) +
    scale_fill_viridis(discrete=FALSE) +
    theme_minimal()
```

So we find here that there is more sensitivity over typical ranges, that a standard fixed design or central estimates performs poorly, but that we can improve significantly by finding the minimax design.

# Superceded

## Binary cRCT

If we have the effectiveness measure above, we have already looked at a 2D parameter space. So do we need this example? Or is it enough to put in the discussion that it's easy to apply to other cases, e.g. using a power metric in a binary setting with control rate in place of the sd? Could potentially include this (and others?) in an appendix. Or go in the other direction and make it explicitly limited to our continuous case, but then make the most of this by developing an R package which implements the above methods and thus helps with uptake? 

Rutterford2015

To illustrate the genral approach being applied to a different and more complex problem, consider a cRCT with a binary outcome. Power is then a function of the control group rate and the between cluster variance / the ICC.


```{r}
pow <- function(m, n, var_b, p_0)
{
  # Large sample power
  p_1 <- p_0 - 0.2
  1 - pnorm(qnorm(0.975) - 0.2/sqrt((2*var_b + p_0*(1 - p_0)/n + p_1*(1 - p_1)/n)/m))
}

v <- function(m, n, p, lambda, gamma)
{
  # Value function
  -(log(p*gamma + 1)/log(gamma + 1) - lambda[1]*m - lambda[2]*m*n)# - lambda[1]*4*(m > 0))
}

pow(m = 21, n = 7, var_b = 0.021, p_0 = 0.7)
lambda <- grad(function(z) pow(z[1], z[2]/z[1], var_b = 0.021, p_0 = 0.7), c(21,7))
lambda <- 0.63*c(0.013839575, 0.0008678032)
gamma <- 2
  
df <- expand.grid(m = 2:50, n = c(7))
df$p_0 <- 0.7
df <- rbind(df, df)
  
df$var_b <- c(rep(0.021, nrow(df)/2), rep(0.042, nrow(df)/2))
df$p <- pow(df$m, df$n, df$var_b, df$p_0)

df$v <- v(df$m, df$n, df$p, lambda, gamma)
  
opt1 <- df[which.min(df$v - 10000*(df$n == 7 & df$var_b == 0.021)),]
opt2 <- df[which.min(df$v - 10000*(df$n == 7 & df$var_b == 0.042)),]
opt1
  
gr <- expand.grid(m = seq(2, 50, l = 10),
                  p = seq(0, 1, l = 10))
gr$v <- v(gr$m, n = 7, gr$p, lambda, gamma)
  
ggplot(df, aes(m, p)) + 
  geom_line(aes(colour=as.factor(var_b))) +
    geom_hline(yintercept = 0.8, linetype=2) +
    geom_contour(data = gr, aes(z = v), alpha = 0.5, colour = cols[3]) +
    geom_contour(data = gr, aes(z = v), breaks = c(0), colour = cols[4]) +
    geom_point(data = df[as.numeric(row.names(rbind(opt1, opt2))), ]) +
    ylab("Power") + xlab("Sample size") +
    scale_colour_manual(values=cols[1:2]) +
    labs(colour = expression(sigma[b]^2)) +
    theme_minimal()
```

```{r}
df <- expand.grid(p_0 = seq(0.2001, 1, l=100),
                  var_b = seq(0.0001, 0.5, l = 100))

sols <- expand.grid(m = 2:40,
                    n = 2:70)

opt_sols_ids <- apply(df, 1, function(z) which.min(v(sols$m, sols$n, pow(sols$m, sols$n, as.numeric(z[2]), as.numeric(z[1])), lambda, gamma)))

df$m <- sols[opt_sols_ids, "m"]
df$n <- sols[opt_sols_ids, "n"]
df$v <- v(df$m, df$n, pow(df$m, df$n, df$var_b, df$p_0), lambda, gamma)

# Get fixed design for later, using point estimates in the middle of the intervals
fixed <- df[df$p_0 > 0.7 & df$var_b > 0.025,][1, 3:4]


plot1 <- ggplot(df, aes(p_0, var_b, fill = m)) + geom_tile() +
  theme_minimal() +
  scale_fill_gradient(low="white", high=cols[4]) +
  xlab("Control probability") + ylab("Between cluster variance") +
  labs(fill = "m")

plot2 <- ggplot(df, aes(p_0, var_b, fill = n*m)) + geom_tile() +
  theme_minimal() +
  scale_fill_gradient(low="white", high=cols[5]) +
  xlab("Control probability") + ylab("Between cluster variance") +
  labs(fill = "m x n")

plot1 + plot2
```
Find the minimax regret design, plot maximum regret.

```{r}
p_0_int <- c(0.6, 0.8)
var_b_int <- c(0.01, 0.04)

get_reg <- function(x, m, n)
{
  var_b <- x[1]; p_0 <- x[2]
  # Get locally optimal design
  sols <- expand.grid(m = 2:50,
                    n = 2:70)

  l_opt_id <- which.min(v(sols$m, sols$n, pow(sols$m, sols$n, var_b, p_0), lambda, gamma))
  l_m <- sols[l_opt_id, "m"]; l_n <- sols[l_opt_id, "n"]
  l_opt_v <- v(l_m, l_n, pow(l_m, l_n, var_b, p_0), lambda, gamma)
  
  - v(m, n, pow(m, n, var_b, p_0), lambda, gamma) + v(l_m, l_n, pow(l_m, l_n, var_b, p_0), lambda, gamma)
}

get_max_reg <- function(x)
{
  m <- x[1]; n <- x[2]
  optim(c(0.02, 0.7), get_reg,
        lower = c(var_b_int[1], p_0_int[1]), upper = c(var_b_int[2], p_0_int[2]),
        method = "L-BFGS-B",
        m = m, n = n)$value
}

sols$max_r <- apply(sols, 1, get_max_reg)
sols_points <- sols[which.max(sols$max_r),]
sols_points <- rbind(sols_points, sols[sols$m == fixed$m & sols$n == fixed$n,])
sols_points$t <- c("Minimax", "Fixed")

ggplot(sols, aes(m, n)) + 
  geom_tile(aes(fill = max_r)) + 
  geom_contour(aes(z = max_r)) +
  geom_point(data = sols_points, aes(shape = t)) +
  scale_fill_gradient(low="white", high=cols[5]) +
  theme_minimal() +
  labs(fill = "Max regret",
       shape = " ")
```

Plot regret for the optimal design.

```{r}
p_0_int <- c(0.6, 0.8)
var_b_int <- c(0.01, 0.04)

mm_opt <- sols_points[1,1:2]

df <- expand.grid(var_b = seq(0.0001, 0.05, l=100),
                  p_0 = seq(0.5, 0.9, l=100))
df$reg <- apply(df, 1, function(z) get_reg(z[1:2], m = mm_opt$m, n = mm_opt$n))

ggplot(df, aes(p_0, var_b, fill = reg)) + geom_tile() + 
#ggplot(df, aes(p_0, var_b, z = reg)) + geom_contour() + 
  geom_rect(xmin = 0.6, xmax = 0.8, ymin  = 0.01, ymax = 0.04, fill = NA, colour = "black", linetype = 2) +
  xlab("Control probability") + ylab("Between cluster variance") +
  labs(fill = "Regret") +
  theme_minimal()
```



## Running example

Value function is
$$
v(n, \beta) = \beta + \lambda n.
$$
```{r}
value <- function(x, lambda)
{
  n <- x[1]; pow <- x[2]
  pow - lambda*n
}
```


Plotting value for a two-sample t-test over a range of $n$:
```{r, echo=F}
sliderInput("lambda", "Choose lambda", 0, 0.1, 0.025, step=0.01, animate = T)
sliderInput("sd", "Choose sd", 0.5, 2, 1, step=0.1)

renderPlot({

  df <- expand.grid(n=2:50, pow=1)
  df$pow <- sapply(df$n, function(n) power.t.test(n=n, delta=1, sd=input$sd)$power)
  df$v <- apply(df, 1, value, lambda = input$lambda)

  opt <- df[which.max(df$v),]
  
  const <- data.frame(n=ceiling(power.t.test(delta=1, sd=input$sd, power=0.8)$n),
                      pow=0.8)

  # value contours
  df2 <- expand.grid(n=2:50, pow=seq(0,1,0.1))
  df2$v <- apply(df2, 1, value, lambda=input$lambda)

  ggplot(df2, aes(n, pow)) + geom_contour(aes(z=v, colour=..level..)) +
    geom_point(data=df) + geom_point(data=opt, colour=cols[1]) +
    #geom_abline(slope=input$lambda, intercept = opt$p - input$lambda*opt$n, colour="red") +
    geom_segment(aes(x=opt$n, y=opt$pow, xend=opt$n, yend=0), colour=cols[1], linetype=2) +
    geom_point(data=const, colour=cols[2]) +
    geom_segment(aes(x=const$n, y=const$pow, xend=const$n, yend=0), colour=cols[2], linetype=2) +
    ylab("Power") + theme_minimal()
  
})
```

```{r, echo=F}
lambda <- 0.025
df <- expand.grid(n=2:50, pow=1)
df$pow <- sapply(df$n, function(n) power.t.test(n=n, delta=1, )$power)
df$v <- apply(df, 1, value, lambda = lambda)

opt <- df[which.max(df$v),]

# value contours
df2 <- expand.grid(n=2:50, pow=seq(0,1,0.05))
df2$v <- apply(df2, 1, value, lambda=lambda)

ggplot(df2, aes(n, pow)) + geom_contour(aes(z=v, colour=..level..)) +
  geom_point(data=df) + geom_point(data=opt, colour="red") +
  geom_abline(slope=lambda, intercept = opt$p - lambda*opt$n, colour="red") +
  ylab("Power") + theme_minimal()

#ggsave("./paper/figures/ex1_tangent.pdf", height=9, width=14, units="cm")
```

As the figure suggests, if our assumption about the form of the value function holds then we can determine the value of $\lambda$ by simply plotting the power curve and choosing $n$; $\lambda$ is then the gradiant of the tangent of the power curve at that point. Is this an accurate representation of our preferences? Do we aggree with its implications? Note that, for example, $\lambda = 0.025$ implies that $(n=0, \beta=0) \sim (n=40, \beta=1)$, and so 40 is the maximum sample size we would ever consider. When it comes to determining $\lambda$, we could use any hypothetical "power" curve and if our assumptions hold we should always choose the sample size that gives the same tangent gradient. For example, consider two other power curves obtained by changing the standard deviation from 1 to 0.7 and 1.3:

```{r}

df3 <- expand.grid(n=2:50, pow=1)
df3$pow <- sapply(df3$n, function(n) power.t.test(n=n, delta=1, sd=0.7)$power)
df3$v <- apply(df3, 1, value, lambda = lambda)
opt3 <- df3[which.max(df3$v),]

df4 <- expand.grid(n=2:50, pow=1)
df4$pow <- sapply(df4$n, function(n) power.t.test(n=n, delta=1, sd=1.3)$power)
df4$v <- apply(df4, 1, value, lambda = lambda)
opt4 <- df4[which.max(df4$v),]

ggplot(df2, aes(n, pow)) + geom_contour(aes(z=v, colour=..level..)) +
  geom_point(data=df) + geom_point(data=opt, colour="red") +
  geom_abline(slope=lambda, intercept = opt$p - lambda*opt$n, colour="red") +
  
  geom_point(data=df3, shape=15) + geom_point(data=opt3, colour="red") +
  geom_abline(slope=lambda, intercept = opt3$p - lambda*opt3$n, colour="red") +
  
  geom_point(data=df4, shape=17) + geom_point(data=opt4, colour="red") +
  geom_abline(slope=lambda, intercept = opt4$p - lambda*opt4$n, colour="red") +
  
  geom_hline(yintercept = 0.8, linetype=2) +
  
  ylab("Power") + theme_minimal()

#ggsave("./paper/figures/ex1_3tangents.pdf", height=9, width=14, units="cm")

cbind(rbind(opt3, opt, opt4), sd=c(0.7, 1, 1.3))
```

In the above we have proposed a _normative_ model for choosing the sample size of a trial, and argured that it may also be a _descriptive_ model of what happens in practice, in which case there would be no benefit of using it as it will lead to the same decisions. However, the potential implications of our model are hinted at in the above example - when the true power function is not known, as will be the case whenever we are uncertain about a nuisance parameter value, using a value function to make decisions provides a unified approach which will ensure consistency. 

Contrast with a common method for dealing with nuisance parameter uncertainty, sample size re-estimation (SSR). Following on from the example above, suppose we guess that $\sigma = 1$ and then choose $n = 17$ to give 80\% power. Under the SSR approach, if we learn from an interim analysis that $\sigma = 1.3$ then we should inflate our sample size to $n = 27$ to maintain the same power. Under our value model, this behaviour is not internally coherent. We are now saying that an increase in power of `r power.t.test(n=27, delta=1, sd=1.3)$power - power.t.test(n=26, delta=1, sd=1.3)$power` justifies an increase in sample size from 26 to 27; but when we did our original calculations, we felt that the increase of `r power.t.test(n=18, delta=1, sd=1)$power - power.t.test(n=17, delta=1, sd=1)$power` obtained by moving from 17 to 18 was _not_ justified.

Another way to see the flaw in SSR is to consider an extreme scenrio, e.g. discovering that $\sigma = 10$. Following SSR we should then increase the sample size to `r power.t.test(delta=1, sd=10, power=0.8)$n`!! Of course in practice such a discover would mean the trial is terminated - but the SSR methods available do not provide any guidance on exactly when we should decide the inflation is simply too much. Only by explicitly incoprorating cost into the method can we avoid such problems. 

As suggested by the example above, an interesting property of our method when applied to a problem with normally distributed outcomes is that as $\sigma$ increases, the required sample size does not necessarily increase (in contrast with the SSR apporach). We can illustrate by extending the previous plot, showing the power curve for a larger range of $\sigma$ along with their optimal sample sizes:

```{r}
df <- expand.grid(n=2:70, sig=seq(0.3,2,0.05))
df$pow <- apply(df, 1, function(x) power.t.test(n=x[1], delta=1, sd=x[2])$power)
df$v <- df$pow - lambda*df$n

f <- function(n, sig)
{
  pow <- power.t.test(n=n, delta=1, sd=sig)$power
  -(pow - lambda*n)
}

opt <- data.frame(sig=seq(0.3,2,0.05), n=1)
opt$n <- sapply(opt$sig, function(x) optim(10, f, method="Brent", lower=2, upper=50, sig=x)$par)
opt$pow <- apply(opt, 1, function(x) power.t.test(n=x[2], delta=1, sd=x[1])$power)
opt$inter <- opt$pow - lambda*opt$n

tangent <- df[,1:2]
tangent$pow <- apply(tangent, 1, function(x, opt) opt[opt$sig == x[2],]$inter + lambda*x[1], opt=opt)

const <- data.frame(sig=seq(0.3,2,0.05), n=1)
const$n <- sapply(const$sig, function(s) power.t.test(delta=1, sd=s, power=0.8)$n)
const$pow <- 0.8

#ggplot(df, aes(n, pow, colour=sig, group=as.factor(sig))) + geom_line() +
#  geom_point(data=opt, colour="red") +
#  geom_vline(xintercept = max(opt$n), linetype=2, colour="red")

p <- ggplot(df, aes(n, pow)) + geom_line() +
  geom_point(data=opt, colour=cols[1]) + geom_line(data=tangent, colour=cols[1], linetype=2) +
  geom_point(data=const, colour=cols[2]) + geom_hline(yintercept = 0.8, colour=cols[2], linetype=2) +
  ylab("Power") + scale_y_continuous(breaks=seq(0,1,0.2), limits=c(0,1)) + theme_minimal() +
  transition_time(time=sig)
 
a_gif <- animate(p, rewind=T)

p2 <- ggplot(opt, aes(y=n)) + 
  geom_point(aes(x=sig), colour=cols[1], size=2) + 
  geom_point(data=const, aes(x=sig), colour=cols[2], size=2) +
  geom_line(data=data.frame(sig2=opt$sig, n=opt$n), aes(x=sig2), colour=cols[1]) +
  geom_line(data=data.frame(sig2=const$sig, n=const$n), aes(x=sig2), colour=cols[2]) +
  theme_minimal() +
  transition_time(time=sig)  

b_gif <- animate(p2, rewind=T)

a_mgif <- image_read(a_gif)
b_mgif <- image_read(b_gif)

new_gif <- image_append(c(a_mgif[1], b_mgif[1]))
for(i in 2:100){
  combined <- image_append(c(a_mgif[i], b_mgif[i]))
  new_gif <- c(new_gif, combined)
}

new_gif

#opt[which.max(opt$n),]
```

Clearly, the optimal sample size under our formulation is much less sensitive to the nuisance parameter. Can we choose a fixed sample size design, and thus avoid the logistic and practical difficulties associated with interim analyses? To start, we can look at the original design choice based on the best estimate of the nuisance parameter. We can calculate the value of that design accross a range of parameter values, and contrast with the value of the optimal design:

```{r}
lambda <- grad(function(n) power.t.test(n=n, delta=0.3, sd=1)$power, 235)

get_value <- function(n, sig, lambda)
{
  pow <- power.t.test(n=n, delta=0.3, sd=sig)$power
  -(pow - lambda*n)
}

opt <- data.frame(n= 22, sig=seq(0.3,3,0.05))

opt$n <- sapply(opt$sig, function(x) optim(10, get_value, method="Brent", lower=2, upper=500, sig=x, lambda=lambda)$par)
opt$v <- apply(opt, 1, function(x) get_value(x[1],x[2], lambda))

rob <- data.frame(n= 235, sig=seq(0.3,3,0.05))
rob$v <- apply(rob, 1, function(x) get_value(x[1], x[2], lambda))

ggplot(opt, aes(sig, -v)) + geom_line() +
  geom_line(data=rob, colour="red")
```

We find that more generally any reasonable choice of n will in general not be far from the optimal n in terms of our value function. For example, if our original design was for n=22 to give 90% power at sd=1, keeping that n means that for sd in the range 0.75 to 2.3 (approx) the difference is within 0.05, which recall is in units of power. So we can avoid SSR and be pretty confident that even if the sd is significantly underestimated, we are not losing much. Changing the scale, asking for 90 power initially for an effect of 0.3 rather than 1, leads to the same differences in values for optimal and fixed designs.

Plotting the differences in the optimal and fixed values for a range of sds, we see that one rationale for the choice of fixed design is to restrict attention to a specific range of sds and find that which minimises the maximal difference. This is similar to the approach in [@Breukelen2015], although there the design problem is to choose number of clusrees and cluster size given a fixed overall budget; and the value measure they use is just the variance of the estimate. SO our approach is quite different, since we provide a way to decide how much overall budget should be used, and as shown below the use of precision as a value measure is not helpful since it does not plateu. Note that work is extended in [@Breukelen2018] to allow for heterogeneity in the costs and variances in each arm. The minimax approach is, however, going to be very sensitive to the "plausable" range of sds. It might be easier to simply plot the value curves for a range of ns and qualitatively judge them.

An alternative optimality criterion for a fixed design is to define a tollerable deviation from the optimal design's value, and to choose the fixed design which lies within this margin for the largest region of the parameter space. Apply this to the above example:

```{r, eval=F}
coverage <- function(fixed, opt, diff, lambda)
{
  df <- data.frame(sig = opt$sig)
  df$n <- fixed
  df$v <- sapply(df$sig, function(x) get_value(fixed, x, lambda))
  df$d <- df$v - opt$v
  -sum(df$d <= diff)
}

opt <- data.frame(sig=seq(0.3,2,0.01))
opt$n <- sapply(opt$sig, function(x) optim(10, get_value, method="Brent", lower=2, upper=500, sig=x, lambda=lambda)$par)
opt$v <- apply(opt, 1, function(x) get_value(x[2],x[1], lambda))

diff <- 0.02
o <- psoptim(200, coverage, opt=opt, diff=diff, lambda=lambda,
              lower = 2, upper = 500)
```

A key question now is - how sensitive is this optimal fixed design to the choice of the value parameter lambda? Given that there may be some variability in its choice, both within and between decision makers, robustness in this regard would make the methodology quite attractive.


Note that for the same lambda, the optimal designs and their values will be constant. As we change the fixed sample size, we change the value of sd for which the fixed design is also the optimal. We see that as this sd value reduces, the fixed value curve becomes more discrepant. In the above example, if we thought an sd of 0.7 was most likely but used the same lambda, we would get a trial of 95% power and n=145. But when we compare a fixed design of n=145 against the optimal the largest difference is around 0.127 - much larger than the 0.05 before. So our approach looks like it won't work well if the plausable sd values correspond with a very high power. In conrast, if lambda is the same but we think the sd is higher at 1.5, the optimal design is n=345 (giving power 75%) and as a fixed design this is very robust around sd=1.5. So broadly speaking, the lambda gives the optimal designs and the maximum sd for which a trial can be considered, and a fixed design approach works better the closer our plausable sd rage is to that maximum (or equivalently, the lower the power of the locally optimal design at the best guess). 

## Example - t-test


```{r}
regret <- function(sig, n, lambda, gamma)
{
  # Get the optimal design for that sig
   opt_n <- which.min(v(3:500, pow(3:500, sig), lambda, gamma)) + 2
   opt_pow <- pow(opt_n, sig)
   opt_n2 <- opt_n*(v(opt_n, opt_pow, lambda, gamma) < 0)
   opt_pow <- opt_pow*(v(opt_n, opt_pow, lambda, gamma) < 0)
   v(opt_n2, opt_pow, lambda, gamma) - v(n, pow(n, sig), lambda, gamma)
}

minimax <- function(n, lim)
{
  -optim(1, regret, lower = lim[1], upper = lim[2], method = "Brent",
        n = n, lambda = lambda, gamma = gamma)$value
}

res <- NULL
for(sig in 0.3/c(0.3, 0.5)){
  # For sample sd estimates corresponding to standardised effect sizes
  # of 0.3, 0.5, 0.8
  for(j in 1:5){
    # degrees of freedom from which sample sd = 1 was obtained
    k <- c(10000, 100, 50, 30, 12)[j]
    # upper CI
    up <- sqrt(k*sig^2/qchisq(0.025, k)) 
    # lower
    lo <- sqrt(k*sig^2/qchisq(0.975, k)) 
      
    for(i in 1:nrow(params)){
      lambda <- params[i,1]; gamma <- params[i, 2]
      
      # minimax regret n
      n <- optim(100, minimax, lower = 3, upper = 500, method = "Brent",
            lim = c(lo, up))$par
      
      res <- rbind(res, c(n, up-lo, sig, i))
    }
      
      # Use the upper CI point, as in Browne
      n_ci <- which(pow(3:500, up) >= 0.8)[1] + 2
      
      res <- rbind(res, c(n_ci, up - lo, sig, 5))
      
      # Use the non-central t method of Julious
      ns <- 3:500
      z <- (2*qt(0.8, k, qt(0.975, 2*ns - 2))^2)*(sig^2)/(0.3^2)
      n_nct <- 2*ns[ns >= z][1]
      
      res <- rbind(res, c(n_nct, up-lo, sig, 6))
  }
}

df <- as.data.frame(res)
names(df) <- c("n", "w", "sig", "m")

ggplot(df, aes(w, n, linetype = as.factor(sig), colour = as.factor(m))) + geom_line() +
  scale_colour_manual(name = "Method", values = cols,
                      labels = c("Value (i)",
                                 "Value (ii)",
                                 "Value (iii)",
                                 "Value (iv)",
                                 "Upper CI",
                                 "NCT")) +
  xlab("Confidence interval width") +
  theme_minimal()
```

So, for the four value functions we've looked at, the precision of the SD estimate makes a negligible difference to the minimaxed sample size. For both cases (an sd estimate of 1 and 0.6), less precision leads to a slight reduction in the sample size for all four example value functions. This contrasts with the upper CI and NCT methods, which both increase n as precision worsens.

This suggests that there is little to be gained from following the outlined procedure - eliciting the value function and minimaxing the regret over an interval - compared to simply using the point estimate and maximising value locally. The final sample size will not be much different.

Plot the value functions of the locally optimal designs and that of the optimal design for the point estimate, when the point estimate is 1.
```{r}
sig <- 1

comp_vals <- function(sig, n, lambda, gamma)
{
  # Get the optimal design for that sig
   opt_n <- which.min(v(3:500, pow(3:500, sig), lambda, gamma)) + 2
   opt_pow <-  pow(opt_n, sig)
   opt_n2 <- opt_n*( v(opt_n, opt_pow, lambda, gamma) < 0)
   opt_pow <- opt_pow*( v(opt_n, opt_pow, lambda, gamma) < 0)
   c(v(opt_n2, opt_pow, lambda, gamma), v(n, pow(n, sig), lambda, gamma))
}

plots3 <- vector("list", 4)
for(i in 1:nrow(params)){
  lambda <- params[i,1]; gamma <- params[i, 2]
  
  fix_n <- which.min(v(3:500, pow(3:500, sig), lambda, gamma)) + 2
  
  df <- data.frame(sig=seq(0.1,3,0.1))
  df <- cbind(df, t(sapply(df$sig, comp_vals, n = fix_n, lambda = lambda, gamma = gamma)))
  df2 <- data.frame(sig = df$sig,
                    value = df[,2] - df[,3])
  df <- melt(df, id.vars = "sig")
  
  plots3[[i]] <- ggplot(df, aes(sig, -value)) + geom_line(aes(colour = variable)) +
    geom_line(data = df2, linetype = 2) +
    scale_colour_manual(name = "", 
                        values = cols,
                        labels = c("Local", "Fixed")) +
    xlab("Standard deviation") + ylab("Value") +
    ylim(c(-0.5, 0.75)) +
    theme_minimal()
}

(plots3[[1]] + plots3[[2]]) /(plots3[[3]] + plots3[[4]])
```

In some cases, we see that there is very little difference in value between the fixed and the locally optimal design over a good range of SDs.

Now, consider the SSR problem. We have some estimate and interval of $\sigma$, and we are considering running a pilot trial to get a better one. This new estimate will thn be used to determine the main trial sample size, using the approach above. Now, the value of this whole process is uncertain since the pilot outcome is uncertain. We then need to move from value (deterministic) to utility (stochastic). We will assume we are risk-neitral, so these are the same thing and we can focus on expected value. To calculate this, we need to integrate over the pilot sampling distribution. We do this by first fidning the interval over which the optimal main sample size is not 0, and integrating over that. This makes things numerically stable, as it avoids discontinuities. We can speed things up by finding the optimal n for sigs and then model with a gam, so we can pull these optimal ns out repeatedly over the simulations in a vectorised way.

```{r}
exp_val_def<- function(n_p, lambda, gamma, w, sig, lim, g, fit)
{
  # Expected value of program with pilot sample size n_p, value params
  # lambda and gamma, and relative sampling cost w (low double means cheap pilot)
  
  # Shape and scale params of the sample variance dist
  k <- (n_p-1)/2; theta <- 2*sig^2/(n_p-1)
  
  # Transform quad nodes
  to_eval <- 0.5*(lim - 0.1^2)*g$nodes + 0.5*(lim + 0.1^2)
  
  # Find optimal ns for those variances
  ns <- predict(fit, newdata = data.frame(sig = sqrt(to_eval)))
  
  # Get overall value at each point, weighted by the sampling distribution
  vs <- v(ns, pow(ns, sig), lambda, gamma)*dgamma(to_eval, shape = k, scale = theta)
  
  # Calculate the integral as the weighted sum of these values
  # and add the penalty for the pilot sample size
  sum(g$weights*vs*0.5*(lim - 0.1^2)) + lambda*w*n_p #+ 0.3365181*w
}

n_p <- 15; w <- 1; sig <- 1.2

lambda <- params[i, 1]; gamma <- params[i, 2]

# Get the internal definite integral
ptm <- proc.time()
g <- gauss.quad(1000,"legendre")
exp_val_def(n_p, lambda, gamma, w, sig, lims[i], g, fits[[i]])
proc.time() - ptm

# Check using MC - simulate from sampling dist and take mean value
ptm <- proc.time()
k <- (n_p-1)/2; theta <- 2*sig^2/(n_p-1)
to_eval <- rgamma(10^5, shape = k, scale = theta)
to_eval <- to_eval[to_eval <= lims[i] & to_eval >= 0.1^2]
ns <- predict(fits[[i]], newdata = data.frame(sig = sqrt(to_eval)))
vs <- v(ns, pow(ns, sig), lambda, gamma) + lambda*w*n_p
mean(vs)
proc.time() - ptm
```


For each of the four value functions, we can plot the expected value for a range of $n_p$ and a number of weights, conditional on some true value of $\sigma$:

```{r}
sig <- 0.8
g <- gauss.quad(200,"legendre")
plots4 <- vector("list", 4)
for(i in 1:nrow(params)){
  print(i)
  lambda <- params[i, 1]; gamma <- params[i, 2]
  
  df <- expand.grid(n_p = 3:50,
                   w = c(0.2, 0.5, 0.8, 1))

  df$v <- apply(df, 1, function(x) exp_val_def(x[1], lambda, gamma, x[2], sig,
                                               lims[i], g, fits[[i]]))
  
  df2 <- data.frame(n_p = c(5, 15, 30, 50))
  df2$lo <- sqrt(df2$n_p*1^2/qchisq(0.975, df2$n_p))
  df2$up <- sqrt(df2$n_p*1^2/qchisq(0.025, df2$n_p))
  
  fix_ns <- apply(df2, 1, function(x) optim(100, minimax, lower = 3, upper = 500, method = "Brent", lim = c(x[2], x[3]))$par)
  df2$v <- v(fix_ns, pow(fix_ns, sig), lambda, gamma)

  plots4[[i]] <- ggplot(df, aes(n_p, -v)) + geom_line(aes(linetype = as.factor(w))) +
    geom_point(data = df2, colour = cols[1]) +
    theme_minimal() +
    xlab("Pilot sample size") + ylab("Expected value") +
    labs(linetype = "w")
}

(plots4[[1]] + plots4[[2]]) /(plots4[[3]] + plots4[[4]])
```

We can now define expected regret as the expected value of the pilot-main trial program, minus the value of the locally optimal main trial. This provides a basis for choosing $n_p$ - we want to minimise the maximum (worst-case) regret over our initial interval for $\sigma$.

```{r}
g <- gauss.quad(200,"legendre")
i <- 3
lambda <- params[i, 1]; gamma <- params[i, 2]
  
sigs <- seq(0.1, 3, 0.01)
n_ps <- c(3:30)
res <- matrix(rep(NA, length(sigs)*length(n_ps)), nrow = length(sigs))
for(j in 1:length(n_ps)){
  for(k in 1:length(sigs)){
    if(sigs[k] < sqrt(lims[i])){
      opt_n <- predict(fits[[i]], newdata = data.frame(sig = sigs[k]))
      opt_v <- v(opt_n, pow(opt_n, sigs[k]), lambda, gamma)
    } else {
      opt_n <- 0
      opt_v <- 0
    }
    res[k, j] <- exp_val_def(n_ps[j], lambda, gamma, 1, sigs[k], lims[i], g, fits[[i]]) - opt_v
  }
}
# Locally optimal n_p
plot(max.col(-res))

sigs <- seq(0.1, 3, 0.01)
ns <- 3:400
res2 <- matrix(rep(NA, length(sigs)*length(ns)), nrow = length(sigs))
for(j in 1:length(ns)){
  for(k in 1:length(sigs)){
    if(sigs[k] < sqrt(lims[i])){
      opt_n <- predict(fits[[i]], newdata = data.frame(sig = sigs[k]))
      opt_v <- v(opt_n, pow(opt_n, sigs[k]), lambda, gamma)
    } else {
      opt_n <- 0
      opt_v <- 0
    }
    res2[k, j] <- v(ns[j], pow(ns[j], sigs[k]), lambda, gamma) - opt_v
  }
}
# Locally optimal n
plot(max.col(-res2))

minmax_np <- function(est, k, sigs, res, res2)
{
  # upper CI
  up <- sqrt(k*est^2/qchisq(0.025, k))
  up <- head(which(sigs > up), 1)
  # lower
  lo <- sqrt(k*est^2/qchisq(0.975, k))
  lo <- tail(which(sigs < lo), 1)
  
  max_reg <- rep(NA, length(n_ps))
  for(j in 1:length(n_ps)){
    max_reg[j] <- max(res[lo:up,j])
  }
    
  max_reg2 <- rep(NA, length(ns))
  for(j in 1:length(ns)){
    max_reg2[j] <- max(res2[lo:up,j])
  }
  
  plot(res2[,which.min(max_reg2)])
  points(res[,which.min(max_reg)], col="red")
  
  if(min(max_reg) < min(max_reg2)){
    n_ps[which.min(max_reg)]
  } else {
    0
  }
}

minmax_np(1, 100, sigs, res, res2)
```



We can compare against Whitehead et al, who suggest optimal sample sizes for different estimated sds (or equivalently, standardised effect sizes). 




```{r}
MC_vs <- function(n_p)
{
  k <- (n_p-1)/2; theta <- 2*sig^2/(n_p-1)
  to_eval <- rgamma(50, shape = k, scale = theta)
  ns <- sapply(to_eval, function(x) which.min(v(3:1000, pow(3:1000, sqrt(x)), lambda, gamma)) + 2)
  v(ns + 1*n_p, pow(ns, sig), lambda, gamma)
}

n_ps <- NULL; vs <- NULL
for(j in 3:50){
  n_ps <- c(n_ps, j)
  vs <- c(vs, mean(MC_vs(j)))
}

plot(n_ps, vs)
```

We see that the optimal pilot size tends to 0, so the benefits are not justified by the cost of extra sampling. Tbis makes sense based on what we saw earlier - the width of the CI over which we do minimax regret doesnt have much effect on the chosen n; so reducing the width via a larger pilot will make no difference to the n we end up with. But, a variable pilot will gives us chaotic point estimates, and so more variation in the chosen n...



```{r}
# Re-estimation - for a true sd and a pilot sample of k = 6, get distribution of final sample size
v <- (c(0.7, 1, 1.3)*sig)^2
df <- data.frame(vs = rep(v, 10^3))
df$v_est <- df$vs*rchisq(10^3, 11)/11
df$n <- sapply(df$v_est, function(x) optim(10, fn=f, lower=2, upper=1000, sig=sqrt(x), lambda=lambda, method="Brent")$par)
df$p <- apply(df, 1, function(x) pow(x[3], sqrt(x[1])))
df$t <- "v"

df2 <- df[,1:2]
df2$n <- sapply(df2$v_est, function(x) power.t.test(delta=0.3, sd=sqrt(x), power=0.8)$n*2)
df2$p <- apply(df2, 1, function(x) pow(x[3], sqrt(x[1])))
df2$t <- "c"

df2 <- df[,1:2]
df2$n <- sapply(df2$v_est, function(x) power.t.test(delta=0.3, sd=sqrt(x), power=0.8)$n*2)
df2$p <- apply(df2, 1, function(x) pow(x[3], sqrt(x[1])))
df2$t <- "c"

df <- rbind(df, df2)

p1 <- ggplot(df, aes(as.factor(vs), n, fill=t)) + geom_boxplot() +
  theme_minimal() + ylab("Sample size") +
  scale_fill_manual(name="Method", values=cols[1:2], labels=c("Constrained", "Value-based")) +
  scale_x_discrete(name="Standard deviation", 
                   labels = c(expression(0.7*sigma), 
                              expression(sigma),
                              expression(1.3*sigma))) +
  theme(panel.grid.major.x = element_blank())

#ggsave("./paper/figures/n_dist_t_test.pdf", width = 15, height = 10, units="cm")
#ggsave("./paper/figures/n_dist_t_test.eps", width = 15, height = 10, units="cm", device = cairo_ps())
  
p2 <- ggplot(df, aes(as.factor(vs), p, fill=t)) + geom_boxplot() +
  theme_minimal() + ylab("Power") + 
  scale_y_continuous(breaks=seq(0,1,0.2)) +
  scale_fill_manual(name="Method", values=cols[1:2], labels=c("Constrained", "Value-based")) +
  scale_x_discrete(name="Standard deviation", 
                   labels = c(expression(0.7*sigma), 
                              expression(sigma),
                              expression(1.3*sigma))) +
  theme(panel.grid.major.x = element_blank())

p1 + p2

#ggsave("./paper/figures/p_dist_t_test.pdf", width = 15, height = 10, units="cm")
#ggsave("./paper/figures/p_dist_t_test.eps", width = 15, height = 10, units="cm", device = cairo_ps())

#ggpubr::ggarrange(p1, p2, ncol=2, nrow=1, common.legend = TRUE, legend="bottom")

#ggsave("./paper/figures/dists_t_test.pdf", width = 15, height = 10, units="cm")
#ggsave("./paper/figures/dists_t_test.eps", width = 15, height = 10, units="cm", device = cairo_ps())
```

For a number of pilot sample size choices, simulate pilot estimates, use these to get main n, compare value against fixed design based on an initial guess.

```{r}
g <- function(x)
{
  v <- (x[1]*sig)^2
  
  # Get fixed design based on sig
  n_fix <- optim(10, fn=f, lower=2, upper=1000, sig=sig, lambda=lambda, method="Brent")$par
  
  # Sample pilot estimates
  n_p <- x[2]
  v_est <- v*rchisq(10^3, (n_p - 1))/(n_p - 1)
  
  # Get optimal designs based on pilot estimates
  n <- sapply(v_est, function(x) optim(10, fn=f, lower=2, upper=1000, sig=sqrt(x), lambda=lambda, method="Brent")$par)
  
  # Calculate actual power and value
  a <- 1
  p <- pow(n, sqrt(v))
  val <- (p - lambda*(n + n_p))
  #val <- (1 - exp(-a*val))/a
  
  fix_val <- pow(n_fix, sqrt(v)) - lambda*n_fix
  #fix_val <- (1 - exp(-a*fix_val))/a
  
  c(#mean(val > fix_val), 
    mean(val), 
    fix_val)
}

df <- expand.grid(sig_prop = seq(0.5, 2, 0.05),
                  n_p = c(3, 6, 12, 30, 60))
df <- cbind(df, t(apply(df, 1, g)))
names(df)[3:4] <- c("value", "fixed")

ggplot(df, aes(sig_prop, colour=as.factor(n_p))) + geom_line(aes(y = value)) +
  geom_line(aes(y = fixed), linetype = 2)
```

```{r}
g <- function(n, lambda, delta, samp)
{
  # Given a sample of points in the nuisance parameter space and their optimal values,
  # evaluate a fixed design by seeing at how many of these points the fixed value
  # is within a delta tolerance of the optimal value
  sum( (samp[,2] - f(n, sig=samp[,1], lambda)) > -delta )
}

opt$v <- f(opt$n, opt$sig, lambda)

# Get samples, their optimal designs, and values
M <- 2000
samp <- data.frame(sig=runif(M, min(opt[,1]), max(opt[,1])))
samp$v <- apply(samp, 1, function(x) optim(10, fn=f, lower=2, upper=1000, sig=x, lambda=lambda, method="Brent")$value)

# Find the m which best evaluates according to g
delta <- 0.03
rob <- (4:100)[which.max(sapply(4:100, g, lambda=lambda, delta=delta, samp=samp))]
rob

# Get value of a fixed design for comparison
opt3 <- opt[,1:3]; opt3$n <- rob; opt3$t <- "f"
opt3$v<- f(n=rob, sig=opt3[,1], lambda)

d <- -opt$v + opt3$v

ggplot(rbind(opt, opt3), aes(sig)) + geom_line(aes(y=-v, colour=t)) +
  geom_line(data=opt3, aes(y=d), colour="black", linetype=2) +
  theme_minimal() +
  geom_hline(yintercept = delta, linetype=3) +
  scale_color_manual(name="Design", values=cols[3:4], labels=c("Fixed", "Optimal")) +
  ylab("Value") + xlab("Standard deviation")

#ggsave("./paper/figures/fixed_t_test.pdf", width = 15, height = 10, units="cm")
#ggsave("./paper/figures/fixed_t_test.eps", width = 15, height = 10, units="cm", device = cairo_ps())
```

```{r}
# Given a range of nuisnace parameter values, find the design with the minimax discrepancy over that range
h <- function(sig, n, lambda)
{
  v <- -optim(10, fn=f, lower=2, upper=1000, sig=sig, lambda=lambda, method="Brent")$value
  -(v + f(n, sig, lambda))
}

h2 <- function(n, lambda, sig_range, samp)
{
  sub <- samp[samp[,1] > sig_range[1] & samp[,1] < sig_range[2], ]
  vs <- f(n, sub$sig, lambda)
  optim(sub[which.max(-sub$v + vs), 1], fn=h, lower=sig_range[1], upper=sig_range[2], n=n, lambda=lambda, method="L-BFGS-B")$value
}

sig_range <- c(0.75*sig, 1.25*sig)
rob2 <- (4:50)[which.max(sapply(4:50, h2, lambda=lambda, sig_range=sig_range, samp=samp))]
rob2

h2(rob2, lambda=lambda, sig_range=sig_range, samp=samp)

# Get value of a fixed design for comparison
opt4 <- opt[,1:3]; opt4$n <- rob2; opt4$t <- "f"
opt4$v<- f(n=rob2, sig=opt4[,1], lambda)

d <- -opt$v + opt4$v

ggplot(rbind(opt, opt4), aes(sig)) + geom_line(aes(y=-v, colour=t)) +
  geom_line(data=opt4, aes(y=d), colour="black", linetype=2) +
  theme_minimal() +
  scale_color_manual(name="Design", values=cols[3:4], labels=c("Fixed", "Optimal")) +
  ylab("Value") + xlab("Standard deviation") +
  geom_vline(xintercept = sig_range, linetype=3)

#ggsave("./paper/figures/minimax_t_test.pdf", width = 15, height = 10, units="cm")
#ggsave("./paper/figures/minimax_t_test.eps", width = 15, height = 10, units="cm", device = cairo_ps())
```


For a given choice of sample size, there is a trade-off parameter corresponding to every possible MCID. We are willing to pay a higher rate for a larger MCID. For example, if we determine value based on 80\% power to detect MCID of 0.3 we get a robust design of 29, but if instead it is 96\% power to detect 0.4 our robust design is 70. Does this make sense? So, two people have the same optimal design conditional on a np estimate; but very different robust designs. That is, one is more sensitive to changes in the np than the other. 

First, note that this is not specific to our approach. under the constrained approach, the optimal design becomes more sensitive to the np as the threshold is increased. 

A more general framework would be that we want to simultaneously maximise power at all effect sizes greater than the true MCID where we want 50\%. We need to combine this continum of objectives into a single measure, e.g. through a weighted sum. In the continuous case, this is integrating with respect to some weighting function, where the weighting function integrates to 1, i.e. is a probability distribution. So, this is equivalent to maximising expected power with respect to a prior on the effect size. Since trunctaed at the true MCID, this could be interpreted as prior prob of trial success, given there is a true worthwhile effect.

```{r}
pow2 <- function(n, sig, mu)
{
  n <- n/2
  #power.t.test(n = n, delta = 0.3, sd = sig)$power
  1-pt(qt(0.975, 2*(n-1)), 2*(n-1), mu/sqrt(2*sig^2/n))
}

n <- 2*19; sig <- 0.3205898; mu <- 0.4
pow2(n, sig, mu=mu)

lambda <- grad(pow2, n, sig=sig, mu=mu)
```

## Example - cluster RCT, m and k

```{r}
clus_pow <- function(x, var_t, rho)
{
  k <- x[1]; n <- x[2]; m <- n/k
  clus_var <- var_t*rho + (var_t - var_t*rho)/m
  power.t.test(n=k, delta=0.3, sd=sqrt(clus_var))$power
}

# plot power function to find value params
df <- expand.grid(k=seq(10, 40, 5), n=seq(2,700,20))
df$pow <- apply(df, 1, clus_pow, var_t=1, rho=0.05)

ggplot(df, aes(n, pow, group=k, colour=k)) + geom_line() +
  geom_point(data=data.frame(k=15, n=470, pow=clus_pow(c(15, 470), 1, 0.05)), colour="red") +
  theme_minimal()

#ggsave("./paper/figures/cluster_pow.pdf", height=9, width=14, units="cm")

#des <- c(25, 450)
#des <- c(15, 470)
des <- c(25, 270)
des <- c(19, 19*18)
clus_pow(des, 1, 0.05)
lambda <- grad(clus_pow, des, var_t=1, rho=0.05)

f <- function(x, var_t, rho, lambda)
{
  pow <- clus_pow(x, var_t, rho)
  -(pow - sum(lambda*x))
}

opt <- expand.grid(var_t=seq(0.5, 1.5, 0.05), rho=seq(0.025, 0.1, 0.005))
opt <- cbind(opt, t(apply(opt, 1, function(x) optim(c(10,10), fn=f, lower=c(2,2), upper=c(50,1000), var_t=x[1], rho=x[2], lambda=lambda, method="L-BFGS-B")$par)))
names(opt)[3:4] <- c("k", "n")
opt$v <- apply(opt, 1, function(x) f(x[3:4],x[1],x[2], lambda))

ggplot(opt, aes(k,n, colour=rho, size=var_t)) + geom_point()
```

```{r}
g <- function(x, lambda, delta, samp)
{
  # Given a sample of points in the nuisance parameter space and their optimal values,
  # evaluate a fixed design by seeing at how many of these points the fixed value
  # is within a delta tolerance of the optimal value
  sum( (samp[,3] - f(x, var_t=samp[,1], rho=samp[,2], lambda)) > -delta )
}

# Get samples, their optimal designs, and values
M <- 5000
samp <- data.frame(var_t=runif(M, min(opt[,1]), max(opt[,1])), rho=runif(M, min(opt[,2]), max(opt[,2])))
samp$v <- apply(samp, 1, function(x) optim(c(10,10), fn=f, lower=c(2,2), upper=c(50,1000), var_t=x[1], rho=x[2], lambda=lambda, method="L-BFGS-B")$value)

# Find the m which best evaluates according to g
delta <- 0.03
eval <- data.frame(k=rep(4:50, each=29)); eval$n <- 2:30*eval$k
eval <- eval[eval$n < 600,]
rob <- eval[which.max(apply(eval, 1, g, lambda=lambda, delta=delta, samp=samp)),]
rob

g(as.numeric(rob), lambda, delta, samp)

# Get value of a fixed design for comparison
opt3 <- opt[,1:2]; opt3$k <- rob[[1]]; opt3$n <- rob[[2]]; opt3$t <- "f"
opt3$v<- f(x=as.numeric(rob), var_t = opt3[,1], rho = opt3[,2], lambda)

opt$d <- -opt$v + opt3$v

ggplot(opt, aes(var_t, rho, z=d, colour=as.factor(..level..))) + geom_contour(breaks=seq(0.01, 0.05, 0.01)) + #geom_contour(breaks=seq(0.01, 0.05, 0.005)) +
  theme_minimal() +
  ylab("Intracluster correlation") + xlab("Total variance") +
  scale_colour_discrete(name = "Difference in value")

#ggsave("./paper/figures/mult_design.pdf", width = 15, height = 10, units="cm")
#ggsave("./paper/figures/mult_design.eps", width = 15, height = 10, units="cm", device = cairo_ps())
```

## Example - survival

```{r}
# see chapter 9, D.Collet Modelling survival data in medical research
s_bar <- function(x, med1, med2)
{
  # Assuming exponential models with medians med1 and med2,
  # so S(x) = exp(-ln(2)x/med)
  (exp( -(log(2)/med1)*x ) + exp( -(log(2)/med2)*x ))/2
}
  
get_pow <- function(x, med1)
{
  # med1 - median survival control
  # x - (accrual time period, total trial time)
  # a - accrual time period
  # f - follow-up time period
  a <- x[[1]]; f <- max(x[[2]] - x[[1]], 0)
  dif <- 2
  med2 <- med1 + dif
  m <- 10.3 # monthly accrual rate
  theta <- log(med1/med2) # log harzard ratio
  
  # Expected sample size
  n <- m*a
  # Probability of an event
  prob_d <- 1 - (s_bar(f, med1, med2) + 4*s_bar(0.5*a + f, med1, med2) + s_bar(a + f, med1, med2))/6
  # Expected number of events
  d <- n*prob_d
  # Power
  pnorm( -theta*sqrt(d)/2 - qnorm(0.975) )
}

get_pow(c(23,48), 4.5)
lambda <- grad(get_pow, c(23,48), med1=4.5)
```

Finding the optimal design (i.e. that which maximises the value function) for any given value of the nuisance parameter:

```{r}
get_value <- function(x, med1, lambda)
{
  #if(x[1] < 1 | x[1] > 100 | x[2] < 1 | x[2] > 100) return(100000)
  pow <- get_pow(x, med1)
  
  -(pow - sum(lambda*x))
}

get_design <- function(med1)
{
  o <- optim(c(25,50), get_value, med1=med1, lambda=lambda,
              lower = c(0,0), upper = c(100,100), method="L-BFGS-B")
  return(c(o$par, -o$value))
} 

df <- data.frame(med1=seq(2, 8, 0.05))
df <- cbind(df, t(sapply(df$med1, get_design)))
names(df)[2:4] <- c("a", "t", "v")

df2 <- data.frame(med1=df$med1)
fixed <- c(21, 47)
df2$a <- fixed[1]; df2$t <- fixed[2]
df2$v <- -sapply(df2$med1, function(x) get_value(fixed, x, lambda))
df2$d <- df2$v - df$v

# Optimal designs
ggplot(df, aes(a, t-a, colour=med1)) + geom_point() +
  xlab("Accrual") + ylab("Follow-up") +
  theme_minimal()

# Optimal and fixed design values
ggplot(df2, aes(med1)) + geom_line(aes(y=v)) + geom_line(aes(y=-d), colour="blue") +
  geom_line(data=df, aes(y=v), colour="red") + 
  xlab("Median survival time") + ylab("Value") +
  theme_minimal()
```

The initial choice of design appears to generally be quite robust, and a visual inspection of the above plot might be enough to confirm its choice. But more generally, what metrics could we use to find the best fixed design? One obvious solution is to use a distribution on the nuisance parameter to give an expected value and maximise that, but we want to avoid any half Bayesian methods and stick with the frequentist paradigm here. Two possible approaches:

- Define a range of interest on the nuisance parameter and then choose the minimax design, i.e. that with the smallest maximum difference in value between fixed and optimal designs;
- Define a maximum tollerated difference in value between fixed and optimal designs, and choose that which gives the largest area of nuisance parameter space within this margin.

The first will be highly sensitive to the choice of range which we want to avoid, so let's examine the second approach.

```{r, eval=F}
coverage <- function(fixed, df, diff, lambda)
{
  df2 <- data.frame(med1=df$med1)
  df2$a <- fixed[1]; df2$t <- fixed[2]
  df2$v <- -sapply(df2$med1, function(x) get_value(fixed, x, lambda))
  df2$d <- df2$v - df$v
  -sum(-df2$d <= diff)
}

df <- data.frame(med1=seq(2, 8, 0.01))
df <- cbind(df, t(sapply(df$med1, get_design)))
names(df)[2:4] <- c("a", "t", "v")

diff <- 0.01
opt <- psoptim(c(35,50), coverage, df=df, diff=diff, lambda=lambda,
              lower = c(0,0), upper = c(100,100))#, method="L-BFGS-B")
```

## Stepped wedge

From Hooper et al. (2016).

```{r}
def_c <- function(m, rho)
{
  # Design effect due to clustering
  1 + (m - 1)*rho
}

def_r <- function(r, l, t, b, c, d)
{
  # Design effect due to repeated assessment 
  l^2 * (1 - r)*(1 + t*r)/(4*(l*b - d + (b^2 + l*t*b - t*d - l*c)*r))
}

m <- 36 # cluster size
rho <- 0.33; pi <- 0.9; sig <- 5; tau <- 0.7 # nuisance parameter estimates
l <- 3; t <- 3; k <- 4 # l=t= number of arms (ie rows) in stepped wedge design; k = number of clusters per arm
a <- matrix(c(0,0,0, 1,0,0, 1,1,0, 1,1,1), nrow = 3) # stepped wedge matrix
b <- sum(a); c <- sum(colSums(a)^2); d <- sum(rowSums(a)^2)
```

```{r}
# Repeated cross section
r_cc_rcs <- function(m, rho, pi)
{
  # Correlation in a repeated cross-section cluster design
  (m*rho*pi)/(1 + (m - 1)*rho)
}

pow_rcs <- function(mm, k, rho, pi, l, t, b, c, d, sig)
{
  # Effective individually randomised sample size
  eff_n <- l*k*mm/(def_r(r_cc_rcs(mm, rho, pi), l, t, b, c, d)*def_c(mm, rho))
  power.t.test(n = eff_n/2, delta = 2, sd = sig)$power
}

r_cc_rcs(m, rho, pi)

def_r(r_cc_rcs(m, rho, pi), l, t, b, c, d)

pow_rcs(m, k, rho, pi, l, t, b, c, d, sig)

lambda <- grad(pow_rcs, m, method="Richardson", k=k, rho=rho, pi=pi, l=l, t=t, b=b, c=c, d=d, sig=sig)

ms <- 2:45; ps <- pow_rcs(ms, k, rho, pi, l, t, b, c, d, sig)

f <- function(m, k, rho, pi, l, t, b, c, d, sig, lambda)
{
  # Value function
  p <- pow_rcs(m, k, rho, pi, l, t, b, c, d, sig)
  -(p - lambda*m)
}

# Find optimal m for various nuisance parameter values
opt <- expand.grid(rho=seq(0,0.7,0.01), pi=seq(0.7, 1, 0.01), sig=c(4,5,6,7))
opt <- cbind(opt, apply(opt, 1, function(x) optim(10, fn=f, lower=2, upper=1000, 
                                                    k=k, rho=x[1], pi=x[2], l=l, t=t, b=b, c=c, d=d, sig=x[3], lambda=lambda,
                                                    method="L-BFGS-B")$par))
names(opt)[4] <- c("m")
opt$v <- f(opt[,4], k, opt[,1], opt[,2], l, t, b, c, d, opt[,3], lambda)

ggplot(opt, aes(rho, pi, z=m, colour=..level..)) + geom_contour() +
  facet_wrap(.~sig)

g <- function(m, k, l, t, b, c, d, lambda, vs, delta, samp)
{
  # Given a sample of points in the nuisance parameter space and their optimal values,
  # evaluate a fixed design by seeing at how many of these points the fixed value
  # is within a delta tolerance of the optimal value
  sum( (vs - f(m, k, samp[,1], samp[,2], l, t, b, c, d, samp[,3], lambda)) > -delta )
}

# Get samples, their optimal designs, and values
M <- 2000
samp <- data.frame(rho=runif(M, min(opt[,1]), max(opt[,1])), 
                   pi=runif(M, min(opt[,2]), max(opt[,2])),
                   sig=runif(M, min(opt[,3]), max(opt[,3])))
samp$v <- apply(samp, 1, function(x) optim(10, fn=f, lower=2, upper=1000, 
                                                    k=k, rho=x[1], pi=x[2], l=l, t=t, b=b, c=c, d=d, sig=x[3], lambda=lambda,
                                                    method="L-BFGS-B")$value)

# Find the m which best evaluates according to g
rob <- (2:50)[which.max(sapply(2:50, g, k=k, l=l, t=t, b=b, c=c, d=d, lambda=lambda, vs=samp[,4], 
                        delta=0.05, samp=samp))]

# Get value of a fixed design for comparison
opt$v_fix <- f(m=rob, k, opt[,1], opt[,2], l, t, b, c, d, opt[,3], lambda)
ggplot(opt, aes(rho, pi, z=v - v_fix, colour=..level..)) + geom_contour(breaks=-c(0.01, 0.025, 0.05, 0.1)) +
  facet_wrap(.~sig)
```



```{r}
# Closed cohort
r_cc_clch <- function(m, rho, pi, tau)
{
  # Correlation in a closed cohort
  (m*rho*pi + (1 - rho)*tau)/(1 + (m - 1)*rho)
}

pow_clch <- function(mm, k, rho, pi, tau, l, t, b, c, d, sig)
{
  # Effective individually randomised sample size
  eff_n <- l*k*mm/(def_r(r_cc_clch(mm, rho, pi, tau), l, t, b, c, d)*def_c(mm, rho))
  power.t.test(n = eff_n/2, delta = 2, sd = sig)$power
}

m <- 10

r_cc_clch(m, rho, pi, tau)

def_r(r_cc_clch(m, rho, pi, tau), l, t, b, c, d)

pow_clch(m, k, rho, pi, tau, l, t, b, c, d, sig)

lambda <- grad(pow_clch, m, method="Richardson", k=k, rho=rho, pi=pi, tau=tau, l=l, t=t, b=b, c=c, d=d, sig=sig)

ms <- 2:45; ps <- pow_clch(ms, k, rho, pi, tau, l, t, b, c, d, sig)

f2 <- function(m, k, rho, pi, tau, l, t, b, c, d, sig, lambda)
{
  # Value function
  p <- pow_clch(m, k, rho, pi, tau, l, t, b, c, d, sig)
  -(p - lambda*m)
}

# Find optimal m for various nuisance parameter values
opt <- expand.grid(rho=seq(0,0.7,0.01), pi=seq(0.7, 1, 0.01), sig=c(4,5,6), tau=c(0.5, 0.7, 0.9))
opt <- cbind(opt, apply(opt, 1, function(x) optim(10, fn=f2, lower=2, upper=1000, 
                                                    k=k, rho=x[1], pi=x[2], tau=x[4], l=l, t=t, b=b, c=c, d=d, sig=x[3], lambda=lambda,
                                                    method="L-BFGS-B")$par))
names(opt)[5] <- c("m")
opt$v <- f2(opt[,5], k, opt[,1], opt[,2], opt[,4], l, t, b, c, d, opt[,3], lambda)

# Plot optimal m for different parameter values
ggplot(opt, aes(rho, pi, z=m, colour=..level..)) + geom_contour() +
  facet_wrap(sig ~ tau) +
  theme_minimal()
```

```{r}
g2 <- function(m, k, l, t, b, c, d, lambda, vs, delta, samp)
{
  # Given a sample of points in the nuisance parameter space and their optimal values,
  # evaluate a fixed design by seeing at how many of these points the fixed value
  # is within a delta tolerance of the optimal value
  sum( (vs - f2(m, k, samp[,1], samp[,2], samp[,4], l, t, b, c, d, samp[,3], lambda)) > -delta )
}

# Get samples, their optimal designs, and values
M <- 5000
samp <- data.frame(rho=runif(M, min(opt[,1]), max(opt[,1])), 
                   pi=runif(M, min(opt[,2]), max(opt[,2])),
                   sig=runif(M, min(opt[,3]), max(opt[,3])),
                   tau=runif(M, min(opt[,4]), max(opt[,4])))
samp$v <- apply(samp, 1, function(x) optim(10, fn=f2, lower=2, upper=1000, 
                                                    k=k, rho=x[1], pi=x[2], tau=x[4], l=l, t=t, b=b, c=c, d=d, sig=x[3], lambda=lambda,
                                                    method="L-BFGS-B")$value)

# Find the m which best evaluates according to g
rob2 <- (2:50)[which.max(sapply(2:50, g2, k=k, l=l, t=t, b=b, c=c, d=d, lambda=lambda, vs=samp[,5], 
                        delta=0.03, samp=samp))]
rob2

# Get value of a fixed design for comparison
opt$v_fix <- f2(m=rob2, k, opt[,1], opt[,2], opt[,4], l, t, b, c, d, opt[,3], lambda)

ggplot(opt, aes(rho, pi, z=-(v - v_fix), colour=as.factor(..level..))) + geom_contour(breaks= seq(0.01, 0.05, 0.01)) +
  facet_wrap(sig ~ tau) + 
  theme_minimal() +
  theme(legend.position="bottom") +
  scale_colour_discrete(name = "Difference in value") +
  xlab("Intracluster correlation") + ylab("Cluster autocorrelation")
  
#ggsave("./paper/figures/fixed_sw.pdf", width = 15, height = 15, units="cm")
#ggsave("./paper/figures/fixed_sw.eps", width = 15, height = 15, units="cm", device = cairo_ps())
```

```{r}
# Closed cohort - fixed m
r_cc_clch <- function(m, rho, pi, tau)
{
  # Correlation in a closed cohort
  (m*rho*pi + (1 - rho)*tau)/(1 + (m - 1)*rho)
}

pow_clch <- function(k, mm, rho, pi, tau, l, t, b, c, d, sig)
{
  # Effective individually randomised sample size
  eff_n <- l*k*mm/(def_r(r_cc_clch(mm, rho, pi, tau), l, t, b, c, d)*def_c(mm, rho))
  power.t.test(n = eff_n/2, delta = 2, sd = sig)$power
}

m <- 10
k <- 4

r_cc_clch(m, rho, pi, tau)

def_r(r_cc_clch(m, rho, pi, tau), l, t, b, c, d)

pow_clch(k, m, rho, pi, tau, l, t, b, c, d, sig)

lambda <- grad(pow_clch, k, method="Richardson", mm=m, rho=rho, pi=pi, tau=tau, l=l, t=t, b=b, c=c, d=d, sig=sig)

ms <- 2:45; ps <- pow_clch(ms, k, rho, pi, tau, l, t, b, c, d, sig)
ks <- 2:6; ps <- pow_clch(ks, m, rho, pi, tau, l, t, b, c, d, sig)

f2 <- function(k, m, rho, pi, tau, l, t, b, c, d, sig, lambda)
{
  # Value function
  p <- pow_clch(k, m, rho, pi, tau, l, t, b, c, d, sig)
  -(p - lambda*k)
}

# Find optimal m for various nuisance parameter values
opt <- expand.grid(rho=seq(0,0.7,0.01), pi=seq(0.7, 1, 0.01), sig=c(4,5,6), tau=c(0.5, 0.7, 0.9))
opt <- cbind(opt, apply(opt, 1, function(x) optim(5, fn=f2, lower=2, upper=1000, 
                                                    m=m, rho=x[1], pi=x[2], tau=x[4], l=l, t=t, b=b, c=c, d=d, sig=x[3], lambda=lambda,
                                                    method="L-BFGS-B")$par))
names(opt)[5] <- c("k")
opt$v <- f2(opt[,5], m, opt[,1], opt[,2], opt[,4], l, t, b, c, d, opt[,3], lambda)

# Plot optimal m for different parameter values
ggplot(opt, aes(rho, pi, z=k, colour=as.factor(..level..))) + geom_contour(breaks = 2:6) +
  facet_wrap(sig ~ tau) +
  theme_minimal() +
  theme(legend.position="bottom") +
  scale_colour_discrete(name = "Number of clusters") +
  xlab("Intracluster correlation") + ylab("Cluster autocorrelation")

#ggsave("./paper/figures/opt_sw_k.pdf", width = 15, height = 15, units="cm")
#ggsave("./paper/figures/opt_sw_k.eps", width = 15, height = 15, units="cm", device = cairo_ps())
```

```{r}
g2 <- function(k, m, l, t, b, c, d, lambda, vs, delta, samp)
{
  # Given a sample of points in the nuisance parameter space and their optimal values,
  # evaluate a fixed design by seeing at how many of these points the fixed value
  # is within a delta tolerance of the optimal value
  sum( (vs - f2(k, m, samp[,1], samp[,2], samp[,4], l, t, b, c, d, samp[,3], lambda)) > -delta )
}

# Get samples, their optimal designs, and values
M <- 5000
samp <- data.frame(rho=runif(M, min(opt[,1]), max(opt[,1])), 
                   pi=runif(M, min(opt[,2]), max(opt[,2])),
                   sig=runif(M, min(opt[,3]), max(opt[,3])),
                   tau=runif(M, min(opt[,4]), max(opt[,4])))
samp$v <- apply(samp, 1, function(x) optim(5, fn=f2, lower=2, upper=1000, 
                                                    m=m, rho=x[1], pi=x[2], tau=x[4], l=l, t=t, b=b, c=c, d=d, sig=x[3], lambda=lambda,
                                                    method="L-BFGS-B")$value)

# Find the m which best evaluates according to g
rob2 <- (2:10)[which.max(sapply(2:10, g2, m=m, l=l, t=t, b=b, c=c, d=d, lambda=lambda, vs=samp[,5], 
                        delta=0.03, samp=samp))]
rob2

# Get value of a fixed design for comparison
opt$v_fix <- f2(k=rob2, m, opt[,1], opt[,2], opt[,4], l, t, b, c, d, opt[,3], lambda)

ggplot(opt, aes(rho, pi, z=-(v - v_fix), colour=as.factor(..level..))) + geom_contour(breaks= seq(0.01, 0.05, 0.01)) +
  facet_wrap(sig ~ tau) + 
  theme_minimal() +
  theme(legend.position="bottom") +
  scale_colour_discrete(name = "Difference in value") +
  xlab("Intracluster correlation") + ylab("Cluster autocorrelation")
  
#ggsave("./paper/figures/fixed_sw_k.pdf", width = 15, height = 15, units="cm")
#ggsave("./paper/figures/fixed_sw_k.eps", width = 15, height = 15, units="cm", device = cairo_ps())
```

## Discussion

Is power the right metric for value? Could consider precision, e.g. expected width of a confidence interval, instead.  

```{r}
ns <- 2:80
pow <- sapply(ns, function(x) power.t.test(n=x, delta=1)$power)
se <- sqrt(2*1/ns)

plot(ns, se)
plot(se, pow)
```

Looking at precision (in this case the standard error of a mean difference) doesn't work, because precision will keep increasing with n - there isn't a natural plataue. As a result, minimising a wieghted sum of n and of se will lead to steady increases in sample size as we increase the variance. 

So really, power can be thought of as a value function on precision/information, as it encapsulates the fact that after a point, extra precision is not useful for us because we will already have enough to reliably make our decisions. It gives us a better idea of how much precision we need for the question at hand.

Does this hold up? Increasing precision can let us decrease the portion of parameter space wehere decisions will be random. 

```{r}
n <- 100; d <- 0.3; sig <- 1
power.t.test(n=n, delta=d, sd=sig)$power

pnorm(d/sqrt(2*sig/n) - qnorm(0.975))

lambda <- 0.0025

f <- function(n, lambda)
{
  -(dnorm(d*sqrt(n)/sqrt(2*sig^2) - qnorm(0.975))*d/sqrt(8*n*sig^2) - lambda)
}

g <- function(n, lambda)
{
  -(pnorm(d*sqrt(n)/sqrt(2*sig^2) - qnorm(0.975)) - lambda*n)
}

f <- function(x)
{
  v <- max(pnorm(d*sqrt(1:5000)/sqrt(2*x[1]^2) - qnorm(0.975)) - x[2]*(1:5000))
  
  cn <- ceiling((qnorm(0.8) + qnorm(0.975))^2*x[1]^2*2/d^2)
  c <- max(pnorm(d*sqrt(cn)/sqrt(2*x[1]^2) - qnorm(0.975)) - x[2]*(cn))
  
  v - c
}

df <- expand.grid(sig = seq(0.01, 1, l=100),
                  lambda = seq(0.0001, 0.1, l=100))
df$o <- apply(df, 1, f)

ggplot(df, aes(sig, lambda, z=o)) + stat_contour()
```

## Application - cluster randomised trials

Consider the problem of choosing the sample size of a cluater randomised trial comparing two groups on a continuous endpoint. The size of each cluster will vary and is not under the control of the experiment, but we know the expected cluster size. We expect outcomes within the same cluster to be correlated. Within and between cluster variances are expeceted to be different in each arms, and there is considerable uncertainty about their values. For the purposes of power calculations, we assume that the trial will be analysed by comparing the mean cluster outcomes between the two arms. We restrict attention to the case of equal numbers of clusters in each arm, noting that our uncertainty in the variance components means any unequal randomisation would not be justified (at least not on the grounds of statistical efficiency).

# Comparisons

The alternative approach to dealing with uncertain nuisance parameters within a frequentist framework is SSR, so let's do a comparison of the two approaches. We will use the same example in a paper on SSR for cluster trials, where uncertainty is in the variance compoentns and in the variability of cluster size as in our application above.

```{r, eval=F}
library(numDeriv)

get_power <- function(k, sd)
{
  if(k <= 1){
    return(0)
  } else {
    d <- 1
    # Use the large-sample normal approximation
    p <- 1-pnorm(qnorm(0.975)*sqrt((2*sd^2)/k), d, sqrt((2*sd^2)/k))
    return(p)
  }
}

# Guess of the sd of cluster means is ~ 1.9
get_power(57, 1.9)
# Infer the value function parameter based on our original choice
lambda <- grad(get_power, 57, sd=1.9)

get_value <- function(k, sd)
{
  pow <- get_power(k, sd)
  return(-(pow - lambda*k))
}

get_k <- function(sd)
{
  # Find the k such that value is maximised
  sd <- sd[[1]]
  o <- optim(1, get_value, lower=c(0), upper=c(100), sd=sd, method="Brent")
  return(c(o$par, o$value))
}

get_k_SSR <- function(sd)
{
  # For comparison, show the k needed to get 80% power
  k <- 2*(qnorm(1-0.025) + qnorm(0.8))^2 * (sd^2/1)
  return(c(k, get_value(k, sd)))
}


df <- data.frame(sd=seq(0.1,4,0.05))
df <- cbind(df, t(sapply(df$sd, get_k)))
names(df)[2:3] <- c("k", "v")
df <- cbind(df, t(sapply(df$sd, get_k_SSR)))
names(df)[4:5] <- c("k_SSR", "v_SSR")
df$pow <- sapply(1:nrow(df), function(i, df) get_power(df[i,2], df[i,1]), df=df)

ggplot(df, aes(sd, k)) + geom_line() +
  geom_vline(xintercept = 1.9, linetype=2) +
  geom_line(data=df, aes(sd,k_SSR), colour="darkred")
```

We see that the usual SS method agrees with our method when the initial guess of the nuisance parameter is correct. Now, take the largest sample size required by our method as our robust choice, and examine the 

```{r, eval=F}
# Robust sample size
k_rob <- max(df$k)
df_rob <- data.frame(sd = df$sd)
df_rob$pow <- sapply(df_rob, get_power, k=k_rob)


# Simulate SSR
sim_SSR <- function(sd)
{
  k <- 29; m <- 4
  c_means <- rnorm(2*k, 0, sd)
  sd_est <- sd(c_means)
  new_k <- max(57, get_k_SSR(sd_est)[1])
  return(new_k)
}

batch_SSR <- function(sd)
{
  ks <- replicate(1000, sim_SSR(sd))
  pows <- sapply(ks, get_power, sd=sd)
  return(c(as.numeric(quantile(ks, c(0.01,0.1,0.5,0.9,0.99))), as.numeric(quantile(pows, c(0.01,0.1,0.5,0.9,0.99)))))
}

df_SSR <- data.frame(sd = df$sd)
df_SSR <- cbind(df_SSR, t(sapply(df_SSR$sd, batch_SSR)))
names(df_SSR)[2:6] <- c("kq01", "kq10", "kq50", "kq90", "kq99")
names(df_SSR)[7:11] <- c("pq01", "pq10", "pq50", "pq90", "pq99")

# Plot sample sizes
ggplot(df_SSR, aes(sd, kq50)) + geom_line(colour="darkred") +
  geom_ribbon(aes(ymin = kq10, ymax = kq90), alpha=0.2, fill="darkred") + 
  geom_ribbon(aes(ymin = kq01, ymax = kq99), alpha=0.2, fill="darkred") +
  geom_hline(yintercept = k_rob, colour="darkgreen") +
  ylim(c(0,400))

# Plot powers
ggplot(df_SSR, aes(sd, pq50)) + geom_line(colour="darkred") +
  geom_ribbon(aes(ymin = pq10, ymax = pq90), alpha=0.2, fill="darkred") + 
  geom_ribbon(aes(ymin = pq01, ymax = pq99), alpha=0.2, fill="darkred") +
  geom_line(data=df_rob, aes(sd, pow), colour="darkgreen") +
  ylim(c(0,1))
```

What is the problem with SSR? The key point is that it entails a strict following of the constrained approach to trial design, in contrast to the initial design where flexibility is implictly allowed, generally done, but always hidden. A better SSR apprach would be to re-estimate the parameter and then do the SS calculation as we do in practice, including allowing us to change the effect size we want to detect to avoid admitting we have low power. 

# Controlling type I error

In the bove we have fixed $\alpha$ at some standard choice. Consider varying it as a design parameter.

```{r, eval=F}
get_power <- function(x, sd)
{
  k <- x[1]; alpha <- x[2]; d <- 0.3
  if(k <= 1){
    return(0)
  } else {
    # Use the large-sample normal approximation
    p <- 1-pnorm(qnorm(1-alpha)*sqrt((2*sd^2)/k), d, sqrt((2*sd^2)/k))
    return(p)
  }
}

get_power(c(235, 0.025), 1)
lambda <- grad(get_power, c(235, 0.025), sd=1)

get_value <- function(x, sd)
{
  k <- x[1]; alpha <- x[2]; d <- 1
  if(k < 2 | k > 800 | alpha <=0 | alpha >=1) return(10000)
  pow <- get_power(c(k, alpha), sd=sd)
  return(-(pow - lambda[1]*k - lambda[2]*alpha))
}

get_design <- function(sd, starting=c(95.5,0.025))
{
  o <- optim(starting, get_value, sd=sd)
  return(c(o$par, o$value))
}


sds <- seq(0.1,3,0.02)
df <- NULL
df <- rbind(df, c(sds[1], get_design(sds[1])))
for(i in 2:length(sds)){
  df <- rbind(df, c(sds[i], get_design(sds[i], starting = df[i-1,2:3])))
}
df <- as.data.frame(df)
names(df) <- c("sd", "k", "a", "v")
df$p <- apply(df[,1:3], 1, function(x) get_power(x[2:3], sd=x[1]))
df$c <- apply(df, 1, function(x) sqrt(2*x[1]^2/x[2])*qnorm(1-x[3]))

ggplot(df, aes(k, a, colour=sd)) + geom_point() 

# Get value of fixed design
rob <- data.frame(sd=sds, k=235, a=0.025)
rob$v <- apply(rob, 1, function(x) get_value(x[2:3], x[1]))

df2 <- rbind(rob, df[,c("sd", "k", "a", "v")])

ggplot(df2, aes(sd, v, colour=k)) + geom_point()

# Get vakue of optimal design with constrained alpha

get_value2 <- function(x, sd)
{
  k <- x[1]; d <- 1; alpha <- 0.025
  pow <- get_power(c(k, alpha), sd=sd)
  return(-(pow - lambda[1]*k - lambda[2]*alpha))
}

get_design2 <- function(sd, starting=c(95.5))
{
  o <- optim(starting, get_value2, sd=sd, method="Brent", lower = 2, upper = 800)
  return(c(o$par, o$value))
}

df3 <- data.frame(sd=df$sd)
df3 <- cbind(df3, t(sapply(df3$sd, get_design2)))
names(df3)[2:3] <- c("k", "v")
df3$a <- 0.025

df2 <- rbind(df2, df3[,c(1,2,4,3)])
df2$t <- c(rep("rob", 146), rep("opt", 146), rep("opt2", 146))

ggplot(df2, aes(sd, v, colour=k, shape=t)) + geom_point()
```

An increase in $\sigma$ leads to a higher optimal type I error rate, accopanied by a lower inflation of the sample size compared to the previous case where $\alpha$ was fixed. If we want to ptotect the design against only a $\sigma$ larger than our best guess, we could argue that the conservative approach is to keep $\alpha$ at the low initial value and then conditional on this, choose the maximum optimal sample size (as we did previously). 

Key point is that for a larger $\sigma$ the optimal design will increase the type I error, but we can instead choose to keep it fixed and increase the sample size instead. Bear in mind that the addtive value function assumption and the method of eliciting the parameters may not be applicable for type I error rates - wehereas we can argue that we get round the power "constraiint" by adjusting the MCID, it appears that the 0.025 one sided $\alpha$ is usued pretty much all the time.

## Application - survival 

For our cluster randomised trial we had a single design variable, the sample size. Here we consider a problem with two design variables, the sample size and the follow-up time for a trial comparing a time-to-event outcome. We start as before, with a power function and a value function whose parameters are deterimined from the choice of local design. 

```{r, eval=F}
# see chapter 9, D.Collet Modelling survival data in medical research
s_bar <- function(x, med1, med2)
{
  # Assuming exponential models with medians med1 and med2,
  # so S(x) = exp(-ln(2)x/med)
  (exp( -(log(2)/med1)*x ) + exp( -(log(2)/med2)*x ))/2
}
  
get_beta <- function(x, med1)
{
  # med1 - median survival control
  # x - (accrual time period, total trial time)
  # a - accrual time period
  # f - follow-up time period
  a <- x[[1]]; f <- x[[2]] - x[[1]]
  dif <- 2
  med2 <- med1 + dif
  m <- 10.3 # monthly accrual rate
  theta <- log(med1/med2) # log harzard ratio
  
  # Expected sample size
  n <- m*a
  # Probability of an event
  prob_d <- 1 - (s_bar(f, med1, med2) + 4*s_bar(0.5*a + f, med1, med2) + s_bar(a + f, med1, med2))/6
  # Expected number of events
  d <- n*prob_d
  # Power
  pow <- pnorm( -theta*sqrt(d)/2 - qnorm(0.975) )
  return(1-pow)
}

get_beta(c(23,48), 4.5)
w <- grad(get_beta, c(23,48), med1=4.5)
```

Finding the optimal design (i.e. that which maximises the value function) for any given value of the nuisance parameter:

```{r, eval=F}

get_value <- function(x, med1)
{
  if(x[1] < 1 | x[1] > 100 | x[2] < 1 | x[2] > 100) return(100000)
  beta <- get_beta(x, med1)
  return(beta - sum(w*x))
}

get_design <- function(med1)
{
  o <- optim(c(5,5), get_value, med1=med1)
  return(c(o$par, o$value))
}

df <- data.frame(med1=seq(2, 10, 0.05))
df <- cbind(df, t(sapply(df$med1, get_design)))
names(df)[2:4] <- c("a", "t", "v")

ggplot(df, aes(a, t, colour=med1)) + geom_point() 
```


Now, take the total study time $t$ to be fixed so we are down to a single design variable, the accrual time $a$. Taking the same approach as above, what value of $a$ is robust to uncertainty in the nuisance parameter (the median time in the control group)?

```{r, eval=F}
# Fixed t, total time for study (i.e. accrual time + follow-up time)
obj_func_fixed_t <- function(a, t, med1)
{
  x <- c(a, t)
  beta <- get_beta(x, med1=med1)
  return(beta - sum(w*x))
}

eval_p_fixed_t <- function(med1, t)
{
  # Find accrual time a that maximises value for known med1
  opt <- optim(par=3, fn=obj_func_fixed_t, t=t, med1=med1,
               lower = 2, upper = t, method="Brent")
  return(-opt$par)
}

opt_fixed_t <- function(t)
{
  # Search over med1 to find the largest a possibly required
  opt <- optim(par=5, fn=eval_p_fixed_t, t=t, lower = 1, upper = 100, method="L")
  return(-opt$value)
}

# For example, 
opt_fixed_t(36)
```

Similarly, we can do the same but taking the accrual time as fixed and looking for the maximum total study time we would want if the nuisance parameter were known.

```{r, eval=F}
# Fixed a, accrual time
obj_func_fixed_a <- function(t, a, med1)
{
  x <- c(a, t)
  beta <- get_beta(x, med1=med1)
  return(beta - sum(w*x))
}

eval_p_fixed_a <- function(med1, a)
{
  opt <- optim(par=10, fn=obj_func_fixed_a, a=a, med1=med1,
               lower = 3, upper = 100, method="Brent")
  return(-opt$par)
}

opt_fixed_a <- function(a)
{
  opt <- optim(par=5, fn=eval_p_fixed_a, a=a, lower = 1, upper = 100, method="L")
  return(-opt$value)
}

# For example,
opt_fixed_a(24)
```


So, for any value of $a$ we can find the value of $t$ that is robust to the nuisance parameter, and vice versa:

```{r, eval=F}
ts <- 3:75
t_o <- sapply(ts, opt_fixed_t)

as <- 3:75
a_o <- sapply(as, opt_fixed_a)

df <- data.frame(t=ts, t_o=t_o, a=as, a_o=a_o)

ggplot() + geom_point(data=df, aes(t, t_o, colour="t_fix")) + geom_point(data=df, aes(a_o, a, colour="a_fix"))
```

We see that the functions cross at an equilibrium point $(a^*, t^*)$, which can be found using the following algorithm:

```{r, eval=F}
t1 <- runif(1,3,100); a1 <- runif(1,3,100)
converged <- FALSE
iter <- 1
print(c(a1,t1))
while(!converged & iter < 100){
  t2 <- opt_fixed_a(a1)
  a2 <- opt_fixed_t(t2)
  print(c(a2, t2))
  if(dist(rbind(c(t1,a1),c(t2,a2))) < 0.00001) converged <- TRUE
  t1 <- t2; a1 <- a2
}
```

For a given design, is there a value of $med1$ where another design, not smaller in either aspect, gives a better value?
```{r, eval=F}

f <- function(y, med1, x)
{
  if(sum(y < x) > 0 | y[1] >= y[2]) return(10000)
  get_value(y, med1)
}

g <- function(med1, x)
{
  if(med1 < 1 | med1 > 20) return(10000)
  opt <- optim(par=x, fn = f, x=x, med1=med1, lower=x, method="L-BFGS-B")
  return(-sum(opt$par - x))
}

h <- function(x)
{
  meds <- runif(100, 1, 20)
  start <- meds[which.min(sapply(meds, g, x=x))]
  #opt <- optim(par=start, fn=g, x=x, lower=1, upper=1+2*start, method="Brent")
  opt <- nlm(g, start, x=x)
  opt$minimum
}

df <- expand.grid(a=seq(25,35,1), t=seq(55, 85, 1))
df <- df[df$t > df$a,]
df$v <- apply(df, 1, h)

ggplot(df, aes(a,t,colour=v<0)) + geom_point()
```

## Pilot trials

Connecting with our work on testing feasibility in pilots, where we assume a fixed main trial sample siZe and define null and alternative hypothees in terms of the power that will be obtained. Will be essentially the same when we are focussing on nuisance parameters like a common variance rather than feasibility parameters. The method as given above gives us a fixed main trial sample size, which will be conservative - it will never be optimal to increase it, no matter what the true value of the parameter is. But is some cases the optimal sample size will be zero, so we would have run a futile trial. Can we use the testing idea to reduce the chance of this happeneing? How will it compare with the alternative SSR approach?

Copied from above:
```{r, eval=F}
lambda <- 0.005
df <- expand.grid(n=2:500, sig=seq(1,5,0.1))
df$pow <- apply(df, 1, function(x) power.t.test(n=x[1], delta=1, sd=x[2])$power)
df$v <- df$pow - lambda*df$n

f <- function(n, sig)
{
  pow <- power.t.test(n=n, delta=1, sd=sig)$power
  -(pow - lambda*(n + 32))
}

opt <- data.frame(sig=seq(1,5,0.1), n=1)
opt$n <- sapply(opt$sig, function(x) optim(10, f, method="Brent", lower=2, upper=500, sig=x)$par)
opt$pow <- apply(opt, 1, function(x) power.t.test(n=x[2], delta=1, sd=x[1])$power)
opt$value <- opt$pow - 0.005*(opt$n + 32)

ggplot(df, aes(n, pow, colour=sig, group=as.factor(sig))) + geom_line() +
  geom_point(data=opt, colour="red") +
  geom_vline(xintercept = max(opt$n), linetype=2, colour="red")

opt[which.max(opt$n),]
df0 <- opt
```


So we have a fixed main trial sample size of 88. What does power look like for our range of variances?
```{r, eval=F}
df2 <- data.frame(sig=seq(1,4,0.1))
df2$pow <- sapply(df2$sig, function(x) power.t.test(n=88, delta=1, sd=x)$power)
ggplot(df2, aes(sig, pow)) + geom_point()
```
At some point the power obtained won't justify the expense. Say that 60\% is our threshold. If the true sd is around 2.98, we will get 60\% power and will be indifferent about running vs not running the trial. Given the value function, this translates into a set-up cost of 32 patients or 0.005*32 = 0.16 value units (we have included this in the above code). We can see the value function crossing 0 at this point:

```{r, eval=F}
ggplot(df0, aes(sig, n)) + geom_line() + 
  geom_line(data=df0, aes(sig, 100*value), colour="darkgreen") +
  geom_vline(xintercept = 2.98, linetype =2)
```

So, if the true $\sigma$ is > 2.98 our robust trial will actually be worse than no trial at all. If we want to test feasibility in a pilot, we could ask that we have 50\% power for a $\sigma = 2.98$ so that our indfiference at this point is reflected. Then we need to decide how large the pilot should be. We can take the same approach as for the main trial, by extending the value function, finding the optimal pilot sample size for a range of $\sigma$, and then being conservative by choosing the largest of these. To extend the value function we need to now think of the overall power of the two-trial system, and now consider the expected sample size (where the expectation is over the pilot data / test result):

```{r, eval=F}
get_value <- function(n_p, sig)
{
  alpha <- 0.5; n <- 88
  c <- qchisq(alpha, n_p-1)*2.98^2/(n_p-1)
  pilot_pow <- pchisq(c*(n_p-1)/sig^2, n_p-1)
  main_pow <- power.t.test(n=n, delta=1, sd=sig)$power
  # Return both the pilot value, and the value of not doing a pilot at all
  c(-(pilot_pow*main_pow - lambda*(n_p/2+pilot_pow*(n+32))), main_pow - lambda*(n+32))
}

df3 <- data.frame(sig=seq(1,6,0.1))
df3$n <- sapply(df3$sig, function(x) optim(30, function(y, x) get_value(y, x)[1], x=x,
                                           lower = 2, upper = 200, method="Brent")$par)
ggplot(df3, aes(sig, n)) + geom_line()
```
In this example we see that optimal pilot size drops to 0 as we get to the point of equivalence. There are two peaks, one on either side of this point. As we move to the extremes, the optimal size reduces, as we would expect - it becomes easier to make the correct decision with only a few data. The maximum here is $n_p \approx 12$. What value would we obtain if we use that?

```{r, eval=F}
df4 <- expand.grid(sig=seq(1,6,0.1))
df4 <- cbind(df4, t(sapply(df4$sig, function(x, n_p) get_value(n_p, x), n_p = max(df3$n))))
names(df4)[2:3] <- c("v", "v2")

ggplot(df4, aes(sig, -v)) + geom_line() +
  geom_line(data=df4, aes(sig, -(v+v2)), linetype=3) +
  geom_vline(xintercept = 2.98, linetype =2)
```
The differnece in value between our "robust" pilot and not running one at all (dotted line) is slightly negative for $\sigma < 2.98$ and positive thereafter, with the difference becoming very large as $\sigma$ increases past this threshold.

We have in the above fixed the main $n$ and optimised over $n_p$. What happens if we do the reverse?

```{r, eval=F}
get_value2 <- function(n, sig)
{
  alpha <- 0.5; n_p <- 12
  c <- qchisq(alpha, n_p-1)*2.98^2/(n_p-1)
  pilot_pow <- pchisq(c*(n_p-1)/sig^2, n_p-1)
  main_pow <- power.t.test(n=n, delta=1, sd=sig)$power
  # Return both the pilot value, and the value of not doing a pilot at all
  c(-(pilot_pow*main_pow - lambda*(n_p/2+pilot_pow*(n+32))), -(main_pow - lambda*(n+32)))
}

df5 <- data.frame(sig=seq(1,6,0.1))
df5$n <- sapply(df5$sig, function(x) optim(30, function(y, x) get_value2(y, x)[1], x=x,
                                           lower = 2, upper = 200, method="Brent")$par)
ggplot(df5, aes(sig, n)) + geom_line()
```

We see that the optimal $n$ is the same regardless of the pilot sample size, including when it s $n_p = 0$. This makes sense when we see that the overall value function takes the form

$$
\begin{aligned}
v_p(n, n_p) &= x(n_p)f(n) - \lambda \big[y(n_p) + xg(n) \big] \\
&= x(n_p) \big[f(n) - \lambda g(n) \big] - \lambda y(n_p) \\ 
&= x(n_p) v(n) - \lambda y(n_p),
\end{aligned}
$$

so for fixed $n_p$ the extedned value is a linear transformation of the mian trial value.

What does change is the value function. As the pilot size increases the break even point where $v = 0$ reduces - for example when $n_p = 12$ it is at $\sigma \approx 2.81$, not 2.98 as before when $n_p = 0$.

# cRCT + SW

```{r}
clus_pow2 <- function(x, var_w, rho)
{
  n <- x; m <- 18; k <- n/m
  var_b <- rho*var_w/(1-rho)
  clus_var <- var_b + var_w/m
  power.t.test(n=k, delta=0.3, sd=sqrt(clus_var))$power
}

# Plot the motivating example, with two nuisance parameters power curves
df <- data.frame(n=seq(2,700,20))
df$k <- df$n/18
df <- rbind(df, df)
df$rho <- c(rep(0.02, nrow(df)/2), rep(0.07, nrow(df)/2))
df$p <- apply(df, 1, function(x) clus_pow(x[2:1], var_w=1, rho=x[3]))

grad(clus_pow2, 262, var_w=1, rho=0.02)

ggplot(df, aes(n, p, colour=as.factor(rho))) + geom_line() +
  geom_hline(yintercept = 0.8, linetype=2) +
  ylab("Power") + xlab("Sample size") +
  scale_y_continuous(breaks=seq(0,1,0.2)) +
  theme(panel.grid.major = element_line(colour = "grey50")) +
  theme(panel.grid.minor = element_line(colour = "grey80")) +
  scale_colour_manual(values=cols) +
  labs(colour = "ICC") +
  theme_minimal() +
  geom_point(data=data.frame(n=c(262,442), p=c(0.8,0.8), rho=c(0.02, 0.07)), shape=16, size=2) +
  geom_abline(intercept = 0.8 - 0.001592332*262, slope = 0.001592332, linetype=3) + 
  geom_abline(intercept = 0.16, slope = 0.001592332, linetype=3) +
  geom_point(data=data.frame(n=280, p=0.61, rho=0.07), shape=3, size=2, stroke=2)

ggsave("./presentations/cRCT+SW 2019/motivate.png", height=9, width=15, units="cm", bg = "transparent")
```


```{r}
clus_pow <- function(x, var_w, rho)
{
  k <- x[1]; n <- x[2]; m <- n/k
  var_b <- rho*var_w/(1-rho)
  clus_var <- var_b + var_w/m
  power.t.test(n=k, delta=0.3, sd=sqrt(clus_var))$power
}

# plot power function to find value params
df <- expand.grid(k=seq(10, 40, 5), n=seq(2,700,20))
df$pow <- apply(df, 1, clus_pow, var_w=1, rho=0.05)

ggplot(df, aes(n, pow, group=k, colour=as.factor(k))) + geom_line() +
  geom_point(data=data.frame(k=15, n=470, pow=clus_pow(c(15, 470), 1, 0.05)), colour="red") +
  theme_minimal()

#ggsave("./paper/figures/cluster_pow.pdf", height=9, width=14, units="cm")

des <- c(25, 500) # 90
des <- c(20, 360) # 80
clus_pow(des, 1, 0.05)
lambda <- grad(clus_pow, des, var_w=1, rho=0.05)

# Plot the optimal designs as we vary rho

f <- function(x, var_w, rho)
{
  pow <- clus_pow(x, var_w, rho)
  -(pow - sum(lambda*x))
}

opt <- expand.grid(var_t=1, rho=seq(0,0.25,0.001))
opt <- cbind(opt, t(apply(opt, 1, function(x) optim(c(10,10), fn=f, lower=c(2,2), upper=c(70,1000), var_w=x[1], rho=x[2], method="L-BFGS-B")$par)))
names(opt)[3:4] <- c("k", "n")

ggplot(opt, aes(rho)) + geom_line(aes(y=k), colour=cols[3]) + geom_line(aes(y=n/k), colour=cols[4]) +
  theme_minimal() + xlab("Intracluster correlation coefficient (ICC)") + ylab("")

ggsave("./presentations/cRCT+SW 2019/opt_ssr.png", height=9, width=11, units="cm", bg = "transparent")
```

```{r}
opt$v <- apply(opt, 1, function(x) f(x[3:4],x[1],x[2]))

rob <- opt
rob$k <- des[1]
rob$n <- des[2]
opt$v2 <- apply(rob, 1, function(x) f(x[3:4],x[1],x[2]))
opt$d <- opt$v - opt$v2

# get optimal conventional design, assuming m = 20

f2 <- function(x, var_w, rho)
{
  pow <- clus_pow(x, var_w, rho)
  sum(lambda*x) + 10000*(0.8-pow)^2
}

opt <- cbind(opt, t(apply(opt, 1, function(x) optim(c(10,100), fn=f2, lower=c(2,2), upper=c(100,1000), var_w=x[1], rho=x[2], method="L-BFGS-B")$par)))
names(opt)[8:9] <- c("k2", "n2")

ggplot(opt, aes(rho)) + 
  geom_line(aes(y=-v), colour=cols[1]) +
  geom_line(aes(y=-v2), colour=cols[2]) +
  geom_line(aes(y=-d), colour=cols[8], linetype=2) +
  theme_minimal() + xlab("Intracluster correlation coefficient (ICC)") + 
  ylab("Value")

ggsave("./presentations/cRCT+SW 2019/value.png", height=9, width=11, units="cm", bg = "transparent")
```

```{r}
# Get true power of each of these designs when rho = 0.05
opt$p <- apply(opt, 1, function(x) clus_pow(x[3:4], 1, 0.05))
opt$p2 <- apply(opt, 1, function(x) clus_pow(x[8:9], 1, 0.05))

var_c <- 0.1026316; var_b <- var_c - 1/18
vb_est <- var_b*rchisq(10^3, 11)/11 
rho_est <- vb_est/(vb_est + 1) 
rho_ind <- round(rho_est, 3)*1000 
k_est <- opt[rho_ind,"k"]; n_est <- opt[rho_ind,"n"]; m_est <- n_est/k_est 
k2_est <- opt[rho_ind,"k2"]; n2_est <- opt[rho_ind,"n2"]
p_est <- opt[rho_ind,"p"]; p2_est <- opt[rho_ind,"p2"]

hi <- data.frame(rho=rho_est, k=k_est, n=n_est, k2=k2_est, n2=n2_est, p=p_est, p2=p2_est)

ggplot(hi, aes(rho)) + geom_density(fill=cols[1], alpha=0.2) +
  theme_minimal() + xlab("Estimated ICC sampling distribution") +
  theme(panel.background = element_blank()) +
  theme(
    plot.background = element_blank()
   ,panel.grid.major = element_blank()
   ,panel.grid.minor = element_blank()
   ,panel.border = element_blank()
  )

ggsave("./presentations/cRCT+SW 2019/ICC.png", height=5, width=8, units="cm", bg = "transparent")

hi2 <- data.frame(k=c(k_est, k2_est), n=c(n_est, n2_est), p=c(p_est, p2_est), t=c(rep("Value-based", length(vb_est)), rep("Constrained", length(vb_est))))

ggplot(hi2, aes(t, n, fill=t)) + geom_boxplot() +
  scale_x_discrete(breaks = NULL) +
  theme_minimal() + xlab("") + ylab("Sample size") +
  scale_fill_manual(values=cols[5:6], name= "Method") + theme(legend.position="bottom")

ggsave("./presentations/cRCT+SW 2019/SS_dist.png", height=8, width=8, units="cm", bg = "transparent")

ggplot(hi2, aes(t, p, fill=t)) + geom_boxplot() +
  scale_x_discrete(breaks = NULL) +
  theme_minimal() + xlab("") + ylab("Power") +
  scale_fill_manual(values=cols[5:6], name= "Method") + theme(legend.position="bottom")

ggsave("./presentations/cRCT+SW 2019/pow_dist.png", height=8, width=8, units="cm", bg = "transparent")
```

```{r}
s <- 1; n <- 200; mu <- 0.3
c <- qnorm(0.975)*sqrt(2/n)

z <- rnorm(100, mu, sqrt(2/200))

mean(z > c)
mean(z[z > c])

pow <- function(n)
{
  1 - pnorm(qnorm(0.975)*sqrt(2*s/n), 0.3, sqrt(2*s/n))
}

bias <- function(n)
{
  # Bias of the effect estimate when conditioning on a signifcant result (z > c)
  (dnorm(qnorm(0.975) - 0.3/sqrt(2*s/n)) / (1 - pnorm(qnorm(0.975) - 0.3/sqrt(2*s/n))))*sqrt(2*s/n)
}

ns <- 100:300
plot(ns, bias(ns)/mu)
plot(pow(ns), bias(ns))

s <- 1; n <- 200; mu <- 0.3
c <- qnorm(0.975)*sqrt(2/n)

ps1 <- ps2 <- NULL
for(i in 1:10000){
  z <- rnorm(100, mu, sqrt(2/n))
  ps1 <- c(ps1, 1-pnorm(c, mu, sd(z)))
  ps2 <- c(ps2, mean(z > c))
}

```





EVSI

Rather than optimising over splines, suppose we optimise over the individual samples. Doing this directly doesn't work, as we end up with an over-fitted and not continuous function. 