\documentclass[sagev, Crown]{sagej}

%\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{array}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{url}

\setcounter{secnumdepth}{3} %Gives section numbers for cross referencing

\begin{document}

\runninghead{Wilson et al.}

\title{Robust, value-based sample size determination for clinical trials when nuisance parameters are unknown}

\author{Duncan T. Wilson\affilnum{1}}%,

\affiliation{\affilnum{1}Leeds Institute of Clinical Trials Research, University of Leeds, Leeds, UK} 

\corrauth{Duncan T. Wilson, Clinical Trials Research Unit, Leeds Institute of Clinical Trials Research, University of Leeds, Leeds, LS2 9JT, UK}
\email{d.t.wilson@leeds.ac.uk}

\begin{abstract}
% 200 word limit (250 for YSM)

\textbf{Background:} The conventional approach to determining the sample size of a clinical trial is to choose the smallest value such that the power of the trial is above a nominal threshold. This sample size can be highly sensitive to nuisance parameters, such as the variance of a continuous primary outcome. Moreover, this approach does not formally account for the costs of sampling, and as a result can lead to incoherent decisions.

\textbf{Methods:} We present an alternative model for sample size determination which explicitly balances costs and benefits by introducing a value function to be maximised. We explore the implications of the model and argue it provides a better representation of sample size determination in practice than the conventional approach. We show the method is significantly less sensitive to nuisance parameters, to the point where a fixed design with no interim sample size adjustment can be near-optimal for large regions of the nuisance parameter space. We propose a criterion for choosing an optimal fixed sample size, considering the range of nuisance parameter values for which the value of the fixed design is within a tolerable distance of the value of the best possible design.

\textbf{Results:} We illustrate our approach by applying it to two trial design problems: choosing the accrual and follow-up times for a parallel group trial comparing overall survival, where the median survival time in the control arm is unknown; and choosing the number of clusters in a cluster randomised trial with unknown variance components at both the individual and cluster levels.

\textbf{Conclusion:} Accounting for the costs of sampling when determining the sample size of a clinical trial,  we can find simple, fixed sample size designs which are highly robust to nuisance parameter uncertainty.
\end{abstract}

\keywords{Clinical trials, sample size, power, interim analysis, internal pilot, sample size re-estimation}

\maketitle

Outline:

Intro. Scope - deciding on the main trial sample size based on available point and interval estimates. Costs and benefits. The incoherence of the constrained approach.

Methods - constrained and value-based for known $\sigma$; and for unknown $\sigma$. Power and precision as value function components. Link the two - contrained is minimising the worst case regret for a lexicographic step value function at a power threshold (or have this in the discussion)

Illustration (or include as part of methods) - apply to a simple two-sample t-test with unknown variance, for a few example value functions, for a single point and interval estimate scenario.

Evaluation - consider a range of point and interval estimates and show how the sample size (and other characteristics?) varies.

Illustration - apply to a binary endpoint cRCT to show how to deal with different setups; keep one value function and one set of point and interval estimates throughout.

Extension - incorporating uncertainty in effect size by using an expected effectiveness (or just power again?) metric in the value function.

Discussion - key points are: "under/overpowered" trials and ethical arguments; the sample size samba; extending naturally to Bayesian DT formulation; how other methods can be represented in our framework (esp. upper CI method); teeing up extension to SSR / pilot trial design; extension to other aspects of trial design like recruitment and follow-up rates.


\section{Introduction}\label{sec:intro}

The standard approach to sample size determination (SSD) in Randomised Clinical Trials (RCTs) is to choose the smallest sample size which will give a power of at least a pre-specified threshold value (often 80 or 90\%). Power is defined in relation to a target effect size (specifically, the minimal clinically important difference \cite{Cook2018}), and under an assumed model of the data generating process. This model will typically assume a value for one or more nuisance parameters (such as the standard deviation of a continuous primary outcome), and so these must be estimated prior to the trial. Incorrect nuisance parameter estimates will lead to the actual power of the trial differing from the planned power. The greater the error in estimation, the greater the discrepancy between planned and actual power.

The conventional approach to SSD does not formally allow for the costs of sampling to be taken into account, which leads to undesirable behaviour. Here, we investigate alternative approaches to SSD which explicitly account for costs, incorporating these into a so-called \emph{value function} which formalises the preferences of the decision maker and leads to defensible sample size decisions.

The remainder of this paper is organised as follows. In Section \ref{sec:methods} we define the conventional method for SSD and introduce the value-based alternative. The performance of these methods is compared in Section \ref{sec:evaluation}, before we illustrate the value-based approach by applying it to an example SSD problem in Section \ref{sec:illustration}. In Section \ref{sec:extensions} we extend to method to allow for SSD based on uncertain estimates of the effect size in addition to nuisance parameters, before concluding with a discussion in Section \ref{sec:discussion}.

\section{Methods}\label{sec:methods}

Although the methods described here are quite general and can be applied in a range of scenarios (as will be shown in Section \ref{sec:illustration}), we will focus on SSD for a two-arm parallel group RCT with a continuous primary outcome assessing the mean difference between groups, denoted by $\mu$. The primary analysis will test the null hypothesis $H_0: \mu = \mu_0$ against an alternative hypotheses $H_1: \mu = \mu_1$ with a (two-sided) type I error rate of $\alpha = 0.05$. We denote the (common between arms) standard deviation by $\sigma$, its estimate by $\hat{\sigma}$, and the sample size by $n$.

\subsection{Constrained design}

The conventional approach to sample size determination sets a constraint on the trial's power, $\beta^*$, and finds the smallest $n$ which satisfies that constraint. Formally, we find
\begin{equation}
n^*_c (\mu_1, \sigma, \beta^*) = {\arg\min}_{n \in \mathbb{N}}~ n ~\text{ subject to } ~ \beta(n, \mu_1, \sigma) \leq \beta^*.
\end{equation}
The chosen sample size $n^*_c$ depends on the target difference $\mu_1$, the nuisance parameter $\sigma$, and the nominal type II error rate $\beta^*$. Although originally intended to be set at a level appropriate to the specific context \cite{Neyman1933}, a convention of arbitrarily setting $\beta^* = 0.2$ or 0.1 has emerged \cite{Bacchetti2019}. When $n^*_c$ is considered to be infeasibly large, it is common to inflate the target difference $\mu_1$ from the true MCID to a higher value and thus reduce the required sample size to a feasible value - the so-called ``sample soze samba'' \cite{Schulz2005}.

When $\mu_1$ and $\beta^*$ are fixed, the sample size is a function of the nuisance parameter only. In practice, $\sigma$ will never be known and an estimate, $\hat{\sigma}$, must be used in its place. When there is some uncertainty in this estimate, it has been recommended that it be inflated to guard against choosing a sample size which is too low (in the sense of leading to a power less than the nominal level). For example, when the estimate $\hat{\sigma}$ has an associated confidence interval, the upper end of that interval may be used instead \cite{Browne1995, Teare2014}. This approach is justified as being `conservative' in the sense that it will likely lead to a trial with power greater than the nominal rate, since $n^*_c$ is strictly increasing in $\hat{\sigma}$. 

\subsection{Value-based design}

An alternative approach to SSD is to describe our preferences with a \emph{value} function, in the sense that one scenario is preferred to another if and only if that value function of that scenario is larger than the other. For our problem, a scenario is characterised by two attributes: the sample size of the trial, $n$, and the nuisance parameter, $\sigma$. Thus, we would like to define a function $v(n, \sigma): \mathbb{N} \times \mathbb{R}^+ \rightarrow \mathbb{R}$ such that
$$
(n, \sigma) \text{ is preferred to }(n', \sigma') \Leftrightarrow v(n, \sigma) > v(n', \sigma').
$$
That such a function exists follows from some decision-theoretic axioms \cite{French2000}. We will consider the following two specific value function:
$$
\begin{aligned}
v_1(n, \sigma) & = \underbrace{1 - \Phi\left(z_{1 - \alpha} - \frac{\delta_a}{\sqrt{2\sigma^2/n}}\right)}_\textrm{Power} - \lambda_1 n - c_1 \\
v_2(n, \sigma) & = \underbrace{\sqrt{\frac{n}{2\sigma^2}}}_\textrm{Precision} - \lambda_2 n - c_2.
\end{aligned}
$$
These functions share a similar structure, each comprising a term denoting the benefit of the trial (its power n $v_1$, and its precision in $v_2$) which is offset in a linear manner by the cost of sampling and by a fixed set-up cost ($\lambda_i n$ and $c_i$ respectively, for $i = 1, 2$). Thus, each value function implies a constant trade-off between cost and benefit, but the way benefit is quantified differs.

Some value functions are illustrated in Figure \ref{fig:p1}. The left-hand column shows the contour plots of function $v_1$ for $lambda_1 = $ (top) and $\lambda_2 = $ (bottom) in the sample size by power space. Superimposed over the contours are two power functions, corresponding to standard deviations of $\sigma = 1$ and 1.3. The optimal value-maximising sample size, assuming $\sigma$ is known, is highlighted in each case. The contour which corresponds to a value of 0 is also highlighted. Note that in the bottom left plot the power function for $\sigma = 1.3$ lies completely below this contour, and so the optimal sample size is 0 in this case. Similar plots for the function $v_2$, now in the sample size by precision place, are given for $lambda_2 = $ (top and bottom, respectively).

\begin{figure}
\centering
\includegraphics[scale=0.7]{./figures/p1}
\caption{Power curves for a two-sample $t$-test, for two values of the standard deviation of the outcome. The dotted lines indicate the rate at which power and sample size can be traded off with each other. The points where they are tangential to the power curves are the optimal value-based sample sizes.}
\label{fig:p1}
\end{figure}

\subsection{Eliciting value function parameters}

Conditional on the assumed structure of the value function, the parameters $\lambda_1$ and $c_2$ (respectively $\lambda_2, c_2$) can be determined by plotting a hypothetical power (precision) function over $n$ and selecting (i) the most preferred sample size, and (ii) the lowest sample size such that the trial would be worth conducting. The gradient of the power (precision) function at the former point gives $\lambda_i$. The intercept of the straight line drawn through the second point, with gradient $\lambda_i$, gives $c_i$.

\subsection{Uncertain nuisance parameters}

If $\sigma$ is known then the sample size can be chosen so as to maximise the value function. When $\sigma$ is unknown we instead propose to choose the sample size based on \emph{regret}, we difference in value between the proposed $n$ and the optimal $n$ at a some $\sigma$:
$$
r(n, \sigma) = v(n^*_v(\sigma)) - v(n, \sigma).
$$ 
Specifically, we choose the sample size which minimises the worst-case regret over a given interval $\hat{\Sigma} = [\hat{\sigma}^-, \hat{\sigma}^+]$. Formally, 
$$
n^*_{mm} = {\arg\min}_{n \in \mathbb{N}} \left[ \max_{\sigma \in \hat{\Sigma}} r(n, \sigma) \right].
$$

For example, consider the case where we have a point estimate of $\hat{\sigma} = 1$ and an interval estimate of $\hat{\Sigma} = [0.80, 1.34]$ . Figure \ref{fig:p2} plots the value of the optimal sample size over $\sigma$ for each of the four scenarios described in Table \ref{tab:}. This is contrasted with the value function of the minimax regret sample size $n_{mm}^*$. 

\begin{figure}
\centering
\includegraphics[scale=0.7]{./figures/p2}
\caption{Caption.}
\label{fig:p2}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.7]{./figures/p4}
\caption{Caption.}
\label{fig:p4}
\end{figure}

\section{Evaluation}\label{sec:evaluation}

In the running example presented throughout Section \ref{sec:methods}, we found that the value-based approach significantly outperforms the constrained method for a specific point and interval estimate.

\begin{figure}
\centering
\includegraphics[scale=0.7]{./figures/p3}
\caption{Caption.}
\label{fig:p3}
\end{figure}

\section{Application to the examples}\label{sec:illustration}

\subsection{Cluster randomised trial}

To apply the value-based approach to the cluster RCT example of \ref{sec:ex_cRCT}, we note that the fixed cluster size was $m = 18$ and the best estimates of total variance and the ICC were 1 and 0.05, respectively. These give an initial estimate of the nuisance parameter of $\hat{\sigma} = 0.32$. The MCID is $\mu_1 = 0.3$. We suppose that the trade-off parameter is $\lambda = 0.01089517$. As illustrated in Figure \ref{fig:p_curves}, for  $\sigma = 0.32$ the value-based and constrained (using $\beta^* = 0.2$) methods agree, giving $ k = 37.87$ clusters.

Optimal sample sizes for both methods over a range of $\sigma$ values are illustrated in Figure \ref{fig:opt_t_test}. We see that the value-based sample size increases with $\sigma_c$, reaches a maximum, and then rapidly decreases. So, there is a point where the imprecision of measurement becomes too great to justify sampling. This can be contrasted with the constrained approach, where the sample size increases indefinitely with $\sigma$.

\begin{figure}
\centering
\includegraphics[scale=0.8]{./figures/opt_t_test}
\caption{Optimal sample size as a function of the standard deviation according to the constrained and value-based methods.}
\label{fig:opt_t_test}
\end{figure}

We examined using both methods in a re-estimation context. For three different values of the nuisance parameter, we simulated pilot estimates based on a pilot sample size of $k_p = 6$ clusters in total. Specifically, we sampled $10^3$ variates $\hat{\sigma}_j^{2 (i)} \sim \frac{\sigma^2}{k_p-1} \chi^2_{k_p-1}$ for both arms $j=1,2$ and $i = 1, 2, \ldots , 10^3$. The pooled sample standard deviation was then calculated as 
$$
\hat{\sigma}^{(i)} = \sqrt{\frac{(k_p - 1)\hat{\sigma}_1^{2 (i)} + (k_p - 1)\hat{\sigma}_2^{2 (i)}}{2k_p - 2}}.
$$
For each $\hat{\sigma}^{(i)}$ we re-estimated the definitive trial sample size using both the constrained and value-based methods. The distributions of the resulting sample sizes and corresponding powers are illustrated in Figure \ref{fig:dists_t_test}. We find considerably less variability in sample size when using the value-based method, particularly when the nuisance parameter has been under-estimated. The constrained method leads to essentially identical power distributions for all true nuisance parameter values, with an average power slightly below the nominal 80\% as a result of the small sample bias in the corrected sample standard deviations.

% Note that in this case (normal enpoint, estimating an sd) we can use the sample size formula given in \cite{Julious2006}, which gives a larger n based on the pilot estimate than the usual formula and as a result an expected power of the nominal level. These adjusted ns are a linear transformation of the standard ns, giving a larger variance in n. Should we use this, or just bring it up in the discussion? 

\begin{figure}
\centering
\includegraphics[scale=0.8]{./figures/dists_t_test}
\caption{Distributions of definitive trial sample size and power when re-estimating using the constrained and value based method, for true nuisance parameters based on $\sigma_c = 0.32$.  Boxplots show the median, first and third quartiles (hinges), and 1.5 $\times$ the inter- quartile range (whiskers).}
\label{fig:dists_t_test}
\end{figure}


To determine an optimal fixed design using the criteria of Equation (\ref{eqn:fix}), we set $\Delta = 0.03$. This gave an optimal fixed design with a total sample size of $n = 29$. The value of this fixed design as a function of $\sigma_c$ is plotted in Figure \ref{fig:fixed_t_test}, along with the value of the locally optimal design and the difference in value between the two. This shows that the fixed design is within $\Delta = 0.03$ of the locally optimal design for $\sigma_c \in [0.2, 0.54]$ (recall the initial estimate was $\sigma_c = 0.32$). If it is believed that this range will contain the true value, using this fixed design will not lead to much value being sacrificed and could lead to a more manageable, predictable, and feasible trial.

\begin{figure}
\centering
\includegraphics[scale=0.8]{./figures/fixed_t_test}
\caption{.}
\label{fig:fixed_t_test}
\end{figure}

\subsection{Closed-cohort stepped wedge trial}

Using the initial nuisance parameter estimates and the suggested optimal design of $k = 4$, we arrive at a trade-off parameter of $\lambda = 0.02502398$.

\begin{figure}
\centering
\includegraphics[scale=0.8]{./figures/opt_sw_k}
\caption{Optimal number of stepped wedge clusters for a range of true nuisance parameter values. Plots vary horizontally with the individual autocorrelation $\tau$, and vertically with the total variance $\sigma^2$.}
\label{fig:fixed_sw}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.8]{./figures/fixed_sw_k}
\caption{Difference in value between fixed and locally optimal stepped wedge designs. Plots vary horizontally with the individual autocorrelation $\tau$, and vertically with the total variance $\sigma^2$.}
\label{fig:fixed_sw}
\end{figure}

\section{Evaluation}

In example 1 we showed that the re-estimated designs are quite different when using constrained or value based methods, but did not comment on their value - assuming that the value method will be better in this respect. In both examples we showed that a fixed design can be quite robust, i.e. that there would be little benefit to be gained from even a perfect re-estimation routine. 

We could extend our analyses by considering the admissibility of these decision rules. Admissable means that, for a given loss function, there is no other rule which is at least as good across the whole parameter space, and better in at least one part, in terms of its risk
$$
E_{x | \theta}[L(\theta, \delta(x))].
$$
Now, every admissible rule is a Bayes rule w.r.t. some prior $p(\theta)$, i.e. a rule which minimises the Bayes risk
$$
E_{\theta}[ E_{x | \theta}[L(\theta, \delta(x))] ] = E_{\theta, x}[L(\theta, \delta(x))].
$$
But every Bayes rule is also a generalised Bayes rule, i.e. a rule which will always minimise the posterior loss upon seeing $x$:
$$
E_{\theta | x} [L(\theta, \delta(x))].
$$
Now, we can see that any fixed design will be a generalised Bayes rule by choosing a degenerate prior which gives probability 1 to the point where that fixed design is optimal. So, any fixed design is admissible. 

Can we say something similar about the value rule? That is, is there a prior s.t. using that rule would be equivalent to minimising the posterior loss?

We may not be able to show that the value rule is admissible, but we can show that it dominates the constrained rule for the linear value function we have considered. The question is then, does there exist a value/loss function s.t. the constrained approach is admissible? 

\section{Extensions}\label{sec:extensions}

\subsection{Re-estimation in internal pilots}

All our arguments and methods hold in the internal pilot setting, where we re-estimate sample size after getting an interim nuisance parameter estimate. The complication is that type I error rate can be inflated, but many papers have identified how to correct for this in a variety of settings, including the t-test setting of our first example. Note that re-estimation is not possible in the second example.

The key difference between internal and external pilots is cost. For internal pilots, the sampling is "free" but there will be a penalty in terms of inflated alpha. So, might be easiest to cover both in the same general framework, where the internal pilot sampling cost term is calculated as the required inflation.

\subsection{Multiple design parameters}

For clarity, we have focussed on problems with a single design parameter, the sample size $n$. The value-based method extends naturally to a vector of design parameters. For example, a cluster RCT may allow for a choice of both the number of clusters $k$, the cluster size $m$, and the nominal type I error rate $\alpha^*$. If value is considered linear in all design parameters, and the power of the trial is convex, the trade-off parameters can be determined as in the single parameter case by choosing a design based on a hypothetical $\sigma$ and calculating the gradient of the power function, in terms of each design parameter dimension, at these points.

For example, returning to the cluster RCT example in Section \ref{sec:ex_cRCT}, we now consider choosing $k$ and $m$. Trade-off parameters are determined based on choosing the same design as before for the estimated nuisance parameter $\sigma_c = 0.32$ (that is, a locally optimal design of $k =19, m=18, n=342$ per arm). Note that we now need both the total variance and the ICC to calculate power, and so have two nuisance parameters: $\sigma_T^2$ and $\rho$. Searching for a robust fixed design as in \ref{} leads to a choice of $k = 16, m = 18, n = 288$ per arm. This was based on a nuisance parameter space where $\sigma_t^2 \in [0.1, 1.5]$ and $\rho \in [0.025, 0.1]$. The difference in value between the fixed and locally optimal designs over this space is illustrated in Figure \ref{} Over this space, the fixed design is within $\Delta = 0.03$ value units of the locally optimal design in approximately 99\% of cases. The only region of the subspace where this difference was exceeded was for small values of both $\sigma_t^2$ and $\rho$.

\begin{figure}
\centering
\includegraphics[scale=0.8]{./figures/mult_design}
\caption{Difference in value between fixed and locally optimal cluster randomised designs where both number of clusters and cluster size can be varied.}
\label{fig:mult_design}
\end{figure}

Extending to different sample size components would be simple enough in terms of the value function, using the same arguments of linearity and value additivity. Computationally, it might not be much worse - key thing is the number of discrete designs to evaluate. In the t-test example we have 1000. Can always argue for a coarser scale.

Key extension should be to cRCTs, either with control over m and k and two nuisnace params, over random m and control over only k, with four nuisance params. Note the latter will be fast, as only a few possible k options. But in both cases, SSR via integration over the sampling dist will be tricky and probably need MC. But it would be useful to show exactly how much more cumbersome the method becomes. Do both versions, include one in the paper and the other as SM. Also, start this extension noting that the t-test example covers cRCTs with fixed cluster size. 

Including alpha could be trickier, in terms of the value function. But this can go in discussion about extending to uncertainty in effect and the BAyesain conclusion, see below.

\subsection{Minimax optimality criterion}

An alternative optimality criteria for a fixed design is
$$
\min_{n \in \mathbb{N}} ~ \left[ \max_{\sigma \in \Sigma} ~  \delta(\sigma | n) \right].
$$
That is, we could choose the sample size with the smallest worst-case difference in value compared with the locally optimal design, over some range $\Sigma$. This criteria will be more sensitive to the choice of $\Sigma$; but it provides a way to better incorporate prior belief regarding likely values. As an example, using this criteria in the cluster RCT example of Section \ref{sec:ex_cRCT} and with $\Sigma = [0.22, 0.42]$ leads to a slightly larger fixed design of $n = 32$. The value of this design is plotted in Figure \ref{fig:minimax_t_test}.

\begin{figure}
\centering
\includegraphics[scale=0.8]{./figures/minimax_t_test}
\caption{.}
\label{fig:minimax_t_test}
\end{figure}

This isa actually what we end up using, minimax regret. So put the other criteria here?

\subsection{Multi-criteria / efficacy}

We have focussed on trading off sampling with power at a specific point. We could argue that we are interested in power across the full (plausable) parameter space. If we had a discrete set of power points, we would start by combining into a weighted sum with weights reflecting the value at each point. The continuous analouge would be integrating the power function with some weighting function. This function would have to integrate to 1, so would be mathematically equivalent to a probability distribution. So this view would end up equivalent to a Bayesian analysis, where we maximise expected utility. But the latter would be easier in practice, as it separates out belief and value when eliciting the weighting function.

This encompasses the point of including alpha, so roll these points together.

\cite{Lin2004} - a phase II design which sets two alternative hypotheses, with different power constraints on each.

\cite{Kim2017} - extends to three hypotheses

\subsection{Simulation-based power}

Potentially another paper - for complex designs where we need MC for power calcs, could propose an alternative scalarising method as opposed to Pareto approach of WP4.1. Could use Laura's or Isabelle's problems as examples and collaborate? With a one-dimension value function to maximise over a several-dimension design space, we have the standard stochastic EGO approach (although note DICEoptim no longer on CRAN). Could extend to minimax regret SSD, which might be a bit more interesting / novel. This addresses the point in WP4.1. discussion that everything was conditional on estimates, but these might not be known, and we could model over the parameter space too. 

We would then want to choose, at each iteration, the design and point in the parameter space at which to simulate. We need to anticipate the improvement that evaluation will lead to. 

\section{Discussion}\label{sec:discussion}

The threshold myth, and how it would justify the constrained model but is a myth.

Incoherence in the standard SSD/R procedure resulting from the thereshold myth and ignorning costs.

Sample size samba in SSD, and note how this is impossible when doing SSR.

Ethics of underpowered and overpowered studies.

\cite{Schulz2005} - Sample size calculations in randomised trials: mandatory and mystical

\cite{Halpern2002} - The continuing unethical conduct of underpowered clinical trials

\cite{Lilford2002} - The Ethics of Underpowered Clinical Trials

\cite{Bacchetti2005a} - Ethics and sample size

\cite{Bacchetti2005} - Bacchetti et al. Respond to ``Ethics and sample size - Another view''

\cite{Norman2012} - Sample size calculations: should the emperor's clothes be off the peg or made to measure?

\cite{Edwards1997} - Why "underpowered" trials are not necessarily unethical

\cite{Girling2007} - Sample-size calculations for trials that inform individual treatment decisions: a 'true-choice' approach

\cite{Claxton1999} - The irrelevance of inference: a decision-making approach to the stochastic evaluation of health care technologies

\cite{Grayling2018} - Blinded and unblinded sample size reestimation procedures for stepped-wedge cluster randomized trial

\cite{Altman1980} - Statistics And Ethics In Medical Research: III How Large A Sample?

\cite{DeMartini2010} - Conservative Sample Size Estimation in Nonparametrics

\cite{Bacchetti2019} - The other arbitrary cutoff

\cite{Bakker2020} - Recommendations in pre-registrations and internal review board proposals promote formal power analyses but do not increase sample size

\subsection{Re-estimation}

One approach to addressing uncertainty in nuisance parameter values is to conduct a small \emph{pilot} trial prior to the planned definitive study \cite{Browne1995, Gould2001, Friede2006}. The pilot data can then be used to estimate the nuisance parameter, with this estimate then being used in the sample size calculation of the definitive trial. This leads to the definitive trial sample size being a random variable (a function of the pilot trial estimate),  meaning a certain degree of precision in the pilot estimate is required to avoid excessive variability in the main trial sample size, and the cost of sampling in the pilot must be weigh

traded-off against the benefits it will bring \cite{Teare2014, Whitehead2015}. In some cases, such as in cluster randomised trials, sufficient precision is not attainable within the constraints of a pilot study \cite{Lake2002, Eldridge2015}. When the pilot will deliver a sufficiently  precise estimate, it may nevertheless be an undesirable procedure due to logistical reasons. For example, a long-term primary endpoint would cause significant delay to starting the definitive trial if it must first be measured in a pilot trial. Thus, pilot estimation may not always be the best way to address uncertainty in the nuisance parameter.

In the fixed design setting, with a single estimate $\hat{\sigma}$, the sample size samba may allow sampling costs to be considered implicitly and thus arrive at the same design choice as would be obtained using the value method. It is, however, an opaque approach, and when the true MCID has been decided upon and published it will not be possible at all. In the re-estimation setting, where we expect at least one further estimate $\hat{\sigma}_2$ to be obtained, it is not possible since the MCID will have already been declared. Following the conventional approach will then force us to accept potentially absurb inflations to the sample size to maintain the same power, or to stop recruiting to the trial and just analyse the data obtained by that point. If re-estimating using the value-based method, any increases in the sample size will be bounded.

\subsection{Sample size samba}

The value-based model for SSD initially appears to be completely incompatible with conventional SSD methods, leading as it does to powers substantially above or below the usual nominal values of 0.8 and 0.9. On reflection, however, it may actually be a superior descriptive model than the constrained method when we account for the common practice of `gaming' SSD calculations by adjusting the difference to be detected until a `feasible' sample size results - the so-called `sample size samba'. For example, suppose the true MCID is $\mu = 1$. An initial guess of $\hat{\sigma} = 1$ gives a sample size of $n = 17$ to achieve 80\% power. If we were told our initial estimate of $\sigma$ was incorrect and that actually $\sigma = 1.3$, the constrained method tells us to revise the sample size to $n = 27$. We consider this to be infeasible, and pretend that our MCID is actually $\delta = 1.23$. Then, $n = 18$ will give us 80\% power for this new target difference, but 61\% for the true MCID. Using the value-based method we would arrive at the same design of $n = 18$, but through an altogether more transparent route.

\subsection{Other}

In the re-estimation context, we don't consider the question of optimising the pilot sample size. As in \cite{Whitehead2015}, we could attempt to minimise the total expected sample size (at least I think this is what they minimise but not completely clear). But this is not straightforward. Using the value-based SSR we can map the distribution of pilot samples to a distribution of definitive ns, and so can get a joint distribution of total n and power for some true hypothesised nuisance parameter and choice of pilot n. We then need to choose between these distributions. One way to do this is to get the expected value, and then maximise this. But this would all be conditional on a guessed parameter value, and the whole point is this isn't known.

Could extend to a Bayesian context, putting priors on all unknown parameters, and moving from value to utility - then choose pilot design which maximises utility. But if introducing these, would want to do MEU based on the pilot data - not this simple value-based method. And this will bring further challenges, particularly computational.

Here we have only looked at value of power at the MCID. Another view is that we want to simultaneously maximise power at all useful effects, and minimise at non-useful effects. From a multi-criteria perspective, if we had a discrete set of points / objectives we could combine these in a weighted sum. In the continuous case, this would translate to integrating over the parameter space w.r.t. a function integrating to one. So, this would be connected to a Bayesian view where we think about value in terms of expected power, or more generally expected utility. 

Thinking about powering for a target difference. The recommended approach is to set the MCID as the `smallest difference' we would not like to miss. If we have control over power and it is not set to an arbitrary threshold, we see that we don't need to find this MCID accurately. Essentially any point will do, then we think about what power we would like at that point. Alternatively, if power is fixed, then the target is defined with respect to that power - `the difference we would like to detect with 80\% power'. So the current reasoning is a bit circular - choose MCID with a vague idea of what power we're going to aim for, then choose power with respect to the MCID we've just imprecisely defined.

Another metric in the lieterature is power over n, or the number of patients required per successful trial. In our t test example, we find the cosntrained method outperforms the value based on this metric when we initially overestimate sigma. It would be helpful to unpack this value function, campare it to our additive one, and understand why these differences arrise.

Note that in low powered trials, estimates will be biased if we condition on significance, and this bias can be very large. Following NP testing properly this isn't an issue, as we are meant to abandon any hope of learning anything about the specific case in hand. In practice this might be considered a drawback, so we could potentially include this bias in our value function. But this is perhaps taking the testing set-up too seriously. Similarly when we have argued that value of power could decease as it approaches 1, since then a faithful testing approach would lead us to accepting treatments with effects only marginally better than 0. But really we should be thinking of power as a surrogate for value as a function of sample size, which we expect to be convex and bounded. From this perspective, the extra value functions not linear in power are maybe not that interesting, and could be relegated to an appendix to help explore the robustness of the main results.

If including alpha in the value function as an extension, in the discussion note that we could fix the power curve at 0.5 at the point of indifference, meaning we only have one free parameter to deal with.

See \cite{Smaldino2016} for an evolutionary model of science arguing that low power is bad practice. If we want to accommodative the idea that low power should be avoided, we can increase set-up costs. This may lead to a narrow window of opportunity, since increasing setup will also lower the point at which the trial is so overpowered as to be not worth doing at all. Might need something else, exploring how the value-based approach would work when applied across many trials - setup might be key here again. Could start by choosing the sample size of two trials simultaneously - at some point on the setup costs scale, it will be better to invest in just one (in the sense of getting more value).

Note that assigning low value to low power means we will typically favour not doing a study at all (even a free one with no set-up costs) than a low power one. But we have to make a decision, and not running a study means an effective error rate of alpha = 0, beta = 1. 

\begin{acks}
Acknowledgements.
\end{acks}

\begin{dci}
The Authors declare that there is no conflict of interest.
\end{dci}

\begin{funding}
This work was supported by the Medical Research Council [grant number xxx].
\end{funding}

\bibliographystyle{SageV}
\bibliography{C:/Users/meddwilb/Documents/Literature/Databases/DTWrefs}

\end{document}
