---
title: "Robust, value-based sample size determination for clinical trials when nuisance parameters are unknown"
author: Duncan T. Wilson
  | Leeds Institute of Clinical Trials Research
date: "01/08/2019"
output: 
  ioslides_presentation:
    smaller: true
#runtime: shiny
---

```{r setup, include=FALSE}
require(ggplot2)
require(plotly)
require(numDeriv)
require(mgcv)
require(gganimate)
require(magick)
require(RColorBrewer)
cols <- brewer.pal(8, "Dark2")
knitr::opts_chunk$set(echo = FALSE, eval=T, warning = F)
```

# Motivation

## Background {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

"Textbook" sample size caclulation for a normal endpoint:

$$
\begin{aligned}
\min_{n \in \mathbf{N}} ~ & n \\
\text{subject to } ~ & g(n, \mu, {\color{red}{\sigma}}) \geq {\color{blue}{1 - \beta^*}}
\end{aligned}
$$
$g(n, \mu, {\color{red}{\sigma}})$ - power of the trial.

${\color{red}{\sigma}}$ - an _unknown_ nuisance parameter.

${\color{blue}{1 - \beta^*}}$ - a power threshold.

## Incoherence {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

Minimial Clinically Imortant Difference: $\mu = 0.3$

Power threshold: $1 - \beta^* = 0.8$

Universe A: $\hat{\sigma} = 1 \rightarrow n = 175$

Universe B: $\hat{\sigma} = 1.3 \rightarrow n = 296$

Same effect to be detected, same power, but different sample size.

More generaly, as nuisnace parameter varies, so does the amount we are willing to invest in a study to get 80\% power to detect $\mu = 0.3$.

## Sample Size Samba {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

<img src="Samba.jpg" alt="drawing" width="600"/>

## Sample Size Samba {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

<img src="Samba_paper.png" alt="drawing" width="800"/>

## Sample Size Samba {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}
```{r}
    mu1 <- 0.3
    mu2 <- 0.4
    sig <- 1.3
    
    df <- data.frame(n=2:400)
    df$p1 <- sapply(df$n, function(n) pnorm(mu1/sqrt(2*sig/n) - qnorm(0.975)))
    df$p2 <- sapply(df$n, function(n) pnorm(mu2/sqrt(2*sig/n) - qnorm(0.975)))
    
    n_opt1 <- df[df$p1 > 0.8,][1,1]
    n_opt2 <- df[df$p2 > 0.8,][1,1]
    
    tru_p <- df[df$n==n_opt2, "p1"]
    
    df <- data.frame(n=rep(df$n, 2), p=c(df$p1, df$p2), t=c(rep("r", nrow(df)), rep("f", nrow(df))))
    
     pl <-  ggplot(df, aes(n, p, colour=t, text=round(p,2))) + geom_line() +
      geom_segment(aes(x=n_opt2, xend=n_opt2, y=0.8, yend=0),linetype=2, colour="black") +
      geom_point(data=data.frame(n=n_opt2, p=tru_p, t="r"), shape=1, size=3, colour="black") +
      annotate("text", x=320, y=0.83, 
               label = paste0("Real MCID = ", mu1), colour=cols[2]) +
      annotate("text", x=30, y=0.65,
               label = paste0("Fake MCID = ", mu2), colour=cols[1]) +
      scale_y_continuous(breaks = seq(0,1,0.2)) +
      scale_color_manual(values=cols, guide=F) +
      theme_minimal() + ylab("Power") + xlab("Sample size")
    
p <- ggplotly(pl, tooltip = c("n", "text"))
hide_legend(p)
```

## Sample size re-estimation {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

- Suppose we have an initial estimate $\hat{\sigma}$ = 1
- Then we choose $n = 175$ for 80% power to detect $\mu = 0.3$
- Bu twe then get an interim estimate of $\hat{\sigma} = 1.3 \rightarrow$ inflate sample size to $n =  296$...

No flexibility to samba - already declared $\mu = 0.3$.

$\rightarrow$ Sample size re-estimation is incoherent.

# Methods

## Proposal {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

We should:

- design trials by considering both costs (sample size) and benefit (power);
- do so in an explicit, transparent way;
- use the same methodology for an initial sample size calculation as for a re-estimation.

This would:

- make sample size re-estimation a coherent procedure, and so give us a useful tool for dealing with nuisance parameter uncertainty;
- may elimiante the need for SSR altogether.

## Value function {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

Choose $n$ to maximise _value_, denoted $v(n, \sigma)$, a weighted sum of power and sample size:

$$
\max_{n} v(n, \sigma) = g(n, \sigma) - \lambda n
$$

Implicit assumptions about value:

- Linear in sample size;
- Linear in power;
- Sample size and power are _preferentially independant_ - i.e. our preferences about sample size are independant of power, and vice versa.

## Illustration {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

Two arm parralel group trial comparing group means of normally distributed outcome.

Difference to detect: 0.3

Best guess of standard deviation: 1 

Trade-off parameter $\lambda$: 0.0022

($\rightarrow n = 175$ under both frameworks)

## Illustration {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

```{r, eval=T, gganimate = list(fig.width = 50)}
lambda  <- 0.0022

df <- expand.grid(n=seq(10,700,10), sig=seq(0.3,2,0.01))
df$pow <- apply(df, 1, function(x) power.t.test(n=x[1], delta=0.3, sd=x[2])$power)
df$v <- df$pow - lambda*df$n

f <- function(n, sig)
{
  pow <- power.t.test(n=n, delta=0.3, sd=sig)$power
  -(pow - lambda*n)
}

opt <- data.frame(sig=seq(0.3,2,0.01), n=1)
opt$n <- sapply(opt$sig, function(x) optim(10, f, method="Brent", lower=2, upper=400, sig=x)$par)
opt$pow <- apply(opt, 1, function(x) power.t.test(n=x[2], delta=0.3, sd=x[1])$power)
opt$inter <- opt$pow - lambda*opt$n

tangent <- df[,1:2]
tangent$pow <- apply(tangent, 1, function(x, opt) opt[opt$sig == x[2],]$inter + lambda*x[1], opt=opt)

const <- data.frame(sig=seq(0.3,2,0.01), n=1)
const$n <- sapply(const$sig, function(s) power.t.test(delta=0.3, sd=s, power=0.8)$n)
const$pow <- 0.8

#ggplot(df, aes(n, pow, colour=sig, group=as.factor(sig))) + geom_line() +
#  geom_point(data=opt, colour="red") +
#  geom_vline(xintercept = max(opt$n), linetype=2, colour="red")

p <- ggplot(df, aes(n, pow, group=sig)) + geom_line() +
  geom_point(data=opt, colour=cols[1]) + geom_line(data=tangent, colour=cols[1], linetype=2) +
  geom_point(data=const, colour=cols[2]) + #geom_hline(yintercept = 0.8, colour=cols[2], linetype=2) +
  ylab("Power") + scale_y_continuous(breaks=seq(0,1,0.2), limits=c(0,1)) + theme_minimal() + xlab("Sample size") +
  transition_states(sig)
 
animate(p, nframes=684, duration=2, rewind=T)
```

## Illustration {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

```{r, eval=T, gganimate = list(fig.width = 50)}
p2 <- ggplot(opt, aes(y=n, group=sig)) + 
  geom_point(aes(x=sig), colour=cols[1], size=2) + 
  geom_point(data=const, aes(x=sig), colour=cols[2], size=2) +
  geom_line(data=data.frame(sig2=opt$sig, n=opt$n), aes(x=sig2), colour=cols[1]) +
  geom_line(data=data.frame(sig2=const$sig, n=const$n), aes(x=sig2), colour=cols[2]) +
  theme_minimal() + ylab("Sample size") + xlab("Standard deviation") +
  transition_states(sig)

animate(p2, nframes=684, duration=2, rewind=T)
```

```{r, eval=F}
a_mgif <- image_read(a_gif)
b_mgif <- image_read(b_gif)

new_gif <- image_append(c(a_mgif[1], b_mgif[1]))
for(i in 2:40){
  combined <- image_append(c(a_mgif[i], b_mgif[i]))
  new_gif <- c(new_gif, combined)
}

new_gif
```

## Fixed designs {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

$\rightarrow$ A value-based approach will lead to a coherent framework for sample size re-estimation, with less variability in $n$ but more variability in power.

But, we can go further - in some cases, we don't need to do re-estimation at all.

## Fixed designs {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

```{r, eval=T}
opt$v <- apply(opt, 1, function(x) -f(x[2], x[1]))

fix <- data.frame(sig = opt$sig)
fix$n <- 175
fix$pow <- apply(fix, 1, function(x) power.t.test(n=x[2], delta=0.3, sd=x[1])$power)
fix$v <- apply(fix, 1, function(x) -f(x[2], x[1]))
fix$d <- opt$v - fix$v
fix$d2 <- round(fix$d, 2)

opt$d <- fix$d2

p <- ggplot(opt, aes(sig, n, text=round(d,2))) + geom_line() +
  geom_line(aes(y=300*v), colour=cols[1]) +
  geom_line(data= fix, aes(y=300*v), colour=cols[2]) +
  geom_line(data=fix, aes(y=300*d), colour=cols[3]) +
  annotate("text", x=0.6, y=260, label="Value of optimal n", colour=cols[1]) +
  annotate("text", x=0.45, y=165, label="Value of n = 175", color=cols[2]) + 
  annotate("text", x=0.8, y=35, label="Difference in value", color=cols[3]) +
  theme_minimal() + ylab("Sample size") + xlab("Standard deviation")

ggplotly(p, tooltip = c("text","sig"))
```

How does the value of a fixed design compare with a re-estimated design, when there is noise in the estimate? For each true SD, and for an interim analysis n, get the distribution of re-estimated ns and their true value.

```{r}
get_s_prob <- function(sig, s, n)
{
  # sig_hat^2 ~ sig^2 \chi_(2n-1) / (2n-1)
  pchisq( ((2*n-1)*s^2)/sig^2 , df=2*n-1)
}

ssr <- data.frame(sig=opt$sig)
ss <- opt$sig
for(s in ss){
  #get cum prob of getting an s estimate
  ssr <- cbind(ssr, s=sapply(ssr$sig, get_s_prob, s=s, n=50))
}

get_s_quant <- function(sig, ssr, q)
{
  which(ssr[ssr$sig==sig,] > q)[2]-1
}

get_n_quants <- function(sig, int_n, opt)
{
  df <- data.frame(n=1:250, probs=1)
  m <- which.max(opt$n)
  opt1 <- opt[1:m,]
  opt2 <- opt[(m+1):nrow(opt),]
  for(n in 1:200){
    lo_s <- opt1$sig[tail(which(opt1$n < n), n=1)]
    hi_s <- opt2$sig[head(which(opt2$n < n), n=1)]
    add <- 0
    if(length(lo_s != 0)) add <- add + pchisq( ((2*int_n-1)*lo_s^2)/sig^2 , df=2*int_n-1)
    if(length(hi_s != 0)) add <- add + 1 - pchisq( ((2*int_n-1)*hi_s^2)/sig^2, df=2*int_n-1)
    df[n,2] <- ifelse(length(add) == 0, 0, add)
  }
  c(df[df$probs > 0.05,][1,1], df[df$probs > 0.5,][1,1], df[df$probs > 0.95,][1,1])
}

opt <- opt[,1:6]
opt <- cbind(opt, t(sapply(opt$sig, get_n_quants, int_n=20, opt=opt)))
names(opt)[7:9] <- c("lo_n", "med_n", "hi_n")

#opt$lo_s <- sapply(opt$sig, get_s_quant, ssr=ssr, q=0.05)
#opt$med_s <- sapply(opt$sig, get_s_quant, ssr=ssr, q=0.5)
#opt$hi_s <- sapply(opt$sig, get_s_quant, ssr=ssr, q=0.95)

#opt$lo_n <- sapply(opt$lo_s, function(p) opt[p, "n"])
#opt$med_n <- sapply(opt$med_s, function(p) opt[p, "n"])
#opt$hi_n <- sapply(opt$hi_s, function(p) opt[p, "n"])

opt$lo_v <- apply(opt, 1, function(x) -f(x[7],x[1]))
opt$med_v <- apply(opt, 1, function(x) -f(x[8],x[1]))
opt$hi_v <- apply(opt, 1, function(x) -f(x[9],x[1]))

sim_value <- function(sig, n, opt)
{
  s <- round(sqrt(rchisq(1, 2*n-1)*sig*sig/(2*n-1)), 2)
  opt[which(sapply(opt$sig, all.equal, s)==T), "n"]
}

ssr <- data.frame(sig = rep(opt$sig,100))
n_int <- 50
ssr$s <- sapply(ssr$sig, function(sig, n) sqrt(rchisq(1, 2*n-1)*sig*sig/(2*n-1)), n=n_int)
ssr$n <- sapply(ssr$s, function(x) optim(10, f, method="Brent", lower=2, upper=400, sig=x)$par)
ssr$v <- apply(ssr, 1, function(x) -f(x[3],x[1]))

fit <- gam(v ~ s(sig, bs="cs"), data=ssr)
plot(fit, residuals = T)

ggplot(opt, aes(sig, n)) + geom_line() +
  geom_line(aes(y=300*v), colour=cols[1]) +
  geom_line(data= fix, aes(y=300*v), colour=cols[2]) +
  #stat_smooth(data=ssr, aes(sig, 300*v)) +
  geom_point(data=ssr,  aes(sig, n), alpha=0.01, siz=0.1) +
  #geom_point(data=ssr,  aes(sig, 300*v), alpha=0.05, siz=0.1) +
  theme_minimal() + ylab("Sample size") + xlab("Standard deviation")

```

# Example

## Example: cluster RCT {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

$$
n = n_i(\sigma_t) [1 + (m-1)\rho]
$$
$n_i(\sigma_t)$ = sample size for an individually randomised trial with no clustering and the same total variance, $\sigma_t^2$.

$\rho =$ Intacluster correlation coeffieint (ICC) - proportion of the total variance due to variability between clusters.

$m =$ number of participants per cluster

$k = $ number of clusters

## Example: cluster RCT {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

```{r}
clus_pow <- function(x, var_t, rho)
{
  k <- x[1]; n <- x[2]; m <- n/k
  clus_var <- var_t*rho + (var_t - var_t*rho)/m
  power.t.test(n=k, delta=0.3, sd=sqrt(clus_var))$power
}

# plot power function to find value params
df <- expand.grid(k=seq(10, 40, 5), n=seq(2,700,20))
df$pow <- apply(df, 1, clus_pow, var_t=1, rho=0.05)

p <- ggplot(df, aes(n, pow, group=k, colour=k)) + geom_line() +
  geom_point(data=data.frame(k=15, n=470, pow=clus_pow(c(15, 470), 1, 0.05)), colour="red") +
  theme_minimal()

ggplotly(p, tooltip = c("group", "x", "y"))

#des <- c(25, 450)
des <- c(15, 470)
#des <- c(25, 270)
#clus_pow(des, 1, 0.05)
lambda <- grad(clus_pow, des, var_t=1, rho=0.05)
```

```{r}
get_value <- function(x, var_t, rho)
{
  pow <- clus_pow(x, var_t, rho)
  -(pow - sum(lambda*x))
}

df <- expand.grid(var_t=seq(0.5, 2, 0.025), rho=seq(0,0.2,0.005))
df <- cbind(df, t(apply(df, 1, function(x) optim(c(10,10), fn=get_value, lower=c(2,2), upper=c(50,1000), var_t=x[1], rho=x[2], method="L-BFGS-B")$par)))
names(df)[3:4] <- c("k", "n")
df$v <- apply(df, 1, function(x) get_value(x[3:4],x[1],x[2]))
```

## Example: cluster RCT {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

Number of clusters: 15
Number of participants: 470

```{r}
tol <- 0.03
    
    df2 <- df
    df2$k <- des[1]
    df2$n <- des[2]
    df2$v <- apply(df2, 1, function(x) get_value(des,x[1],x[2]))
    
    df2$d <- df$v - df2$v
      
    ggplot(df2, aes(var_t, rho, z=d, colour=..level..)) +
      geom_contour(breaks=seq(-0.00,-0.1,-0.01)) +
      geom_contour(breaks=c(-tol), colour="red") +
      ylab("ICC") + xlab("Total variance") +
      theme_minimal()
```

## Example: cluster RCT {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

Number of clusters: 18
Number of participants: 500

```{r}
des <- c(18, 500)
tol <- 0.03
    
    df2 <- df
    df2$k <- des[1]
    df2$n <- des[2]
    df2$v <- apply(df2, 1, function(x) get_value(des,x[1],x[2]))
    
    df2$d <- df$v - df2$v
      
    ggplot(df2, aes(var_t, rho, z=d, colour=..level..)) +
      geom_contour(breaks=seq(-0.00,-0.1,-0.01)) +
      geom_contour(breaks=c(-tol), colour="red") +
      ylab("ICC") + xlab("Total variance") +
      theme_minimal()
```


## Discussion {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

Summary

- Sample size calculations can be incoherent when nuisance parameters are unknown.
- Value-based trial design can lead to coherent decisions and facilitate sample size re-estimation.
- In some cases, re-estimation can be avoided altogether.

Further work

- How to choose optimal fixed designs?
- Do our value function assumptions hold?
- Would precision be a better measure of value than power?
- Are 'underpowered' trials unethical?

# Thank you | \@DTWilson, D.T.Wilson@leeds.ac.uk, https://github.com/DTWilson/Robust_value

`r knitr::knit_exit()`













```{r, eval=F}
numericInput("mu2", "Fake MCID:", 0.3, step = 0.01)
numericInput("sig", "Estimated SD:", 1, step = 0.1)

rmarkdown::render_delayed({
  renderPlot({
    mu1 <- 0.3
    mu2 <- input$mu2
    
    df <- data.frame(n=2:400)
    df$p1 <- sapply(df$n, function(n) pnorm(mu1/sqrt(2*input$sig/n) - qnorm(0.975)))
    df$p2 <- sapply(df$n, function(n) pnorm(mu2/sqrt(2*input$sig/n) - qnorm(0.975)))
    
    n_opt1 <- df[df$p1 > 0.8,][1,1]
    n_opt2 <- df[df$p2 > 0.8,][1,1]
    
    tru_p <- df[df$n==n_opt2, "p1"]
    
    ggplot(df, aes(n)) + geom_line(aes(y=p1)) + geom_line(aes(y=p2)) +
      geom_hline(yintercept = 0.8, colour=cols[1], linetype=2) +
      #geom_segment(aes(x=n_opt1, xend=n_opt1, y=df[df$n==n_opt1,2], yend=0), colour=cols[3], linetype=3) +
      geom_segment(aes(x=n_opt2, xend=n_opt2, y=df[df$n==n_opt2,3], yend=0), colour=cols[3], linetype=3) +
      geom_point(data=df[df$n==n_opt2, 1:2], aes(y=p1), shape=1, size=3) +
      annotate("text", x=n_opt2+40, y=df[df$n==n_opt2,2]-0.02, 
               label = paste0("Actual power = ", round(df[df$n==n_opt2,2], 2))) +
      theme_minimal() + ylab("Power") + xlab("Sample size")
  })
})
```


## Example 2: Surivival

Sample size is time recruiting and time following-up

Nuisance param is the event rate in the control arm

Take conventional design and extract value function parameters.

========

Plot optimal sample size over param space. Just 1 d plot with two lines - plotly here since lines will be on different scales

Plot value of conventional design

Search for best fixed design - can't do manually now, but could animate the 2D search process

Plot value of best fixed design.

=========

```{r, eval=F}
# see chapter 9, D.Collet Modelling survival data in medical research
s_bar <- function(x, med1, med2)
{
  # Assuming exponential models with medians med1 and med2,
  # so S(x) = exp(-ln(2)x/med)
  (exp( -(log(2)/med1)*x ) + exp( -(log(2)/med2)*x ))/2
}
  
get_pow <- function(x, med1)
{
  # med1 - median survival control
  # x - (accrual time period, total trial time)
  # a - accrual time period
  # f - follow-up time period
  a <- x[[1]]; f <- max(x[[2]] - x[[1]], 0)
  dif <- 2
  med2 <- med1 + dif
  m <- 10.3 # monthly accrual rate
  theta <- log(med1/med2) # log harzard ratio
  
  # Expected sample size
  n <- m*a
  # Probability of an event
  prob_d <- 1 - (s_bar(f, med1, med2) + 4*s_bar(0.5*a + f, med1, med2) + s_bar(a + f, med1, med2))/6
  # Expected number of events
  d <- n*prob_d
  # Power
  pnorm( -theta*sqrt(d)/2 - qnorm(0.975) )
}

get_pow(c(23,48), 4.5)
lambda <- grad(get_pow, c(23,48), med1=4.5)
```


```{r, eval=F}
get_value <- function(x, med1, lambda)
{
  pow <- get_pow(x, med1)
  -(pow - sum(lambda*x))
}

get_design <- function(med1)
{
  o <- optim(c(25,50), get_value, med1=med1, lambda=lambda,
              lower = c(0,0), upper = c(100,100), method="L-BFGS-B")
  return(c(o$par, -o$value))
} 

df <- data.frame(med1=seq(2, 8, 0.01))
df <- cbind(df, t(sapply(df$med1, get_design)))
names(df)[2:4] <- c("a", "t", "v")

df2 <- data.frame(med1=df$med1)
fixed <- c(20, 47)
df2$a <- fixed[1]; df2$t <- fixed[2]
df2$v <- -sapply(df2$med1, function(x) get_value(fixed, x, lambda))
df2$d <- df2$v - df$v

# Optimal designs
ggplot(df, aes(a, t-a, colour=med1)) + geom_point() +
  xlab("Accrual") + ylab("Follow-up") +
  theme_minimal()

# Optimal and fixed design values
p <- ggplot(df2, aes(x=med1, text=round(d, 2))) + geom_line(aes(y=v)) + geom_line(aes(y=-d), colour="blue") +
  geom_line(data=df, aes(y=v), colour="red") + 
  xlab("Median survival time") + ylab("Value") +
  theme_minimal()

ggplotly(p, tooltip = c("med1", "text"))
```

```{r, eval=F}
coverage <- function(fixed, fit, diff, lambda)
{
  f <- function(x, fixed, lambda, fit, diff)
  {
    as.numeric(as.numeric(predict(fit, newdata = data.frame(med1=x))) + get_value(fixed, x, lambda) <= diff)
  }
  
  -integrate(f, 2, 8, fixed=fixed, lambda=lambda, fit=fit, diff=diff)$value
}

fit <- gam(v ~ s(med1), data=df)

diff <- 0.03
opt <- optim(c(20,47), coverage, fit=fit, diff=diff, lambda=lambda,
              lower = c(0,0), upper = c(100,100), method="L-BFGS-B")
```


```{r, eval=F}
df <- data.frame(x=seq(0,1,0.01))
df$y1 <- 2*df$x + 0.2
df$y2 <- 2.1*df$x + 0.25
df$d <- df$y2 - df$y1

p <- ggplot(df, aes(x, label = d)) + geom_line(aes(y=y1), linetype=1) + 
  geom_line(aes(y=y2), linetype=2) +
  geom_ribbon(data=df[df$x > 0.3 & df$x < 0.7,], aes(ymin=y1, ymax=y2), fill="blue", alpha=0.1) +
  theme_minimal()

ggplotly(p, tooltip="d")
```

## Shiny

```{r, eval=F}
 fluidRow(style = "padding-bottom: 20px;",
        column(4, numericInput("k", "How many clusters?", des[1])),
        column(4, numericInput("n", "How many patients?", des[2])),
        column(4, numericInput("tol", "How tolerable?", 0.01, step=0.01)))

rmarkdown::render_delayed({
  renderPlot(width = 800, {
    des <- c(input$k, input$n)
    
    df2 <- df
    df2$k <- des[1]
    df2$n <- des[2]
    df2$v <- apply(df2, 1, function(x) get_value(des,x[1],x[2]))
    
    df2$d <- df$v - df2$v
      
    ggplot(df2, aes(var_t, rho, z=d, colour=..level..)) +
      geom_contour(breaks=seq(-0.00,-0.1,-0.01)) +
      geom_contour(breaks=c(-input$tol), colour="red") +
      ylab("ICC") + xlab("Total variance") +
      theme_minimal()
  })
})
```

## Optimality criteria {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

If we want to use a fixed design, how should we choose one?

1. Crtiera (a): Minimax.
    + over some range of the nuisance parameter, choose design with the best worst case performance.
2. Criteria (b): Tolerance
    + Choose the design which maximises the region of nuisance parameter space where its value is within a tolerable distance from the optimal value.
