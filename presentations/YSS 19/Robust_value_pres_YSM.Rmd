---
title: "Untitled"
author: "Duncan T. Wilson"
date: "18/07/2019"
output: 
  ioslides_presentation:
    logo: UoL_logo3.png
#runtime: shiny
---

```{r setup, include=FALSE}
require(ggplot2)
require(plotly)
require(numDeriv)
knitr::opts_chunk$set(echo = FALSE, eval=F)
```

## Background {data-background=CTRU-corner-&-logo.png data-background-position="left bottom" data-background-size=20%}

Conventional SSD - constrained optimisation

Sensitive to nuisance parameters: variance components, control arm rates

## The Threshold Myth

Using conventional SSD procedure makes sense if there is a threshold - but this is a myth

## Incoherence - an example

Continuous normal with unknwon SD

Plot shoiwing how n varies with SD - animate

## Samba

Is constrained opt for SSD incoherent?

Not in practice - becuase of the sample size samba

We do balance sampling costs and power (e..g. choosing a low power trial if 80\% was bad value), but in a sneaky way

Example - shiny, showing the petend power and the actual power

## Incoherence in SSR

Suppose we inflate in SSR

No flexibility to samba - already declared the MCID

So, SSR is an incoherent procedure

## Proposal

We should design based on some explicit combination of cost and benefit 

We aleardy do this in SSD, but not in a transparent way and therefore not as well as we could

Would elimante incoherence from SSR, so we can better deal with nuisance parameter uncertainty (which is everywhere)

We will show that it could actually free us from the need to do SSR - good for situations where interim analysis would be hard (e.g. long term endpoint) or where the nuisance parameter is hard to estimate (e.g. ICC - ref to Eldridge paper and the other SSR cluster one?)

## Method

Simple weighted of sum of power and sample size.

Assumptions: linear cost and linear benefit in power. Latter is a bit shaky - e.g. very high power at the MCID will also lead to high power not far from the null. But for the scales we are interested in it is probably a safe assumption. And in that example, the rate of increase in power would be tiny, so perhaps OK even then.

Implications: e.g. moving from 20 to 25% is of the same value as moving from 75 to 80\%

If we do an SSR re-estimation, compare the joint distributions of power, n, and value for both methods. As we would expect from previous slide, if we misspecify the SD our SSR precedure will have much less variability in n (but now giving some variability in power)

## Fixed designs

... but we can go further.

Suppose we don't want t, or can't, do an interim SSR (e.g. nuisance parameter is very hard to estimate with a small sample, e.g. ICC)

Take our initial design, and compare its value over SD with the value of the best design.

We see there isn't much difference for a large range of SD - so an interim analysis is of little value.

Plotly so we can see the difference in value at different points

## Optimality criteria

If we want to use a fixed design, how to choose one?

We could do a minimax - over some sd range, choose design with the best worst case value. But, sensitive to the range end points.

Alternative - choose the design which maximises the param space size where value is within a tolerable distance from the optimal.

## Example 1: cluster RCT

Sample size is number of clusters - cluster size is fixed.

Uncertainty in the two variance components (although assumed common across arms)

Take conventional design and extract value function parameters.

-------

Plot optimal sample size over param space. Just a lablelled contour plot?

Plot differnce of value of conventional design and optimal design

Search for best fixed design - do this manually, working along k values, plotting the covered area, and plotting the result.

------

```{r}
clus_pow <- function(x, var_t, rho)
{
  k <- x[1]; n <- x[2]; m <- n/k
  clus_var <- var_t*rho + (var_t - var_t*rho)/m
  power.t.test(n=k, delta=0.3, sd=sqrt(clus_var))$power
}

# plot power function to find value params
df <- expand.grid(k=seq(10, 40, 5), n=seq(2,700,20))
df$pow <- apply(df, 1, clus_pow, var_t=1, rho=0.05)

p <- ggplot(df, aes(n, pow, group=k, colour=k)) + geom_line() +
  geom_point(data=data.frame(k=15, n=470, pow=clus_pow(c(15, 470), 1, 0.05)), colour="red") +
  theme_minimal()

ggplotly(p, tooltip = c("group", "x", "y"))

#des <- c(25, 450)
des <- c(15, 470)
#des <- c(25, 270)
clus_pow(des, 1, 0.05)
lambda <- grad(clus_pow, des, var_t=1, rho=0.05)
```

```{r}
get_value <- function(x, var_t, rho)
{
  pow <- clus_pow(x, var_t, rho)
  -(pow - sum(lambda*x))
}

df <- expand.grid(var_t=seq(0.5, 2, 0.025), rho=seq(0,0.2,0.005))
df <- cbind(df, t(apply(df, 1, function(x) optim(c(10,10), fn=get_value, lower=c(2,2), upper=c(50,1000), var_t=x[1], rho=x[2], method="L-BFGS-B")$par)))
names(df)[3:4] <- c("k", "n")
df$v <- apply(df, 1, function(x) get_value(x[3:4],x[1],x[2]))
```

## Shiny

```{r}
numericInput("k", "How many clusters?", des[1])
numericInput("n", "How many patients?", des[2])
numericInput("tol", "How tolerable?", 0.01)

rmarkdown::render_delayed({
  renderPlot({
    des <- c(input$k, input$n)
    
    df2 <- df
    df2$k <- des[1]
    df2$n <- des[2]
    df2$v <- apply(df2, 1, function(x) get_value(des,x[1],x[2]))
    
    df2$d <- df$v - df2$v
      
    ggplot(df2, aes(var_t, rho, z=d, colour=..level..)) +
      geom_contour(breaks=seq(-0.00,-0.1,-0.01)) +
      geom_contour(breaks=c(-input$tol), colour="red") +
      ylab(expression(rho)) + xlab(expression(sigma[t]^2)) +
      theme_minimal()
  })
})
```

## Example 2: Surivival

Sample size is time recruiting and time following-up

Nuisance param is the event rate in the control arm

Take conventional design and extract value function parameters.

========

Plot optimal sample size over param space. Just 1 d plot with two lines - plotly here since lines will be on different scales

Plot value of conventional design

Search for best fixed design - can't do manually now, but could animate the 2D search process

Plot value of best fixed design.

=========

```{r}
# see chapter 9, D.Collet Modelling survival data in medical research
s_bar <- function(x, med1, med2)
{
  # Assuming exponential models with medians med1 and med2,
  # so S(x) = exp(-ln(2)x/med)
  (exp( -(log(2)/med1)*x ) + exp( -(log(2)/med2)*x ))/2
}
  
get_pow <- function(x, med1)
{
  # med1 - median survival control
  # x - (accrual time period, total trial time)
  # a - accrual time period
  # f - follow-up time period
  a <- x[[1]]; f <- max(x[[2]] - x[[1]], 0)
  dif <- 2
  med2 <- med1 + dif
  m <- 10.3 # monthly accrual rate
  theta <- log(med1/med2) # log harzard ratio
  
  # Expected sample size
  n <- m*a
  # Probability of an event
  prob_d <- 1 - (s_bar(f, med1, med2) + 4*s_bar(0.5*a + f, med1, med2) + s_bar(a + f, med1, med2))/6
  # Expected number of events
  d <- n*prob_d
  # Power
  pnorm( -theta*sqrt(d)/2 - qnorm(0.975) )
}

get_pow(c(23,48), 4.5)
lambda <- grad(get_pow, c(23,48), med1=4.5)
```


```{r}
get_value <- function(x, med1, lambda)
{
  pow <- get_pow(x, med1)
  -(pow - sum(lambda*x))
}

get_design <- function(med1)
{
  o <- optim(c(25,50), get_value, med1=med1, lambda=lambda,
              lower = c(0,0), upper = c(100,100), method="L-BFGS-B")
  return(c(o$par, -o$value))
} 

df <- data.frame(med1=seq(2, 8, 0.01))
df <- cbind(df, t(sapply(df$med1, get_design)))
names(df)[2:4] <- c("a", "t", "v")

df2 <- data.frame(med1=df$med1)
fixed <- c(20, 47)
df2$a <- fixed[1]; df2$t <- fixed[2]
df2$v <- -sapply(df2$med1, function(x) get_value(fixed, x, lambda))
df2$d <- df2$v - df$v

# Optimal designs
ggplot(df, aes(a, t-a, colour=med1)) + geom_point() +
  xlab("Accrual") + ylab("Follow-up") +
  theme_minimal()

# Optimal and fixed design values
p <- ggplot(df2, aes(x=med1, text=round(df2$d, 2))) + geom_line(aes(y=v)) + geom_line(aes(y=-d), colour="blue") +
  geom_line(data=df, aes(y=v), colour="red") + 
  xlab("Median survival time") + ylab("Value") +
  theme_minimal()

ggplotly(p)
```

```{r}
coverage <- function(fixed, fit, diff, lambda)
{
  f <- function(x, fixed, lambda, fit, diff)
  {
    as.numeric(as.numeric(predict(fit, newdata = data.frame(med1=x))) + get_value(fixed, x, lambda) <= diff)
  }
  
  -integrate(f, 2, 8, fixed=fixed, lambda=lambda, fit=fit, diff=diff)$value
}

fit <- gam(v ~ s(med1), data=df)

diff <- 0.03
opt <- optim(c(20,47), coverage, fit=fit, diff=diff, lambda=lambda,
              lower = c(0,0), upper = c(100,100), method="L-BFGS-B")
```


```{r}
df <- data.frame(x=seq(0,1,0.01))
df$y1 <- 2*df$x + 0.2
df$y2 <- 2.1*df$x + 0.25
df$d <- df$y2 - df$y1

p <- ggplot(df, aes(x, label = d)) + geom_line(aes(y=y1), linetype=1) + 
  geom_line(aes(y=y2), linetype=2) +
  geom_ribbon(data=df[df$x > 0.3 & df$x < 0.7,], aes(ymin=y1, ymax=y2), fill="blue", alpha=0.1) +
  theme_minimal()

ggplotly(p, tooltip="d")
```

## Implementation

R code

## Discussion

Alternatives: could look at precision rather than power, but we are more familiar with the latter; and could use more elaborate value function, but will be harder to define / elicit. And we find that sample size keeps increasing with SD - so not intuitively what we want.

## Conclusion

Sample size is not a game - shouldn't be playing with patients



